[
  {
    "objectID": "fggk21.html",
    "href": "fggk21.html",
    "title": "Basics with Emotikon Project",
    "section": "",
    "text": "This script uses a subset of data reported in Fühner et al. (2021). To circumvent delays associated with model fitting we work with models that are less complex than those in the reference publication. All the data to reproduce the models in the publication are used here, too; the script requires only a few changes to specify the more complex models in the article.\nThe script is structured in four main sections:"
  },
  {
    "objectID": "fggk21.html#read-data",
    "href": "fggk21.html#read-data",
    "title": "Basics with Emotikon Project",
    "section": "3.1 Read data",
    "text": "3.1 Read data\n\ndf = DataFrame(dataset(:fggk21))\ntransform!(df,\n    :age =&gt; (x -&gt; x .- 8.5) =&gt; :a1,\n    :Sex =&gt; categorical =&gt; :Sex,\n    :Test =&gt; categorical =&gt; :Test,\n  )\nlevels!(df.Sex, [\"male\", \"female\"])\nrecode!(df.Sex, \"male\" =&gt; \"Boys\", \"female\" =&gt; \"Girls\")\nlevels!(df.Test, [\"Run\", \"Star_r\", \"S20_r\", \"SLJ\", \"BPT\"])\nrecode!(\n  df.Test,\n  \"Run\" =&gt; \"Endurance\",\n  \"Star_r\" =&gt; \"Coordination\",\n  \"S20_r\" =&gt; \"Speed\",\n  \"SLJ\" =&gt; \"PowerLOW\",\n  \"BPT\" =&gt; \"PowerUP\",\n)\ndescribe(df)\n\n8×7 DataFrame\n\n\n\nRow\nvariable\nmean\nmin\nmedian\nmax\nnmissing\neltype\n\n\n\nSymbol\nUnion…\nAny\nUnion…\nAny\nInt64\nDataType\n\n\n\n\n1\nCohort\n\n2011\n\n2019\n0\nString\n\n\n2\nSchool\n\nS100043\n\nS800200\n0\nString\n\n\n3\nChild\n\nC002352\n\nC117966\n0\nString\n\n\n4\nSex\n\nBoys\n\nGirls\n0\nCategoricalValue{String, UInt32}\n\n\n5\nage\n8.56073\n7.99452\n8.55852\n9.10609\n0\nFloat64\n\n\n6\nTest\n\nEndurance\n\nPowerUP\n0\nCategoricalValue{String, UInt32}\n\n\n7\nscore\n226.141\n1.14152\n4.65116\n1530.0\n0\nFloat64\n\n\n8\na1\n0.0607297\n-0.505476\n0.0585216\n0.606092\n0\nFloat64\n\n\n\n\n\n\n\n3.1.1 Transformations\nWe center age at 8.5 years and compute z-scores for each Test. With these variables the data frame df contains all variables used for the final model in the original publication.\n\nselect!(groupby(df, :Test),  Not(:score), :score =&gt; zscore =&gt; :zScore)\n\n525126×8 DataFrame525101 rows omitted\n\n\n\nRow\nTest\nCohort\nSchool\nChild\nSex\nage\na1\nzScore\n\n\n\nCat…\nString\nString\nString\nCat…\nFloat64\nFloat64\nFloat64\n\n\n\n\n1\nSpeed\n2013\nS100067\nC002352\nBoys\n7.99452\n-0.505476\n1.7913\n\n\n2\nPowerUP\n2013\nS100067\nC002352\nBoys\n7.99452\n-0.505476\n-0.0622317\n\n\n3\nPowerLOW\n2013\nS100067\nC002352\nBoys\n7.99452\n-0.505476\n-0.0336567\n\n\n4\nCoordination\n2013\nS100067\nC002352\nBoys\n7.99452\n-0.505476\n1.46874\n\n\n5\nEndurance\n2013\nS100067\nC002352\nBoys\n7.99452\n-0.505476\n0.331058\n\n\n6\nSpeed\n2013\nS100067\nC002353\nBoys\n7.99452\n-0.505476\n1.15471\n\n\n7\nPowerUP\n2013\nS100067\nC002353\nBoys\n7.99452\n-0.505476\n0.498354\n\n\n8\nPowerLOW\n2013\nS100067\nC002353\nBoys\n7.99452\n-0.505476\n-0.498822\n\n\n9\nCoordination\n2013\nS100067\nC002353\nBoys\n7.99452\n-0.505476\n-0.9773\n\n\n10\nEndurance\n2013\nS100067\nC002353\nBoys\n7.99452\n-0.505476\n0.574056\n\n\n11\nSpeed\n2013\nS100067\nC002354\nBoys\n7.99452\n-0.505476\n0.0551481\n\n\n12\nPowerUP\n2013\nS100067\nC002354\nBoys\n7.99452\n-0.505476\n0.218061\n\n\n13\nPowerLOW\n2013\nS100067\nC002354\nBoys\n7.99452\n-0.505476\n-0.757248\n\n\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n\n\n525115\nCoordination\n2018\nS401470\nC117964\nBoys\n9.10609\n0.606092\n-1.43175\n\n\n525116\nEndurance\n2018\nS401470\nC117964\nBoys\n9.10609\n0.606092\n-0.944681\n\n\n525117\nSpeed\n2018\nS401470\nC117965\nGirls\n9.10609\n0.606092\n0.31086\n\n\n525118\nPowerUP\n2018\nS401470\nC117965\nGirls\n9.10609\n0.606092\n0.0779146\n\n\n525119\nPowerLOW\n2018\nS401470\nC117965\nGirls\n9.10609\n0.606092\n-0.137027\n\n\n525120\nCoordination\n2018\nS401470\nC117965\nGirls\n9.10609\n0.606092\n-1.8077\n\n\n525121\nEndurance\n2018\nS401470\nC117965\nGirls\n9.10609\n0.606092\n0.513306\n\n\n525122\nSpeed\n2018\nS800200\nC117966\nBoys\n9.10609\n0.606092\n0.0551481\n\n\n525123\nPowerUP\n2018\nS800200\nC117966\nBoys\n9.10609\n0.606092\n0.0779146\n\n\n525124\nPowerLOW\n2018\nS800200\nC117966\nBoys\n9.10609\n0.606092\n-1.32578\n\n\n525125\nCoordination\n2018\nS800200\nC117966\nBoys\n9.10609\n0.606092\n0.473217\n\n\n525126\nEndurance\n2018\nS800200\nC117966\nBoys\n9.10609\n0.606092\n-0.0941883"
  },
  {
    "objectID": "fggk21.html#extract-a-stratified-subsample",
    "href": "fggk21.html#extract-a-stratified-subsample",
    "title": "Basics with Emotikon Project",
    "section": "3.2 Extract a stratified subsample",
    "text": "3.2 Extract a stratified subsample\nFor the prupose of the tutorial, we extract a random sample of 1000 boys and 1000 girls. Child, School, and Cohort are grouping variables. Traditionally, they are called random factors because the units (levels) of the factor are assumed to be a random sample from the population of their units (levels).\nCohort has only nine “groups” and could have been included as a set of polynomical fixed-effect contrasts rather than a random factor. This choice warrants a short excursion: The secular trends are very different for different tests and require the inclusion of interaction terms with Test contrasts (see Figure 4 in (Fühner et al., 2021). The authors opted to absorb these effects in cohort-related variance components for the Test contrasts and plan to address the details of secular changes in a separate analysis.\nFor complex designs, when they are in the theoretical focus of an article, factors and covariates should be specified as part of the fixed effects. If they are not in the theoretical focus, but serve as statistical control variables, they could be put in the RES - if supported by the data.\nStratified sampling: We generate a Child table with information about children. MersenneTwister(42) specifies 42 as the seed for the random number generator to ensure reproducibility of the stratification. For a different pattern of results choose, for example, 84. We randomly sample 1000 boys and 1000 girls from this table; they are stored in samp. Then, we extract the corresponding subset of these children’s test scores from df and store them dat.\n\nChild = unique(select(df, :Cohort, :School, :Child, :Sex, :age))\nsample = let\n  rng = MersenneTwister(42)\n  combine(\n    groupby(Child, :Sex), x -&gt; x[rand(rng, 1:nrow(x), 1000), :]\n  )\nend\ninsamp(x) = x ∈ sample.Child\ndat = @subset(df, insamp(:Child))\n\n9663×8 DataFrame9638 rows omitted\n\n\n\nRow\nTest\nCohort\nSchool\nChild\nSex\nage\na1\nzScore\n\n\n\nCat…\nString\nString\nString\nCat…\nFloat64\nFloat64\nFloat64\n\n\n\n\n1\nSpeed\n2013\nS101540\nC002482\nGirls\n7.99452\n-0.505476\n0.0551481\n\n\n2\nPowerUP\n2013\nS101540\nC002482\nGirls\n7.99452\n-0.505476\n-0.342524\n\n\n3\nPowerLOW\n2013\nS101540\nC002482\nGirls\n7.99452\n-0.505476\n0.74162\n\n\n4\nCoordination\n2013\nS101540\nC002482\nGirls\n7.99452\n-0.505476\n-0.209186\n\n\n5\nEndurance\n2013\nS101540\nC002482\nGirls\n7.99452\n-0.505476\n-0.127938\n\n\n6\nSpeed\n2013\nS102090\nC002513\nGirls\n7.99452\n-0.505476\n-0.422921\n\n\n7\nPowerUP\n2013\nS102090\nC002513\nGirls\n7.99452\n-0.505476\n-0.762963\n\n\n8\nPowerLOW\n2013\nS102090\nC002513\nGirls\n7.99452\n-0.505476\n-0.188712\n\n\n9\nCoordination\n2013\nS102090\nC002513\nGirls\n7.99452\n-0.505476\n0.81382\n\n\n10\nEndurance\n2013\nS102090\nC002513\nGirls\n7.99452\n-0.505476\n0.452557\n\n\n11\nSpeed\n2013\nS103299\nC002621\nGirls\n7.99452\n-0.505476\n0.859704\n\n\n12\nPowerUP\n2013\nS103299\nC002621\nGirls\n7.99452\n-0.505476\n0.218061\n\n\n13\nPowerLOW\n2013\nS103299\nC002621\nGirls\n7.99452\n-0.505476\n0.74162\n\n\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n\n\n9652\nCoordination\n2018\nS111508\nC117805\nGirls\n9.10609\n0.606092\n-0.95589\n\n\n9653\nEndurance\n2018\nS111508\nC117805\nGirls\n9.10609\n0.606092\n-1.67367\n\n\n9654\nSpeed\n2018\nS111648\nC117827\nBoys\n9.10609\n0.606092\n0.859704\n\n\n9655\nPowerUP\n2018\nS111648\nC117827\nBoys\n9.10609\n0.606092\n0.6385\n\n\n9656\nPowerLOW\n2018\nS111648\nC117827\nBoys\n9.10609\n0.606092\n0.483194\n\n\n9657\nCoordination\n2018\nS111648\nC117827\nBoys\n9.10609\n0.606092\n0.708474\n\n\n9658\nEndurance\n2018\nS111648\nC117827\nBoys\n9.10609\n0.606092\n-0.337186\n\n\n9659\nSpeed\n2018\nS112197\nC117868\nBoys\n9.10609\n0.606092\n0.578748\n\n\n9660\nPowerUP\n2018\nS112197\nC117868\nBoys\n9.10609\n0.606092\n0.498354\n\n\n9661\nPowerLOW\n2018\nS112197\nC117868\nBoys\n9.10609\n0.606092\n-0.550508\n\n\n9662\nCoordination\n2018\nS112197\nC117868\nBoys\n9.10609\n0.606092\n0.538978\n\n\n9663\nEndurance\n2018\nS112197\nC117868\nBoys\n9.10609\n0.606092\n0.938553\n\n\n\n\n\n\nDue to missing scores for some tests we have about 2% less than 10,000 observtions."
  },
  {
    "objectID": "fggk21.html#no-evidence-for-age-x-sex-x-test-interaction",
    "href": "fggk21.html#no-evidence-for-age-x-sex-x-test-interaction",
    "title": "Basics with Emotikon Project",
    "section": "3.3 No evidence for age x Sex x Test interaction",
    "text": "3.3 No evidence for age x Sex x Test interaction\nThe main results are captured in the figure constructed in this section. We build it both for the full data and the stratified subset.\n\ndf2 = combine(\n  groupby(\n    select(df, :, :age =&gt; ByRow(x -&gt; round(x; digits=1)) =&gt; :age),\n    [:Sex, :Test, :age],\n  ),\n  :zScore =&gt; mean =&gt; :zScore,\n  :zScore =&gt; length =&gt; :n,\n)\n\n120×5 DataFrame95 rows omitted\n\n\n\nRow\nSex\nTest\nage\nzScore\nn\n\n\n\nCat…\nCat…\nFloat64\nFloat64\nInt64\n\n\n\n\n1\nBoys\nSpeed\n8.0\n-0.0265138\n1223\n\n\n2\nBoys\nPowerUP\n8.0\n0.026973\n1227\n\n\n3\nBoys\nPowerLOW\n8.0\n0.121609\n1227\n\n\n4\nBoys\nCoordination\n8.0\n-0.0571726\n1186\n\n\n5\nBoys\nEndurance\n8.0\n0.292695\n1210\n\n\n6\nGirls\nSpeed\n8.0\n-0.35164\n1411\n\n\n7\nGirls\nPowerUP\n8.0\n-0.610355\n1417\n\n\n8\nGirls\nPowerLOW\n8.0\n-0.279872\n1418\n\n\n9\nGirls\nCoordination\n8.0\n-0.268221\n1381\n\n\n10\nGirls\nEndurance\n8.0\n-0.245573\n1387\n\n\n11\nBoys\nSpeed\n8.1\n0.0608397\n3042\n\n\n12\nBoys\nPowerUP\n8.1\n0.0955413\n3069\n\n\n13\nBoys\nPowerLOW\n8.1\n0.123099\n3069\n\n\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n\n\n109\nBoys\nCoordination\n9.0\n0.254973\n4049\n\n\n110\nBoys\nEndurance\n9.0\n0.258082\n4034\n\n\n111\nGirls\nSpeed\n9.1\n-0.0286172\n1154\n\n\n112\nGirls\nPowerUP\n9.1\n-0.0752301\n1186\n\n\n113\nGirls\nPowerLOW\n9.1\n-0.094587\n1174\n\n\n114\nGirls\nCoordination\n9.1\n0.00276252\n1162\n\n\n115\nGirls\nEndurance\n9.1\n-0.235591\n1150\n\n\n116\nBoys\nSpeed\n9.1\n0.325745\n1303\n\n\n117\nBoys\nPowerUP\n9.1\n0.616416\n1320\n\n\n118\nBoys\nPowerLOW\n9.1\n0.267577\n1310\n\n\n119\nBoys\nCoordination\n9.1\n0.254342\n1297\n\n\n120\nBoys\nEndurance\n9.1\n0.251045\n1294\n\n\n\n\n\n\n\n3.3.1 Figure(s) of interaction\nThe core results of the article are reported in Figure 2 of Fühner et al. (2021). In summary:\n\nMain effects of age and Sex: There are developmental gains in the ninth year of life; boys outperform girls. There is no main effect of Test because of z-scoring.\nInteractions of Test and age: Tests differ in how much children improve during the year (i.e., the magnitude of developmental gain), that is slopes depend on Test.\nInteractions of Test and Sex: The sex difference is test dependent, that is the difference between the slopes depends on Test.\nThe most distinctive result is the absence of evidence for an age x Sex x Test interaction, that is the slopes for boys and girls are statistically parallel for each of the five tests.\n\n\n\nCode\nlet\n  design1 = mapping(:age, :zScore; color=:Sex, col=:Test)\n  lines1 = design1 * linear()\n  means1 = design1 * visual(Scatter; markersize=5)\n  draw(data(df2) * means1 + data(df) * lines1;)\nend\n\n\n\n\n\n\n\n\nFigure 1: Age trends by sex for each Test for the full data set\n\n\n\n\n\nFigure 1 shows performance differences for the full set of data between 8.0 and 9.2 years by sex in the five physical fitness tests presented as z-transformed data computed separately for each test.\n\nEndurance = cardiorespiratory endurance (i.e., 6-min-run test),\nCoordination = star-run test,\nSpeed = 20-m linear sprint test,\nPowerLOW = power of lower limbs (i.e., standing long jump test),\nPowerUP = power of upper limbs (i.e., ball push test),\nSD = standard deviation. Points are binned observed child means; lines are simple regression fits to the observations.\n\nWhat do the results look like for the stratified subsample? Here the parallelism is much less clear. In the final LMM we test whether the two regression lines in each of the five panels are statistically parallel for this subset of data. That is, we test the interaction of Sex and age as nested within the levels of Test. Most people want to know the signficance of these five Sex x age interactions.\nThe theoretical focus of the article, however, is on comparisons between tests displayed next to each other. We ask whether the degree of parallelism is statistically the same for Endurance and Coordination (H1), Coordination and Speed (H2), Speed and PowerLOW (H3), and PowerLow and PowerUP (H4). Hypotheses H1 to H4 require Sequential Difference contrasts c1 to c4 for Test; they are tested as fixed effects for`H1 x age x Sex, H2 x age x Sex, H3 x age x Sex, and H4 x age x Sex.\n\n\nCode\ndat2 = combine(\n  groupby(\n    select(dat, :, :age =&gt; ByRow(x -&gt; round(x; digits=1)) =&gt; :age),\n    [:Sex, :Test, :age],\n  ),\n  :zScore =&gt; mean =&gt; :zScore,\n  :zScore =&gt; length =&gt; :n,\n)\n\n\n120×5 DataFrame95 rows omitted\n\n\n\nRow\nSex\nTest\nage\nzScore\nn\n\n\n\nCat…\nCat…\nFloat64\nFloat64\nInt64\n\n\n\n\n1\nGirls\nSpeed\n8.0\n-0.323114\n28\n\n\n2\nGirls\nPowerUP\n8.0\n-0.590476\n26\n\n\n3\nGirls\nPowerLOW\n8.0\n0.0677992\n27\n\n\n4\nGirls\nCoordination\n8.0\n0.0273318\n25\n\n\n5\nGirls\nEndurance\n8.0\n-0.17337\n26\n\n\n6\nBoys\nSpeed\n8.0\n0.394634\n19\n\n\n7\nBoys\nPowerUP\n8.0\n0.328703\n19\n\n\n8\nBoys\nPowerLOW\n8.0\n0.105077\n19\n\n\n9\nBoys\nCoordination\n8.0\n-0.170018\n19\n\n\n10\nBoys\nEndurance\n8.0\n0.407084\n19\n\n\n11\nGirls\nSpeed\n8.1\n-0.205934\n54\n\n\n12\nGirls\nPowerUP\n8.1\n-0.653961\n54\n\n\n13\nGirls\nPowerLOW\n8.1\n-0.157127\n54\n\n\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n\n\n109\nGirls\nCoordination\n9.0\n-0.000188842\n68\n\n\n110\nGirls\nEndurance\n9.0\n-0.234448\n68\n\n\n111\nGirls\nSpeed\n9.1\n-0.285047\n25\n\n\n112\nGirls\nPowerUP\n9.1\n-0.112684\n25\n\n\n113\nGirls\nPowerLOW\n9.1\n-0.117645\n24\n\n\n114\nGirls\nCoordination\n9.1\n-0.127844\n25\n\n\n115\nGirls\nEndurance\n9.1\n-0.587496\n24\n\n\n116\nBoys\nSpeed\n9.1\n0.379808\n17\n\n\n117\nBoys\nPowerUP\n9.1\n0.523085\n17\n\n\n118\nBoys\nPowerLOW\n9.1\n0.294696\n17\n\n\n119\nBoys\nCoordination\n9.1\n0.209309\n16\n\n\n120\nBoys\nEndurance\n9.1\n0.266512\n16\n\n\n\n\n\n\n\n\nCode\nlet\n  design2 = mapping(:age, :zScore; color=:Sex, col=:Test)\n  lines2 = design2 * linear()\n  means2 = design2 * visual(Scatter; markersize=5)\n  draw(data(dat2) * means2 + data(dat) * lines2;)\nend\n\n\n\n\n\n\n\n\nFigure 2: Age trends by sex for each Test for the stratified sample\n\n\n\n\n\nFigure 2 Performance differences for subset of data between 8.0 and 9.2 years by sex in the five physical fitness tests presented as z-transformed data computed separately for each test.\n\nEndurance = cardiorespiratory endurance (i.e., 6-min-run test),\nCoordination = star-run test,\nSpeed = 20-m linear sprint test,\nPowerLOW = power of lower limbs (i.e., standing long jump test),\nPowerUP = power of upper limbs (i.e., ball push test),\nSD = standard deviation. Points are binned observed child means; lines are simple regression fits to the observations.\n\n\n\n3.3.2 Regression on age by Sex for each Test\nAnother set of relevant statistics are the slopes for the regression of performance on age for boys and girls in each of the five tests. The lines in Figures 1 and 2, however, are computed directly from the raw data with the linear() command.\n\ncombine(\n  groupby(df, [:Sex, :Test]),\n  [:age, :zScore] =&gt; simplelinreg =&gt; :coef,\n)\n\n10×3 DataFrame\n\n\n\nRow\nSex\nTest\ncoef\n\n\n\nCat…\nCat…\nTuple…\n\n\n\n\n1\nBoys\nEndurance\n(0.00256718, 0.0291899)\n\n\n2\nBoys\nCoordination\n(-2.47279, 0.302819)\n\n\n3\nBoys\nSpeed\n(-2.12689, 0.267153)\n\n\n4\nBoys\nPowerLOW\n(-1.4307, 0.189659)\n\n\n5\nBoys\nPowerUP\n(-4.35864, 0.549005)\n\n\n6\nGirls\nEndurance\n(-0.692022, 0.0523217)\n\n\n7\nGirls\nCoordination\n(-2.50524, 0.279119)\n\n\n8\nGirls\nSpeed\n(-2.34431, 0.255687)\n\n\n9\nGirls\nPowerLOW\n(-1.87241, 0.196917)\n\n\n10\nGirls\nPowerUP\n(-4.82271, 0.524799)\n\n\n\n\n\n\n\ncombine(\n  groupby(dat, [:Sex, :Test]),\n  [:age, :zScore] =&gt; simplelinreg =&gt; :coef,\n)\n\n10×3 DataFrame\n\n\n\nRow\nSex\nTest\ncoef\n\n\n\nCat…\nCat…\nTuple…\n\n\n\n\n1\nBoys\nEndurance\n(0.39203, -0.0150694)\n\n\n2\nBoys\nCoordination\n(-3.33518, 0.401051)\n\n\n3\nBoys\nSpeed\n(-1.75685, 0.228662)\n\n\n4\nBoys\nPowerLOW\n(-1.06646, 0.151546)\n\n\n5\nBoys\nPowerUP\n(-4.15536, 0.5245)\n\n\n6\nGirls\nEndurance\n(0.941712, -0.141158)\n\n\n7\nGirls\nCoordination\n(-0.681898, 0.0714891)\n\n\n8\nGirls\nSpeed\n(-0.786382, 0.0725931)\n\n\n9\nGirls\nPowerLOW\n(-0.208472, 0.00150731)\n\n\n10\nGirls\nPowerUP\n(-5.23593, 0.570806)"
  },
  {
    "objectID": "fggk21.html#seqdiffcoding-of-test",
    "href": "fggk21.html#seqdiffcoding-of-test",
    "title": "Basics with Emotikon Project",
    "section": "3.4 SeqDiffCoding of Test",
    "text": "3.4 SeqDiffCoding of Test\nSeqDiffCoding was used in the publication. This specification tests pairwise differences between the five neighboring levels of Test, that is:\n\nH1: Star_r - Run (2-1)\nH2: S20_r - Star_r (3-2)\nH3: SLJ - S20_r (4-3)\nH4: BPT - SLJ (5-4)\n\nThe levels were sorted such that these contrasts map onto four a priori hypotheses; in other words, they are theoretically motivated pairwise comparisons. The motivation also encompasses theoretically motivated interactions with Sex. The order of levels can also be explicitly specified during contrast construction. This is very useful if levels are in a different order in the dataframe.\nNote that random factors Child, School, and Cohort are declared as Grouping variables. Technically, this specification is required for variables with a very large number of levels (e.g., 100K+ children). We recommend the explicit specification for all random factors as a general coding style.\nThe first command recodes names indicating the physical fitness components used in the above figures and tables back to the shorter actual test names. This reduces clutter in LMM outputs.\n\nrecode!(\n  dat.Test,\n  \"Endurance\" =&gt; \"Run\",\n  \"Coordination\" =&gt; \"Star_r\",\n  \"Speed\" =&gt; \"S20_r\",\n  \"PowerLOW\" =&gt; \"SLJ\",\n  \"PowerUP\" =&gt; \"BMT\",\n)\ncontrasts = merge(\n  Dict(nm =&gt; SeqDiffCoding() for nm in (:Test, :Sex)),\n  Dict(nm =&gt; Grouping() for nm in (:Child, :School, :Cohort)),\n);\n\nThe statistical disadvantage of SeqDiffCoding is that the contrasts are not orthogonal, that is the contrasts are correlated. This is obvious from the fact that levels 2, 3, and 4 are all used in two contrasts. One consequence of this is that correlation parameters estimated between neighboring contrasts (e.g., 2-1 and 3-2) are difficult to interpret. Usually, they will be negative because assuming some practical limitations on the overall range (e.g., between levels 1 and 3), a small “2-1” effect “correlates” negatively with a larger “3-2” effect for mathematical reasons.\nObviously, the tradeoff between theoretical motivation and statistical purity is something that must be considered carefully when planning the analysis.\nVarious options for contrast coding are the topic of the MixedModelsTutorial_contrasts_emotikon.jl and MixedModelsTutorial_contrasts_kwdyz.jl notebooks."
  },
  {
    "objectID": "fggk21.html#lmm-m_ovi",
    "href": "fggk21.html#lmm-m_ovi",
    "title": "Basics with Emotikon Project",
    "section": "4.1 LMM m_ovi",
    "text": "4.1 LMM m_ovi\nIn its random-effect structure (RES) we only vary intercepts (i.e., Grand Means) for School (LMM m_ovi), that is we allow that the schools differ in the average fitness of its children, average over the five tests.\nIt is well known that such a simple RES is likely to be anti-conservative with respect to fixed-effect test statistics.\n\nm_ovi = let\n  f = @formula zScore ~ 1 + Test * Sex * a1 + (1 | School)\n  fit(MixedModel, f, dat; contrasts)\nend\n\nMinimizing 19    Time: 0:00:00 (15.92 ms/it)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEst.\nSE\nz\np\nσ_School\n\n\n\n\n(Intercept)\n-0.0174\n0.0198\n-0.88\n0.3787\n0.3417\n\n\nTest: Star_r\n-0.0026\n0.0303\n-0.09\n0.9304\n\n\n\nTest: S20_r\n0.0097\n0.0302\n0.32\n0.7469\n\n\n\nTest: SLJ\n0.0033\n0.0300\n0.11\n0.9119\n\n\n\nTest: BMT\n-0.0539\n0.0299\n-1.80\n0.0711\n\n\n\nSex: Girls\n-0.4372\n0.0205\n-21.35\n&lt;1e-99\n\n\n\na1\n0.1761\n0.0354\n4.97\n&lt;1e-06\n\n\n\nTest: Star_r & Sex: Girls\n0.3802\n0.0605\n6.28\n&lt;1e-09\n\n\n\nTest: S20_r & Sex: Girls\n-0.2038\n0.0604\n-3.38\n0.0007\n\n\n\nTest: SLJ & Sex: Girls\n-0.0669\n0.0600\n-1.11\n0.2651\n\n\n\nTest: BMT & Sex: Girls\n-0.2730\n0.0598\n-4.57\n&lt;1e-05\n\n\n\nTest: Star_r & a1\n0.3146\n0.1038\n3.03\n0.0024\n\n\n\nTest: S20_r & a1\n-0.0767\n0.1034\n-0.74\n0.4584\n\n\n\nTest: SLJ & a1\n-0.0745\n0.1027\n-0.73\n0.4683\n\n\n\nTest: BMT & a1\n0.4724\n0.1025\n4.61\n&lt;1e-05\n\n\n\nSex: Girls & a1\n-0.1044\n0.0709\n-1.47\n0.1405\n\n\n\nTest: Star_r & Sex: Girls & a1\n-0.1912\n0.2076\n-0.92\n0.3570\n\n\n\nTest: S20_r & Sex: Girls & a1\n0.1742\n0.2068\n0.84\n0.3997\n\n\n\nTest: SLJ & Sex: Girls & a1\n0.0084\n0.2054\n0.04\n0.9672\n\n\n\nTest: BMT & Sex: Girls & a1\n0.1923\n0.2050\n0.94\n0.3482\n\n\n\nResidual\n0.9152\n\n\n\n\n\n\n\n\n\nIs the model singular (overparameterized, degenerate)? In other words: Is the model not supported by the data?\n\nissingular(m_ovi)\n\nfalse\n\n\nModels varying only in intercepts are almost always supported by the data."
  },
  {
    "objectID": "fggk21.html#lmm-m_zcp",
    "href": "fggk21.html#lmm-m_zcp",
    "title": "Basics with Emotikon Project",
    "section": "4.2 LMM m_zcp",
    "text": "4.2 LMM m_zcp\nIn this LMM we allow that schools differ not only in GM, but also in the size of the four contrasts defined for Test, in the difference between boys and girls (Sex) and the developmental gain children achieve within the third grade (age).\nWe assume that there is covariance associated with these CPs beyond residual noise, that is we assume that there is no detectable evidence in the data that the CPs are different from zero.\n\nm_zcp = let\n  f = @formula(\n    zScore ~\n      1 + Test * Sex * a1 + zerocorr(1 + Test + Sex + a1 | School)\n  )\n  fit(MixedModel, f, dat; contrasts)\nend\n\nMinimizing 148    Time: 0:00:00 ( 0.95 ms/it)\n  objective:  25923.46046975058\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEst.\nSE\nz\np\nσ_School\n\n\n\n\n(Intercept)\n-0.0247\n0.0198\n-1.24\n0.2136\n0.3257\n\n\nTest: Star_r\n0.0029\n0.0311\n0.09\n0.9254\n0.2208\n\n\nTest: S20_r\n0.0102\n0.0286\n0.36\n0.7209\n0.0503\n\n\nTest: SLJ\n0.0049\n0.0283\n0.17\n0.8634\n0.0000\n\n\nTest: BMT\n-0.0460\n0.0309\n-1.49\n0.1368\n0.2286\n\n\nSex: Girls\n-0.4399\n0.0322\n-13.64\n&lt;1e-41\n0.4503\n\n\na1\n0.1840\n0.0573\n3.21\n0.0013\n0.7780\n\n\nTest: Star_r & Sex: Girls\n0.3784\n0.0577\n6.55\n&lt;1e-10\n\n\n\nTest: S20_r & Sex: Girls\n-0.2019\n0.0570\n-3.54\n0.0004\n\n\n\nTest: SLJ & Sex: Girls\n-0.0666\n0.0566\n-1.18\n0.2392\n\n\n\nTest: BMT & Sex: Girls\n-0.2780\n0.0570\n-4.87\n&lt;1e-05\n\n\n\nTest: Star_r & a1\n0.3101\n0.0992\n3.13\n0.0018\n\n\n\nTest: S20_r & a1\n-0.0728\n0.0976\n-0.75\n0.4558\n\n\n\nTest: SLJ & a1\n-0.0784\n0.0968\n-0.81\n0.4180\n\n\n\nTest: BMT & a1\n0.4741\n0.0979\n4.84\n&lt;1e-05\n\n\n\nSex: Girls & a1\n-0.1453\n0.0791\n-1.84\n0.0662\n\n\n\nTest: Star_r & Sex: Girls & a1\n-0.1570\n0.1982\n-0.79\n0.4284\n\n\n\nTest: S20_r & Sex: Girls & a1\n0.1799\n0.1952\n0.92\n0.3566\n\n\n\nTest: SLJ & Sex: Girls & a1\n0.0061\n0.1936\n0.03\n0.9749\n\n\n\nTest: BMT & Sex: Girls & a1\n0.1657\n0.1958\n0.85\n0.3974\n\n\n\nResidual\n0.8623\n\n\n\n\n\n\n\n\n\nDepending on sampling, this model estimating variance components for School may or may not be supported by the data.\n\nissingular(m_zcp)\n\ntrue"
  },
  {
    "objectID": "fggk21.html#lmm-m_cpx",
    "href": "fggk21.html#lmm-m_cpx",
    "title": "Basics with Emotikon Project",
    "section": "4.3 LMM m_cpx",
    "text": "4.3 LMM m_cpx\nIn the complex LMM investigated in this sequence we give up the assumption of zero-correlation between VCs.\n\nm_cpx = let\n  f = @formula(\n    zScore ~ 1 + Test * Sex * a1 + (1 + Test + Sex + a1 | School)\n  )\n  fit(MixedModel, f, dat; contrasts)\nend\n\nMinimizing 1908    Time: 0:00:01 ( 0.61 ms/it)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEst.\nSE\nz\np\nσ_School\n\n\n\n\n(Intercept)\n-0.0268\n0.0198\n-1.35\n0.1757\n0.3264\n\n\nTest: Star_r\n0.0056\n0.0310\n0.18\n0.8571\n0.2197\n\n\nTest: S20_r\n0.0104\n0.0301\n0.35\n0.7296\n0.1754\n\n\nTest: SLJ\n0.0024\n0.0294\n0.08\n0.9352\n0.1473\n\n\nTest: BMT\n-0.0402\n0.0305\n-1.32\n0.1875\n0.2234\n\n\nSex: Girls\n-0.4356\n0.0320\n-13.61\n&lt;1e-41\n0.4482\n\n\na1\n0.1877\n0.0566\n3.31\n0.0009\n0.7688\n\n\nTest: Star_r & Sex: Girls\n0.3759\n0.0575\n6.53\n&lt;1e-10\n\n\n\nTest: S20_r & Sex: Girls\n-0.1969\n0.0572\n-3.44\n0.0006\n\n\n\nTest: SLJ & Sex: Girls\n-0.0673\n0.0567\n-1.19\n0.2347\n\n\n\nTest: BMT & Sex: Girls\n-0.2717\n0.0566\n-4.80\n&lt;1e-05\n\n\n\nTest: Star_r & a1\n0.3113\n0.0988\n3.15\n0.0016\n\n\n\nTest: S20_r & a1\n-0.0707\n0.0981\n-0.72\n0.4709\n\n\n\nTest: SLJ & a1\n-0.0642\n0.0971\n-0.66\n0.5086\n\n\n\nTest: BMT & a1\n0.4678\n0.0971\n4.82\n&lt;1e-05\n\n\n\nSex: Girls & a1\n-0.1404\n0.0783\n-1.79\n0.0728\n\n\n\nTest: Star_r & Sex: Girls & a1\n-0.1634\n0.1974\n-0.83\n0.4080\n\n\n\nTest: S20_r & Sex: Girls & a1\n0.1863\n0.1961\n0.95\n0.3421\n\n\n\nTest: SLJ & Sex: Girls & a1\n0.0010\n0.1941\n0.01\n0.9960\n\n\n\nTest: BMT & Sex: Girls & a1\n0.1640\n0.1942\n0.84\n0.3982\n\n\n\nResidual\n0.8599\n\n\n\n\n\n\n\n\n\nWe also need to see the VCs and CPs of the random-effect structure (RES).\n\nVarCorr(m_cpx)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nColumn\nVariance\nStd.Dev\nCorr.\n\n\n\n\n\n\n\n\n\nSchool\n(Intercept)\n0.106520\n0.326374\n\n\n\n\n\n\n\n\n\nTest: Star_r\n0.048272\n0.219709\n+0.10\n\n\n\n\n\n\n\n\nTest: S20_r\n0.030768\n0.175408\n-0.19\n-0.21\n\n\n\n\n\n\n\nTest: SLJ\n0.021695\n0.147291\n-0.09\n+0.07\n-0.83\n\n\n\n\n\n\nTest: BMT\n0.049909\n0.223404\n-0.75\n+0.44\n-0.14\n+0.17\n\n\n\n\n\nSex: Girls\n0.200911\n0.448231\n-0.04\n+0.25\n+0.20\n-0.27\n-0.10\n\n\n\n\na1\n0.591056\n0.768802\n+0.02\n-0.13\n+0.03\n-0.49\n+0.17\n-0.06\n\n\nResidual\n\n0.739346\n0.859853\n\n\n\n\n\n\n\n\n\n\n\n\nissingular(m_cpx)\n\nfalse\n\n\nThe complex model may or may not be supported by the data."
  },
  {
    "objectID": "fggk21.html#model-comparisons",
    "href": "fggk21.html#model-comparisons",
    "title": "Basics with Emotikon Project",
    "section": "4.4 Model comparisons",
    "text": "4.4 Model comparisons\nThe checks of model singularity indicate that the three models are supported by the data. Does model complexification also increase the goodness of fit or are we only fitting noise?\n\n4.4.1 LRT and goodness-of-fit statistics\nAs the thee models are strictly hierarchically nested, we compare them with a likelihood-ratio tests (LRT) and AIC and BIC goodness-of-fit statistics derived from them.\n\nMixedModels.likelihoodratiotest(m_ovi, m_zcp, m_cpx)\n\n\n\n\n\n\n\n\n\n\n\n\n\nmodel-dof\ndeviance\nχ²\nχ²-dof\nP(&gt;χ²)\n\n\n\n\nzScore ~ 1 + Test + Sex + a1 + Test & Sex + Test & a1 + Sex & a1 + Test & Sex & a1 + (1 | School)\n22\n26273\n\n\n\n\n\nzScore ~ 1 + Test + Sex + a1 + Test & Sex + Test & a1 + Sex & a1 + Test & Sex & a1 + zerocorr(1 + Test + Sex + a1 | School)\n28\n25923\n349\n6\n&lt;1e-71\n\n\nzScore ~ 1 + Test + Sex + a1 + Test & Sex + Test & a1 + Sex & a1 + Test & Sex & a1 + (1 + Test + Sex + a1 | School)\n49\n25864\n59\n21\n&lt;1e-04\n\n\n\n\n\n\n\nCode\ngof_summary = let\n  nms = [:m_ovi, :m_zcp, :m_cpx]\n  mods = eval.(nms)\n  DataFrame(;\n    name=nms,\n    dof=dof.(mods),\n    deviance=deviance.(mods),\n    AIC=aic.(mods),\n    AICc=aicc.(mods),\n    BIC=bic.(mods),\n  )\nend\n\n\n3×6 DataFrame\n\n\n\nRow\nname\ndof\ndeviance\nAIC\nAICc\nBIC\n\n\n\nSymbol\nInt64\nFloat64\nFloat64\nFloat64\nFloat64\n\n\n\n\n1\nm_ovi\n22\n26272.9\n26316.9\n26317.0\n26474.8\n\n\n2\nm_zcp\n28\n25923.5\n25979.5\n25979.6\n26180.4\n\n\n3\nm_cpx\n49\n25864.4\n25962.4\n25962.9\n26314.0\n\n\n\n\n\n\nThese statistics will depend on sampling. In general, smaller deviance, AIC, and BIC indicate an improvement in goodness of fit. Usually, χ² should be larger than the associated degrees of freedom; for AIC and BIC the decrease should amount to more than 5, according to some literature. Severity of meeting these criteria increases from deviance to AIC to BIC. Therefore, it is not always the case that the criteria are unanimous in their verdict. Basically, the more confirmatory the analysis, the more one may go with deviance and AIC; for exploratory analyses the BIC is probably a better guide. There are grey zones here.\n\n\n4.4.2 Comparing fixed effects of m_ovi, m_zcp, and m_cpx\nWe check whether enriching the RES changed the significance of fixed effects in the final model.\n\n\nCode\nm_ovi_fe = DataFrame(coeftable(m_ovi));\nm_zcp_fe = DataFrame(coeftable(m_zcp));\nm_cpx_fe = DataFrame(coeftable(m_cpx));\nm_all = hcat(\n  m_ovi_fe[:, [1, 2, 4]],\n  leftjoin(\n    m_zcp_fe[:, [1, 2, 4]],\n    m_cpx_fe[:, [1, 2, 4]];\n    on=:Name,\n    makeunique=true,\n  );\n  makeunique=true,\n)\nrename!(\n  m_all,\n  \"Coef.\" =&gt; \"b_ovi\",\n  \"Coef._2\" =&gt; \"b_zcp\",\n  \"Coef._1\" =&gt; \"b_cpx\",\n  \"z\" =&gt; \"z_ovi\",\n  \"z_2\" =&gt; \"z_zcp\",\n  \"z_1\" =&gt; \"z_cpx\",\n)\nm_all2 =\n  round.(\n    m_all[:, [:b_ovi, :b_zcp, :b_cpx, :z_ovi, :z_zcp, :z_cpx]],\n    digits=2,\n  )\nm_all3 = hcat(m_all.Name, m_all2)\n\n\n20×7 DataFrame\n\n\n\nRow\nx1\nb_ovi\nb_zcp\nb_cpx\nz_ovi\nz_zcp\nz_cpx\n\n\n\nString\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\n\n\n\n\n1\n(Intercept)\n-0.02\n-0.02\n-0.03\n-0.88\n-1.24\n-1.35\n\n\n2\nTest: Star_r\n-0.0\n0.0\n0.01\n-0.09\n0.09\n0.18\n\n\n3\nTest: S20_r\n0.01\n0.01\n0.01\n0.32\n0.36\n0.35\n\n\n4\nTest: SLJ\n0.0\n0.0\n0.0\n0.11\n0.17\n0.08\n\n\n5\nTest: BMT\n-0.05\n-0.05\n-0.04\n-1.8\n-1.49\n-1.32\n\n\n6\nSex: Girls\n-0.44\n-0.44\n-0.44\n-21.35\n-13.64\n-13.61\n\n\n7\na1\n0.18\n0.18\n0.19\n4.97\n3.21\n3.31\n\n\n8\nTest: Star_r & Sex: Girls\n0.38\n0.38\n0.38\n6.28\n6.55\n6.53\n\n\n9\nTest: S20_r & Sex: Girls\n-0.2\n-0.2\n-0.2\n-3.38\n-3.54\n-3.44\n\n\n10\nTest: SLJ & Sex: Girls\n-0.07\n-0.07\n-0.07\n-1.11\n-1.18\n-1.19\n\n\n11\nTest: BMT & Sex: Girls\n-0.27\n-0.28\n-0.27\n-4.57\n-4.87\n-4.8\n\n\n12\nTest: Star_r & a1\n0.31\n0.31\n0.31\n3.03\n3.13\n3.15\n\n\n13\nTest: S20_r & a1\n-0.08\n-0.07\n-0.07\n-0.74\n-0.75\n-0.72\n\n\n14\nTest: SLJ & a1\n-0.07\n-0.08\n-0.06\n-0.73\n-0.81\n-0.66\n\n\n15\nTest: BMT & a1\n0.47\n0.47\n0.47\n4.61\n4.84\n4.82\n\n\n16\nSex: Girls & a1\n-0.1\n-0.15\n-0.14\n-1.47\n-1.84\n-1.79\n\n\n17\nTest: Star_r & Sex: Girls & a1\n-0.19\n-0.16\n-0.16\n-0.92\n-0.79\n-0.83\n\n\n18\nTest: S20_r & Sex: Girls & a1\n0.17\n0.18\n0.19\n0.84\n0.92\n0.95\n\n\n19\nTest: SLJ & Sex: Girls & a1\n0.01\n0.01\n0.0\n0.04\n0.03\n0.01\n\n\n20\nTest: BMT & Sex: Girls & a1\n0.19\n0.17\n0.16\n0.94\n0.85\n0.84\n\n\n\n\n\n\nThe three models usually do not differ in fixed-effect estimates. For main effects of age and Sex, z-values decrease strongly with the complexity of the model (i.e., standard errors are larger). For other coefficients, the changes are not very large and not consistent.\nIn general, dropping significant variance components and/or correlation parameters may lead to anti-conservative estimates of fixed effects (e.g., Schielzeth & Forstmeier, 2008). Basically, some of the variance allocated to age and Sex in LMM m_ovi could also be due to differences between schools. This ambiguity increased the uncertainty of the respective fixed effects in the other two LMMs."
  },
  {
    "objectID": "fggk21.html#fitting-an-overparameterized-lmm",
    "href": "fggk21.html#fitting-an-overparameterized-lmm",
    "title": "Basics with Emotikon Project",
    "section": "4.5 Fitting an overparameterized LMM",
    "text": "4.5 Fitting an overparameterized LMM\nThe complex LMM was not overparameterized with respect to School, because there are over 400 schools in the data. When the number of units (levels) of a grouping factor is small relative to the number of parameters we are trying to estimate, we often end up with an overparameterized / degenerate random-effect structure.\nAs an illustration, we fit a full CP matrix for the Cohort. As there are only nine cohorts in the data, we may be asking too much to estimate 5*6/2 = 15 VC/CP parameters.\n\nm_cpxCohort = let\n  f = @formula zScore ~ 1 + Test * a1 * Sex + (1 + Test | Cohort)\n  fit(MixedModel, f, dat; contrasts)\nend\n\n\n\n\n\n\n\n\n\n\n\n\n\nEst.\nSE\nz\np\nσ_Cohort\n\n\n\n\n(Intercept)\n-0.0009\n0.0161\n-0.06\n0.9548\n0.0378\n\n\nTest: Star_r\n-0.0073\n0.0380\n-0.19\n0.8482\n0.0614\n\n\nTest: S20_r\n0.0056\n0.0383\n0.15\n0.8845\n0.0636\n\n\nTest: SLJ\n0.0101\n0.0451\n0.22\n0.8226\n0.0958\n\n\nTest: BMT\n-0.0556\n0.0335\n-1.66\n0.0968\n0.0330\n\n\na1\n0.2055\n0.0349\n5.90\n&lt;1e-08\n\n\n\nSex: Girls\n-0.4238\n0.0201\n-21.07\n&lt;1e-97\n\n\n\nTest: Star_r & a1\n0.2849\n0.1101\n2.59\n0.0097\n\n\n\nTest: S20_r & a1\n-0.1172\n0.1096\n-1.07\n0.2851\n\n\n\nTest: SLJ & a1\n-0.0270\n0.1094\n-0.25\n0.8048\n\n\n\nTest: BMT & a1\n0.4555\n0.1084\n4.20\n&lt;1e-04\n\n\n\nTest: Star_r & Sex: Girls\n0.3700\n0.0640\n5.78\n&lt;1e-08\n\n\n\nTest: S20_r & Sex: Girls\n-0.2116\n0.0638\n-3.32\n0.0009\n\n\n\nTest: SLJ & Sex: Girls\n-0.0552\n0.0634\n-0.87\n0.3843\n\n\n\nTest: BMT & Sex: Girls\n-0.2718\n0.0632\n-4.30\n&lt;1e-04\n\n\n\na1 & Sex: Girls\n-0.1368\n0.0690\n-1.98\n0.0473\n\n\n\nTest: Star_r & a1 & Sex: Girls\n-0.2099\n0.2194\n-0.96\n0.3387\n\n\n\nTest: S20_r & a1 & Sex: Girls\n0.1609\n0.2186\n0.74\n0.4615\n\n\n\nTest: SLJ & a1 & Sex: Girls\n0.0225\n0.2172\n0.10\n0.9174\n\n\n\nTest: BMT & a1 & Sex: Girls\n0.1901\n0.2166\n0.88\n0.3801\n\n\n\nResidual\n0.9671\n\n\n\n\n\n\n\n\n\n\nVarCorr(m_cpxCohort)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nColumn\nVariance\nStd.Dev\nCorr.\n\n\n\n\n\n\n\nCohort\n(Intercept)\n0.0014274\n0.0377813\n\n\n\n\n\n\n\nTest: Star_r\n0.0037641\n0.0613525\n-0.90\n\n\n\n\n\n\nTest: S20_r\n0.0040428\n0.0635833\n-1.00\n+0.87\n\n\n\n\n\nTest: SLJ\n0.0091843\n0.0958347\n+0.98\n-0.97\n-0.97\n\n\n\n\nTest: BMT\n0.0010873\n0.0329747\n-1.00\n+0.91\n+0.99\n-0.99\n\n\nResidual\n\n0.9353141\n0.9671164\n\n\n\n\n\n\n\n\n\n\nissingular(m_cpxCohort)\n\ntrue\n\n\nThe model is overparameterized with several CPs estimated between |.98| and |1.00|. How about the zero-correlation parameter (zcp) version of this LMM?\n\nm_zcpCohort = let\n  f = @formula(\n    zScore ~ 1 + Test * a1 * Sex + zerocorr(1 + Test | Cohort)\n  )\n  fit(MixedModel, f, dat; contrasts)\nend\n\n\n\n\n\n\n\n\n\n\n\n\n\nEst.\nSE\nz\np\nσ_Cohort\n\n\n\n\n(Intercept)\n-0.0022\n0.0152\n-0.15\n0.8837\n0.0341\n\n\nTest: Star_r\n-0.0042\n0.0339\n-0.12\n0.9023\n0.0331\n\n\nTest: S20_r\n0.0088\n0.0319\n0.27\n0.7837\n0.0000\n\n\nTest: SLJ\n0.0045\n0.0317\n0.14\n0.8876\n0.0000\n\n\nTest: BMT\n-0.0536\n0.0316\n-1.69\n0.0903\n0.0000\n\n\na1\n0.1999\n0.0351\n5.70\n&lt;1e-07\n\n\n\nSex: Girls\n-0.4245\n0.0201\n-21.08\n&lt;1e-97\n\n\n\nTest: Star_r & a1\n0.3078\n0.1101\n2.80\n0.0052\n\n\n\nTest: S20_r & a1\n-0.0849\n0.1093\n-0.78\n0.4377\n\n\n\nTest: SLJ & a1\n-0.0748\n0.1086\n-0.69\n0.4911\n\n\n\nTest: BMT & a1\n0.4717\n0.1084\n4.35\n&lt;1e-04\n\n\n\nTest: Star_r & Sex: Girls\n0.3729\n0.0640\n5.82\n&lt;1e-08\n\n\n\nTest: S20_r & Sex: Girls\n-0.2079\n0.0638\n-3.26\n0.0011\n\n\n\nTest: SLJ & Sex: Girls\n-0.0611\n0.0634\n-0.96\n0.3354\n\n\n\nTest: BMT & Sex: Girls\n-0.2697\n0.0632\n-4.27\n&lt;1e-04\n\n\n\na1 & Sex: Girls\n-0.1372\n0.0691\n-1.99\n0.0470\n\n\n\nTest: Star_r & a1 & Sex: Girls\n-0.2041\n0.2195\n-0.93\n0.3525\n\n\n\nTest: S20_r & a1 & Sex: Girls\n0.1733\n0.2187\n0.79\n0.4280\n\n\n\nTest: SLJ & a1 & Sex: Girls\n0.0051\n0.2172\n0.02\n0.9811\n\n\n\nTest: BMT & a1 & Sex: Girls\n0.1962\n0.2168\n0.90\n0.3655\n\n\n\nResidual\n0.9680\n\n\n\n\n\n\n\n\n\n\nissingular(m_zcpCohort)\n\ntrue\n\n\nThis zcpLMM is also singular. Three of the five VCs are estimated as zero. This raises the possibility that LMM m_oviCohort might fit as well as LMM m_zcpCohort.\n\nm_oviCohort = let\n  f = @formula zScore ~ 1 + Test * a1 * Sex + (1 | Cohort)\n  fit(MixedModel, f, dat; contrasts)\nend\n\n\n\n\n\n\n\n\n\n\n\n\n\nEst.\nSE\nz\np\nσ_Cohort\n\n\n\n\n(Intercept)\n-0.0022\n0.0152\n-0.15\n0.8835\n0.0340\n\n\nTest: Star_r\n-0.0032\n0.0320\n-0.10\n0.9196\n\n\n\nTest: S20_r\n0.0087\n0.0319\n0.27\n0.7847\n\n\n\nTest: SLJ\n0.0045\n0.0317\n0.14\n0.8869\n\n\n\nTest: BMT\n-0.0536\n0.0316\n-1.69\n0.0903\n\n\n\na1\n0.1999\n0.0351\n5.69\n&lt;1e-07\n\n\n\nSex: Girls\n-0.4245\n0.0201\n-21.08\n&lt;1e-97\n\n\n\nTest: Star_r & a1\n0.3135\n0.1097\n2.86\n0.0043\n\n\n\nTest: S20_r & a1\n-0.0848\n0.1093\n-0.78\n0.4378\n\n\n\nTest: SLJ & a1\n-0.0748\n0.1086\n-0.69\n0.4910\n\n\n\nTest: BMT & a1\n0.4718\n0.1084\n4.35\n&lt;1e-04\n\n\n\nTest: Star_r & Sex: Girls\n0.3736\n0.0640\n5.84\n&lt;1e-08\n\n\n\nTest: S20_r & Sex: Girls\n-0.2079\n0.0638\n-3.26\n0.0011\n\n\n\nTest: SLJ & Sex: Girls\n-0.0611\n0.0634\n-0.96\n0.3356\n\n\n\nTest: BMT & Sex: Girls\n-0.2697\n0.0632\n-4.27\n&lt;1e-04\n\n\n\na1 & Sex: Girls\n-0.1371\n0.0691\n-1.99\n0.0471\n\n\n\nTest: Star_r & a1 & Sex: Girls\n-0.2022\n0.2195\n-0.92\n0.3568\n\n\n\nTest: S20_r & a1 & Sex: Girls\n0.1733\n0.2187\n0.79\n0.4281\n\n\n\nTest: SLJ & a1 & Sex: Girls\n0.0051\n0.2172\n0.02\n0.9811\n\n\n\nTest: BMT & a1 & Sex: Girls\n0.1961\n0.2168\n0.90\n0.3657\n\n\n\nResidual\n0.9681\n\n\n\n\n\n\n\n\n\n\nissingular(m_oviCohort)\n\nfalse\n\n\nThis solves the problem with singularity, but does LMM m_zcpCohort fit noise relative to the LMM m_oviCohort?\n\nMixedModels.likelihoodratiotest(m_oviCohort, m_zcpCohort)\n\n\n\n\n\n\n\n\n\n\n\n\n\nmodel-dof\ndeviance\nχ²\nχ²-dof\nP(&gt;χ²)\n\n\n\n\nzScore ~ 1 + Test + a1 + Sex + Test & a1 + Test & Sex + a1 & Sex + Test & a1 & Sex + (1 | Cohort)\n22\n26803\n\n\n\n\n\nzScore ~ 1 + Test + a1 + Sex + Test & a1 + Test & Sex + a1 & Sex + Test & a1 & Sex + zerocorr(1 + Test | Cohort)\n26\n26803\n0\n4\n0.9968\n\n\n\n\n\n\ngof_summary2 = let\n  mods = [m_oviCohort, m_zcpCohort, m_cpxCohort]\n  DataFrame(;\n    dof=dof.(mods),\n    deviance=deviance.(mods),\n    AIC=aic.(mods),\n    AICc=aicc.(mods),\n    BIC=bic.(mods),\n  )\nend\n\n3×5 DataFrame\n\n\n\nRow\ndof\ndeviance\nAIC\nAICc\nBIC\n\n\n\nInt64\nFloat64\nFloat64\nFloat64\nFloat64\n\n\n\n\n1\n22\n26802.9\n26846.9\n26847.0\n27004.7\n\n\n2\n26\n26802.7\n26854.7\n26854.8\n27041.3\n\n\n3\n36\n26790.5\n26862.5\n26862.7\n27120.8\n\n\n\n\n\n\nIndeed, adding VCs is fitting noise. Again, the goodness of fit statistics unanimously favor the selection of the LMM m_oviCohort.\nNot shown here, but the Cohort-related VCs for the Test contrasts could be estimated reliably for the full data. Thus, the small number of cohorts does not necessarily prevent the determination of reliable differences between tests across cohorts. What if we include VCs and CPs related to random factors Child and School?"
  },
  {
    "objectID": "fggk21.html#fitting-the-published-lmm-m1-to-the-reduced-data",
    "href": "fggk21.html#fitting-the-published-lmm-m1-to-the-reduced-data",
    "title": "Basics with Emotikon Project",
    "section": "4.6 Fitting the published LMM m1 to the reduced data",
    "text": "4.6 Fitting the published LMM m1 to the reduced data\n\n\n\n\n\n\nWarning\n\n\n\nThe following LMMs m1, m2, etc. take a bit longer (e.g., close to 6 minutes in the Pluto notebook, close to 3 minutes in the REPL on a MacBook Pro).\n\n\nLMM m1 reported in Fühner et al. (2021) included random factors for School, Child, and Cohort. The RES for School was specified like in LMM m_cpx. The RES for Child included VCs and CPs for Test, but not for linear developmental gain in the ninth year of life a1 or Sex; they are between-Child effects.\nThe RES for Cohort included only VCs, no CPs for Test. The parsimony was due to the small number of nine levels for this grouping factor.\nHere we fit this LMM m1 for the reduced data. For a different subset of similar size on MacBook Pro [13 | 15 | 16] this took [303 | 250 | 244 ] s; for LMM m1a (i.e., dropping 1 school-relate VC for Sex), times are [212 | 165 | 160] s. The corresponding lme4 times for LMM m1 are [397 | 348 | 195].\nFinally, times for fitting the full set of data –not in this script–, for LMM m1are [60 | 62 | 85] minutes (!); for LMM m1a the times were [46 | 48 | 34] minutes. It was not possible to fit the full set of data with lme4; after about 13 to 18 minutes the program stopped with: Error in eval_f(x, ...) : Downdated VtV is not positive definite.\n\nm1 = let\n  f = @formula(\n    zScore ~\n      1 +\n      Test * a1 * Sex +\n      (1 + Test + a1 + Sex | School) +\n      (1 + Test | Child) +\n      zerocorr(1 + Test | Cohort)\n  )\n  fit(MixedModel, f, dat; contrasts)\nend\n\nMinimizing 2045    Time: 0:00:24 (12.06 ms/it)\n  objective:  24651.013732728963\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEst.\nSE\nz\np\nσ_Child\nσ_School\nσ_Cohort\n\n\n\n\n(Intercept)\n-0.0133\n0.0192\n-0.69\n0.4879\n0.5954\n0.2137\n0.0132\n\n\nTest: Star_r\n0.0088\n0.0375\n0.24\n0.8135\n0.8049\n0.3329\n0.0637\n\n\nTest: S20_r\n0.0092\n0.0292\n0.31\n0.7533\n0.7052\n0.3281\n0.0000\n\n\nTest: SLJ\n0.0034\n0.0299\n0.11\n0.9108\n0.6238\n0.3212\n0.0336\n\n\nTest: BMT\n-0.0383\n0.0296\n-1.29\n0.1955\n0.7845\n0.3131\n0.0000\n\n\na1\n0.1956\n0.0545\n3.59\n0.0003\n\n0.2824\n\n\n\nSex: Girls\n-0.4287\n0.0312\n-13.75\n&lt;1e-42\n\n0.1437\n\n\n\nTest: Star_r & a1\n0.2897\n0.0887\n3.26\n0.0011\n\n\n\n\n\nTest: S20_r & a1\n-0.0780\n0.0817\n-0.96\n0.3394\n\n\n\n\n\nTest: SLJ & a1\n-0.0633\n0.0769\n-0.82\n0.4105\n\n\n\n\n\nTest: BMT & a1\n0.4713\n0.0851\n5.54\n&lt;1e-07\n\n\n\n\n\nTest: Star_r & Sex: Girls\n0.3756\n0.0511\n7.35\n&lt;1e-12\n\n\n\n\n\nTest: S20_r & Sex: Girls\n-0.1867\n0.0475\n-3.93\n&lt;1e-04\n\n\n\n\n\nTest: SLJ & Sex: Girls\n-0.0695\n0.0445\n-1.56\n0.1184\n\n\n\n\n\nTest: BMT & Sex: Girls\n-0.2812\n0.0495\n-5.68\n&lt;1e-07\n\n\n\n\n\na1 & Sex: Girls\n-0.1267\n0.1048\n-1.21\n0.2270\n\n\n\n\n\nTest: Star_r & a1 & Sex: Girls\n-0.1383\n0.1756\n-0.79\n0.4310\n\n\n\n\n\nTest: S20_r & a1 & Sex: Girls\n0.1754\n0.1633\n1.07\n0.2828\n\n\n\n\n\nTest: SLJ & a1 & Sex: Girls\n-0.0154\n0.1530\n-0.10\n0.9196\n\n\n\n\n\nTest: BMT & a1 & Sex: Girls\n0.1548\n0.1702\n0.91\n0.3631\n\n\n\n\n\nResidual\n0.4864\n\n\n\n\n\n\n\n\n\n\n\n\nVarCorr(m1)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nColumn\nVariance\nStd.Dev\nCorr.\n\n\n\n\n\n\n\n\n\nChild\n(Intercept)\n0.3544881\n0.5953890\n\n\n\n\n\n\n\n\n\nTest: Star_r\n0.6479230\n0.8049366\n+0.13\n\n\n\n\n\n\n\n\nTest: S20_r\n0.4973485\n0.7052294\n+0.00\n-0.52\n\n\n\n\n\n\n\nTest: SLJ\n0.3891358\n0.6238075\n+0.04\n-0.03\n-0.39\n\n\n\n\n\n\nTest: BMT\n0.6154675\n0.7845173\n-0.31\n+0.11\n-0.15\n-0.28\n\n\n\n\nSchool\n(Intercept)\n0.0456848\n0.2137401\n\n\n\n\n\n\n\n\n\nTest: Star_r\n0.1108304\n0.3329121\n-0.06\n\n\n\n\n\n\n\n\nTest: S20_r\n0.1076800\n0.3281462\n-0.08\n-0.39\n\n\n\n\n\n\n\nTest: SLJ\n0.1031693\n0.3211997\n-0.19\n+0.21\n-0.80\n\n\n\n\n\n\nTest: BMT\n0.0980019\n0.3130526\n-0.33\n-0.02\n+0.13\n-0.38\n\n\n\n\n\na1\n0.0797394\n0.2823816\n+0.62\n-0.20\n+0.11\n-0.61\n+0.45\n\n\n\n\nSex: Girls\n0.0206539\n0.1437147\n-0.45\n+0.45\n+0.31\n-0.27\n-0.15\n-0.37\n\n\nCohort\n(Intercept)\n0.0001745\n0.0132087\n\n\n\n\n\n\n\n\n\nTest: Star_r\n0.0040625\n0.0637377\n.\n\n\n\n\n\n\n\n\nTest: S20_r\n0.0000000\n0.0000000\n.\n.\n\n\n\n\n\n\n\nTest: SLJ\n0.0011298\n0.0336125\n.\n.\n.\n\n\n\n\n\n\nTest: BMT\n0.0000000\n0.0000000\n.\n.\n.\n.\n\n\n\n\nResidual\n\n0.2366260\n0.4864422\n\n\n\n\n\n\n\n\n\n\n\n\nissingular(m1)\n\ntrue\n\n\nDepending on the random number for stratified samplign, LMM m1 may or may not be supported by the data.\nWe also fit an alternative parameterization, estimating VCs and CPs for Test scores rather than Test effects by replacing the 1 + ... in the RE terms with 0 + ....\n\nm2 = let\n  f = @formula(\n    zScore ~\n      1 +\n      Test * a1 * Sex +\n      (0 + Test + a1 + Sex | School) +\n      (0 + Test | Child) +\n      zerocorr(0 + Test | Cohort)\n  )\n  fit(MixedModel, f, dat; contrasts)\nend\n\nMinimizing 1637    Time: 0:00:17 (10.95 ms/it)\n  objective:  24646.871983646208\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEst.\nSE\nz\np\nσ_Child\nσ_School\nσ_Cohort\n\n\n\n\n(Intercept)\n-0.0136\n0.0195\n-0.70\n0.4844\n\n\n\n\n\nTest: Star_r\n0.0093\n0.0365\n0.25\n0.7987\n0.7949\n0.3103\n0.0208\n\n\nTest: S20_r\n0.0060\n0.0354\n0.17\n0.8653\n0.7654\n0.3305\n0.0563\n\n\nTest: SLJ\n0.0052\n0.0339\n0.15\n0.8786\n0.7783\n0.2478\n0.0158\n\n\nTest: BMT\n-0.0388\n0.0300\n-1.29\n0.1967\n0.6945\n0.1968\n0.0000\n\n\na1\n0.1935\n0.0545\n3.55\n0.0004\n\n0.2831\n\n\n\nSex: Girls\n-0.4290\n0.0312\n-13.75\n&lt;1e-42\n\n0.1433\n\n\n\nTest: Star_r & a1\n0.2932\n0.0887\n3.31\n0.0009\n\n\n\n\n\nTest: S20_r & a1\n-0.1007\n0.0826\n-1.22\n0.2228\n\n\n\n\n\nTest: SLJ & a1\n-0.0456\n0.0773\n-0.59\n0.5551\n\n\n\n\n\nTest: BMT & a1\n0.4685\n0.0853\n5.50\n&lt;1e-07\n\n\n\n\n\nTest: Star_r & Sex: Girls\n0.3759\n0.0511\n7.36\n&lt;1e-12\n\n\n\n\n\nTest: S20_r & Sex: Girls\n-0.1892\n0.0475\n-3.98\n&lt;1e-04\n\n\n\n\n\nTest: SLJ & Sex: Girls\n-0.0680\n0.0445\n-1.53\n0.1262\n\n\n\n\n\nTest: BMT & Sex: Girls\n-0.2815\n0.0495\n-5.68\n&lt;1e-07\n\n\n\n\n\na1 & Sex: Girls\n-0.1261\n0.1049\n-1.20\n0.2291\n\n\n\n\n\nTest: Star_r & a1 & Sex: Girls\n-0.1371\n0.1756\n-0.78\n0.4348\n\n\n\n\n\nTest: S20_r & a1 & Sex: Girls\n0.1720\n0.1634\n1.05\n0.2926\n\n\n\n\n\nTest: SLJ & a1 & Sex: Girls\n-0.0125\n0.1529\n-0.08\n0.9348\n\n\n\n\n\nTest: BMT & a1 & Sex: Girls\n0.1542\n0.1702\n0.91\n0.3648\n\n\n\n\n\nTest: Run\n\n\n\n\n0.7477\n0.3702\n0.0549\n\n\nResidual\n0.5153\n\n\n\n\n\n\n\n\n\n\n\n\nissingular(m2)\n\ntrue\n\n\nDepending on the random number generator seed, the model may or may not be supported in the alternative parameterization of scores. The fixed-effects profile is not affected (see 2.8 below).\n\n\n\n\n\n\nCaution\n\n\n\nRK: The order of RE terms is critical. In formula f2 the zerocorr() term must be placed last as shown. If it is placed first, School-related and Child-related CPs are estimated/reported (?) as zero. This was not the case for formula m1. Thus, it appears to be related to the 0-intercepts in School and Child terms. Need a reprex.\n\n\n\nVarCorr(m2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nColumn\nVariance\nStd.Dev\nCorr.\n\n\n\n\n\n\n\n\n\nChild\nTest: Run\n0.5590442\n0.7476926\n\n\n\n\n\n\n\n\n\nTest: Star_r\n0.6319425\n0.7949481\n+0.51\n\n\n\n\n\n\n\n\nTest: S20_r\n0.5859061\n0.7654450\n+0.57\n+0.64\n\n\n\n\n\n\n\nTest: SLJ\n0.6057411\n0.7782937\n+0.56\n+0.60\n+0.72\n\n\n\n\n\n\nTest: BMT\n0.4822738\n0.6944594\n+0.20\n+0.41\n+0.37\n+0.49\n\n\n\n\nSchool\nTest: Run\n0.1370711\n0.3702311\n\n\n\n\n\n\n\n\n\nTest: Star_r\n0.0963028\n0.3103270\n+0.53\n\n\n\n\n\n\n\n\nTest: S20_r\n0.1092553\n0.3305378\n+0.47\n+0.48\n\n\n\n\n\n\n\nTest: SLJ\n0.0614022\n0.2477947\n+0.47\n+0.76\n+0.41\n\n\n\n\n\n\nTest: BMT\n0.0387320\n0.1968044\n+0.15\n+0.38\n+0.18\n+0.02\n\n\n\n\n\na1\n0.0801214\n0.2830572\n+0.58\n+0.47\n+0.56\n-0.05\n+0.65\n\n\n\n\nSex: Girls\n0.0205483\n0.1433468\n-0.63\n-0.27\n+0.06\n-0.28\n-0.58\n-0.37\n\n\nCohort\nTest: Run\n0.0030109\n0.0548716\n\n\n\n\n\n\n\n\n\nTest: Star_r\n0.0004327\n0.0208005\n.\n\n\n\n\n\n\n\n\nTest: S20_r\n0.0031715\n0.0563158\n.\n.\n\n\n\n\n\n\n\nTest: SLJ\n0.0002484\n0.0157616\n.\n.\n.\n\n\n\n\n\n\nTest: BMT\n0.0000000\n0.0000000\n.\n.\n.\n.\n\n\n\n\nResidual\n\n0.2654901\n0.5152574"
  },
  {
    "objectID": "fggk21.html#principle-component-analysis-of-random-effect-structure-repca",
    "href": "fggk21.html#principle-component-analysis-of-random-effect-structure-repca",
    "title": "Basics with Emotikon Project",
    "section": "4.7 Principle Component Analysis of Random Effect Structure (rePCA)",
    "text": "4.7 Principle Component Analysis of Random Effect Structure (rePCA)\nThe ìssingular() command is sort of a shortcut for a quick inspection of the principle components (PCs) of the variance-covariance matrix of the RES. With the MixedModels.PCA() command, we also obtain information about the amount of cumulative variance accounted for as we add PCs.\nThe output also provides PC loadings which may facilitate interpretation of the CP matrices (if estimated). This topic will be picked uo in a separate vignette. See also Fühner et al. (2021) for an application."
  },
  {
    "objectID": "fggk21.html#effects-in-res",
    "href": "fggk21.html#effects-in-res",
    "title": "Basics with Emotikon Project",
    "section": "4.8 Effects in RES",
    "text": "4.8 Effects in RES\nFor every random factor, MixedModels.PCA() extracts as many PCs as there are VCs. Therefore, the cumulation of variance across PCs within a random factor will always add up to 100% – at the latest with the last VC, but, in the case of overparameterized LMMs, the ceiling will be reached earlier. The final PCs are usually quite small.\nPCs are extracted in the order of the amount of unique variance they account for. The first PC accounts for the largest and the final PC for the least amount of variance. The number the PCs with percent variance above a certain threshold indicates the number of weighted composites needed and reflects the dimensionality of the orthogonal space within which (almost) all the variance can be accounted for. The weights for forming composite scores are the listed loadings. For ease of interpretation it is often useful to change the sign of some composite scores.\nThe PCA for LMM m1 shows that each of the five PCs for Child accounts for a non-zero percent of unique variance.\nFor School fewer than seven PCs have unique variance. The exact number depends on sampling. The overparameterization of School might be resolved when the CPs for Sex are dropped from the LMM.\nCohort was estimated with CPs forced to zero. Therefore, the VCs were forced to be orthogonal; they already represent the PCA solution. However, depending on sampling, not all PCs may be identified for this random factor either.\nImportantly, again depending on sampling, a non-singular fit does not imply that unique variance is associated with all PCs (i.e., not for last PC for School). Embrace uncertainty!\n\nMixedModels.PCA(m1)\n\n(Child = \nPrincipal components based on correlation matrix\n (Intercept)    1.0     .      .      .      .\n Test: Star_r   0.13   1.0     .      .      .\n Test: S20_r    0.0   -0.52   1.0     .      .\n Test: SLJ      0.04  -0.03  -0.39   1.0     .\n Test: BMT     -0.31   0.11  -0.15  -0.28   1.0\n\nNormalized cumulative variances:\n[0.3286, 0.6156, 0.8279, 0.941, 1.0]\n\nComponent loadings\n                 PC1    PC2    PC3    PC4    PC5\n (Intercept)   -0.12   0.5    0.64  -0.57  -0.0\n Test: Star_r  -0.58  -0.19   0.45   0.44   0.48\n Test: S20_r    0.71   0.1    0.14   0.1    0.68\n Test: SLJ     -0.38   0.43  -0.61  -0.23   0.49\n Test: BMT     -0.05  -0.72  -0.02  -0.64   0.26, School = \nPrincipal components based on correlation matrix\n (Intercept)    1.0     .      .      .      .      .      .\n Test: Star_r  -0.06   1.0     .      .      .      .      .\n Test: S20_r   -0.08  -0.39   1.0     .      .      .      .\n Test: SLJ     -0.19   0.21  -0.8    1.0     .      .      .\n Test: BMT     -0.33  -0.02   0.13  -0.38   1.0     .      .\n a1             0.62  -0.2    0.11  -0.61   0.45   1.0     .\n Sex: Girls    -0.45   0.45   0.31  -0.27  -0.15  -0.37   1.0\n\nNormalized cumulative variances:\n[0.357, 0.6352, 0.806, 0.9687, 1.0, 1.0, 1.0]\n\nComponent loadings\n                 PC1    PC2    PC3    PC4    PC5    PC6    PC7\n (Intercept)   -0.25  -0.49  -0.42  -0.36   0.18   0.53   0.27\n Test: Star_r   0.31   0.12   0.16  -0.76   0.47  -0.25  -0.09\n Test: S20_r   -0.4    0.44  -0.31   0.21   0.52  -0.28   0.4\n Test: SLJ      0.55  -0.31   0.11   0.19   0.03  -0.18   0.72\n Test: BMT     -0.29   0.14   0.79  -0.01   0.15   0.42   0.27\n a1            -0.52  -0.27   0.16  -0.32  -0.41  -0.56   0.22\n Sex: Girls     0.14   0.6   -0.2   -0.34  -0.53   0.24   0.35, Cohort = \nPrincipal components based on correlation matrix\n (Intercept)   1.0  .    .    .    .\n Test: Star_r  0.0  1.0  .    .    .\n Test: S20_r   0.0  0.0  0.0  .    .\n Test: SLJ     0.0  0.0  0.0  1.0  .\n Test: BMT     0.0  0.0  0.0  0.0  0.0\n\nNormalized cumulative variances:\n[0.3333, 0.6667, 1.0, 1.0, 1.0]\n\nComponent loadings\n                PC1   PC2   PC3     PC4     PC5\n (Intercept)   1.0   0.0   0.0     0.0     0.0\n Test: Star_r  0.0   1.0   0.0     0.0     0.0\n Test: S20_r   0.0   0.0   0.0   NaN       0.0\n Test: SLJ     0.0   0.0   1.0     0.0     0.0\n Test: BMT     0.0   0.0   0.0     0.0   NaN)\n\n\n\n4.8.1 Scores in RES\nNow lets looks at the PCA results for the alternative parameterization of LMM m2. It is important to note that the reparameterization to base estimates of VCs and CPs on scores rather than effects applies only to the Test factor (i.e., the first factor in the formula); VCs for Sex and age refer to the associated effects.\nDepending on sampling, the difference between LMM m1 and LMM m2 may show that overparameterization according to PCs may depend on the specification chosen for the other the random-effect structure.\n\n\n\n\n\n\nNote\n\n\n\nFor the complete data, all PCs had unique variance associated with them.\n\n\n\nMixedModels.PCA(m2)\n\n(Child = \nPrincipal components based on correlation matrix\n Test: Run     1.0    .     .     .     .\n Test: Star_r  0.51  1.0    .     .     .\n Test: S20_r   0.57  0.64  1.0    .     .\n Test: SLJ     0.56  0.6   0.72  1.0    .\n Test: BMT     0.2   0.41  0.37  0.49  1.0\n\nNormalized cumulative variances:\n[0.6139, 0.779, 0.8691, 0.9493, 1.0]\n\nComponent loadings\n                 PC1    PC2    PC3    PC4    PC5\n Test: Run     -0.42   0.53   0.63   0.37   0.08\n Test: Star_r  -0.47   0.03  -0.65   0.58  -0.16\n Test: S20_r   -0.49   0.15  -0.24  -0.49   0.66\n Test: SLJ     -0.5   -0.05   0.09  -0.5   -0.7\n Test: BMT     -0.34  -0.83   0.33   0.2    0.2, School = \nPrincipal components based on correlation matrix\n Test: Run      1.0     .      .      .      .      .      .\n Test: Star_r   0.53   1.0     .      .      .      .      .\n Test: S20_r    0.47   0.48   1.0     .      .      .      .\n Test: SLJ      0.47   0.76   0.41   1.0     .      .      .\n Test: BMT      0.15   0.38   0.18   0.02   1.0     .      .\n a1             0.58   0.47   0.56  -0.05   0.65   1.0     .\n Sex: Girls    -0.63  -0.27   0.06  -0.28  -0.58  -0.37   1.0\n\nNormalized cumulative variances:\n[0.4824, 0.694, 0.8509, 0.9554, 1.0, 1.0, 1.0]\n\nComponent loadings\n                 PC1    PC2    PC3    PC4    PC5    PC6    PC7\n Test: Run     -0.44   0.08   0.15  -0.64  -0.16  -0.5    0.29\n Test: Star_r  -0.44   0.28   0.02   0.41  -0.56   0.32   0.38\n Test: S20_r   -0.34   0.27  -0.58  -0.1    0.6    0.24   0.22\n Test: SLJ     -0.32   0.56   0.33   0.25   0.22  -0.24  -0.55\n Test: BMT     -0.32  -0.53  -0.02   0.55   0.25  -0.47   0.19\n a1            -0.41  -0.35  -0.42  -0.14  -0.33   0.1   -0.62\n Sex: Girls     0.34   0.35  -0.59   0.18  -0.29  -0.55   0.0, Cohort = \nPrincipal components based on correlation matrix\n Test: Run     1.0  .    .    .    .\n Test: Star_r  0.0  1.0  .    .    .\n Test: S20_r   0.0  0.0  1.0  .    .\n Test: SLJ     0.0  0.0  0.0  1.0  .\n Test: BMT     0.0  0.0  0.0  0.0  0.0\n\nNormalized cumulative variances:\n[0.25, 0.5, 0.75, 1.0, 1.0]\n\nComponent loadings\n                PC1   PC2   PC3   PC4     PC5\n Test: Run     1.0   0.0   0.0   0.0     0.0\n Test: Star_r  0.0   1.0   0.0   0.0     0.0\n Test: S20_r   0.0   0.0   0.0   1.0     0.0\n Test: SLJ     0.0   0.0   1.0   0.0     0.0\n Test: BMT     0.0   0.0   0.0   0.0   NaN)"
  },
  {
    "objectID": "fggk21.html#summary-of-results-for-stratified-subset-of-data",
    "href": "fggk21.html#summary-of-results-for-stratified-subset-of-data",
    "title": "Basics with Emotikon Project",
    "section": "4.9 Summary of results for stratified subset of data",
    "text": "4.9 Summary of results for stratified subset of data\nReturning to the theoretical focus of the article, the significant main effects of age and Sex, the interactions between age and c1 and c4 contrasts and the interactions between Sex and three test contrasts (c1, c2, c4) are replicated. Obviously, the subset of data is much noisier than the full set."
  },
  {
    "objectID": "fggk21.html#overall-summary-statistics",
    "href": "fggk21.html#overall-summary-statistics",
    "title": "Basics with Emotikon Project",
    "section": "6.1 Overall summary statistics",
    "text": "6.1 Overall summary statistics\n+ julia&gt; m1.optsum         # MixedModels.OptSummary:  gets all info\n+ julia&gt; loglikelihood(m1) # StatsBase.loglikelihood: return loglikelihood\n                             of the model\n+ julia&gt; deviance(m1)      # StatsBase.deviance: negative twice the log-likelihood\n                             relative to saturated model\n+ julia&gt; objective(m1)     # MixedModels.objective: saturated model not clear:\n                             negative twice the log-likelihood\n+ julia&gt; nobs(m1)          # n of observations; they are not independent\n+ julia&gt; dof(m1)           # n of degrees of freedom is number of model parameters\n+ julia&gt; aic(m1)           # objective(m1) + 2*dof(m1)\n+ julia&gt; bic(m1)           # objective(m1) + dof(m1)*log(nobs(m1))\n\nm1.optsum            # MixedModels.OptSummary:  gets all info\n\n\n\n\n\n\n\n\nInitialization\n\n\n\nInitial parameter vector\n[1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n\n\nInitial objective value\n25840.65375540981\n\n\nOptimizer settings\n\n\n\nOptimizer (from NLopt)\nLN_BOBYQA\n\n\nLower bounds\n[0.0, -Inf, -Inf, -Inf, -Inf, 0.0, -Inf, -Inf, -Inf, 0.0, -Inf, -Inf, 0.0, -Inf, 0.0, 0.0, -Inf, -Inf, -Inf, -Inf, -Inf, -Inf, 0.0, -Inf, -Inf, -Inf, -Inf, -Inf, 0.0, -Inf, -Inf, -Inf, -Inf, 0.0, -Inf, -Inf, -Inf, 0.0, -Inf, -Inf, 0.0, -Inf, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n\n\nftol_rel\n1.0e-12\n\n\nftol_abs\n1.0e-8\n\n\nxtol_rel\n0.0\n\n\nxtol_abs\n[1.0e-10, 1.0e-10, 1.0e-10, 1.0e-10, 1.0e-10, 1.0e-10, 1.0e-10, 1.0e-10, 1.0e-10, 1.0e-10, 1.0e-10, 1.0e-10, 1.0e-10, 1.0e-10, 1.0e-10, 1.0e-10, 1.0e-10, 1.0e-10, 1.0e-10, 1.0e-10, 1.0e-10, 1.0e-10, 1.0e-10, 1.0e-10, 1.0e-10, 1.0e-10, 1.0e-10, 1.0e-10, 1.0e-10, 1.0e-10, 1.0e-10, 1.0e-10, 1.0e-10, 1.0e-10, 1.0e-10, 1.0e-10, 1.0e-10, 1.0e-10, 1.0e-10, 1.0e-10, 1.0e-10, 1.0e-10, 1.0e-10, 1.0e-10, 1.0e-10, 1.0e-10, 1.0e-10, 1.0e-10]\n\n\ninitial_step\n[0.75, 1.0, 1.0, 1.0, 1.0, 0.75, 1.0, 1.0, 1.0, 0.75, 1.0, 1.0, 0.75, 1.0, 0.75, 0.75, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.75, 1.0, 1.0, 1.0, 1.0, 1.0, 0.75, 1.0, 1.0, 1.0, 1.0, 0.75, 1.0, 1.0, 1.0, 0.75, 1.0, 1.0, 0.75, 1.0, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75]\n\n\nmaxfeval\n-1\n\n\nmaxtime\n-1.0\n\n\nResult\n\n\n\nFunction evaluations\n2044\n\n\nFinal parameter vector\n[1.224, 0.216, 0.0008, 0.0532, -0.5051, 1.6406, -0.7561, -0.0496, 0.2533, 1.237, -0.6092, -0.1325, 1.1261, -0.5442, 1.4029, 0.4394, -0.0404, -0.0514, -0.1226, -0.2147, 0.3607, -0.1322, 0.6832, -0.2692, 0.1329, -0.027, -0.0974, 0.1244, 0.6164, -0.5338, 0.063, 0.0571, 0.1441, 0.3441, -0.443, -0.4224, -0.0267, 0.4088, 0.1251, -0.1813, 0.0035, -0.0011, 0.0, 0.0272, 0.131, 0.0, 0.0691, 0.0]\n\n\nFinal objective value\n24651.0137\n\n\nReturn code\nFTOL_REACHED\n\n\n\n\n\n\nloglikelihood(m1) # StatsBase.loglikelihood: return loglikelihood of the model\n\n-12325.506866364482\n\n\n\ndeviance(m1)      # StatsBase.deviance: negative twice the log-likelihood relative to saturated mode`\n\n24651.013732728963\n\n\n\nobjective(m1)    # MixedModels.objective: saturated model not clear: negative twice the log-likelihood\n\n24651.013732728963\n\n\n\nnobs(m1) # n of observations; they are not independent\n\n9663\n\n\n\nn_, p_, q_, k_ = size(m1)\n\n(9663, 20, 13076, 3)\n\n\n\ndof(m1)  # n of degrees of freedom is number of model parameters\n\n69\n\n\n\ngeom_df = sum(leverage(m1)) # trace of hat / rank of model matrix / geom dof\n\n6149.275819075056\n\n\n\nresid_df = nobs(m1) - geom_df  # eff. residual degrees of freedom\n\n3513.7241809249444\n\n\n\naic(m1)  # objective(m1) + 2*dof(m1)\n\n24789.013732728963\n\n\n\nbic(m1)  # objective(m1) + dof(m1)*log(nobs(m1))\n\n25284.161833950915"
  },
  {
    "objectID": "fggk21.html#fixed-effect-statistics",
    "href": "fggk21.html#fixed-effect-statistics",
    "title": "Basics with Emotikon Project",
    "section": "6.2 Fixed-effect statistics",
    "text": "6.2 Fixed-effect statistics\n+ julia&gt; coeftable(m1)     # StatsBase.coeftable: fixed-effects statiscs;\n                             default level=0.95\n+ julia&gt; Arrow.write(\"./data/m_cpx_fe.arrow\", DataFrame(coeftable(m1)));\n+ julia&gt; coef(m1)          # StatsBase.coef - parts of the table\n+ julia&gt; fixef(m1)         # MixedModels.fixef: not the same as coef()\n                             for rank-deficient case\n+ julia&gt; m1.beta           # alternative extractor\n+ julia&gt; fixefnames(m1)    # works also for coefnames(m1)\n+ julia&gt; vcov(m1)          # StatsBase.vcov: var-cov matrix of fixed-effects coef.\n+ julia&gt; stderror(m1)      # StatsBase.stderror: SE for fixed-effects coefficients\n+ julia&gt; propertynames(m1) # names of available extractors\n\ncoeftable(m1) # StatsBase.coeftable: fixed-effects statiscs; default level=0.95\n\n\n\n\n\n\n\n\n\n\n\n\nCoef.\nStd. Error\nz\nPr(&gt;\n\n\n\n\n(Intercept)\n-0.0133159\n0.019195\n-0.69\n0.4879\n\n\nTest: Star_r\n0.00884165\n0.0374684\n0.24\n0.8135\n\n\nTest: S20_r\n0.00916784\n0.0291662\n0.31\n0.7533\n\n\nTest: SLJ\n0.0033515\n0.0299139\n0.11\n0.9108\n\n\nTest: BMT\n-0.0382881\n0.0295761\n-1.29\n0.1955\n\n\na1\n0.195558\n0.0544508\n3.59\n0.0003\n\n\nSex: Girls\n-0.428744\n0.0311855\n-13.75\n&lt;1e-42\n\n\nTest: Star_r & a1\n0.289702\n0.0887459\n3.26\n0.0011\n\n\nTest: S20_r & a1\n-0.078042\n0.0816837\n-0.96\n0.3394\n\n\nTest: SLJ & a1\n-0.0632769\n0.0768833\n-0.82\n0.4105\n\n\nTest: BMT & a1\n0.471319\n0.0851493\n5.54\n&lt;1e-07\n\n\nTest: Star_r & Sex: Girls\n0.375595\n0.0510841\n7.35\n&lt;1e-12\n\n\nTest: S20_r & Sex: Girls\n-0.186705\n0.0475086\n-3.93\n&lt;1e-04\n\n\nTest: SLJ & Sex: Girls\n-0.0695246\n0.0445178\n-1.56\n0.1184\n\n\nTest: BMT & Sex: Girls\n-0.281233\n0.0495202\n-5.68\n&lt;1e-07\n\n\na1 & Sex: Girls\n-0.126656\n0.104828\n-1.21\n0.2270\n\n\nTest: Star_r & a1 & Sex: Girls\n-0.138263\n0.175583\n-0.79\n0.4310\n\n\nTest: S20_r & a1 & Sex: Girls\n0.175432\n0.163333\n1.07\n0.2828\n\n\nTest: SLJ & a1 & Sex: Girls\n-0.0154478\n0.152986\n-0.10\n0.9196\n\n\nTest: BMT & a1 & Sex: Girls\n0.154755\n0.170156\n0.91\n0.3631\n\n\n\n\n\n\n#Arrow.write(\"./data/m_cpx_fe.arrow\", DataFrame(coeftable(m1)));\n\n\ncoef(m1)              # StatsBase.coef; parts of the table\n\n20-element Vector{Float64}:\n -0.01331588201859281\n  0.00884164713229603\n  0.009167841597316541\n  0.0033515035135136158\n -0.0382881182300312\n  0.1955584925869149\n -0.42874371740491646\n  0.28970233760258673\n -0.07804198372006879\n -0.06327692734905414\n  0.4713188521327279\n  0.37559529867792923\n -0.18670517572454207\n -0.06952457393129302\n -0.281233388885801\n -0.1266555604572929\n -0.13826258772735348\n  0.1754317039265076\n -0.015447768144049355\n  0.1547551366187529\n\n\n\nfixef(m1)    # MixedModels.fixef: not the same as coef() for rank-deficient case\n\n20-element Vector{Float64}:\n -0.01331588201859281\n  0.00884164713229603\n  0.009167841597316541\n  0.0033515035135136158\n -0.0382881182300312\n  0.1955584925869149\n -0.42874371740491646\n  0.28970233760258673\n -0.07804198372006879\n -0.06327692734905414\n  0.4713188521327279\n  0.37559529867792923\n -0.18670517572454207\n -0.06952457393129302\n -0.281233388885801\n -0.1266555604572929\n -0.13826258772735348\n  0.1754317039265076\n -0.015447768144049355\n  0.1547551366187529\n\n\n\nm1.β                  # alternative extractor\n\n20-element Vector{Float64}:\n -0.01331588201859281\n  0.00884164713229603\n  0.009167841597316541\n  0.0033515035135136158\n -0.0382881182300312\n  0.1955584925869149\n -0.42874371740491646\n  0.28970233760258673\n -0.07804198372006879\n -0.06327692734905414\n  0.4713188521327279\n  0.37559529867792923\n -0.18670517572454207\n -0.06952457393129302\n -0.281233388885801\n -0.1266555604572929\n -0.13826258772735348\n  0.1754317039265076\n -0.015447768144049355\n  0.1547551366187529\n\n\n\nfixefnames(m1)        # works also for coefnames(m1)\n\n20-element Vector{String}:\n \"(Intercept)\"\n \"Test: Star_r\"\n \"Test: S20_r\"\n \"Test: SLJ\"\n \"Test: BMT\"\n \"a1\"\n \"Sex: Girls\"\n \"Test: Star_r & a1\"\n \"Test: S20_r & a1\"\n \"Test: SLJ & a1\"\n \"Test: BMT & a1\"\n \"Test: Star_r & Sex: Girls\"\n \"Test: S20_r & Sex: Girls\"\n \"Test: SLJ & Sex: Girls\"\n \"Test: BMT & Sex: Girls\"\n \"a1 & Sex: Girls\"\n \"Test: Star_r & a1 & Sex: Girls\"\n \"Test: S20_r & a1 & Sex: Girls\"\n \"Test: SLJ & a1 & Sex: Girls\"\n \"Test: BMT & a1 & Sex: Girls\"\n\n\n\nvcov(m1)   # StatsBase.vcov: var-cov matrix of fixed-effects coefficients\n\n20×20 Matrix{Float64}:\n  0.00036845    2.4542e-5    -1.72037e-5   …   3.88625e-6    2.86863e-6\n  2.4542e-5     0.00140388   -0.00042145      -9.28004e-7   -1.02436e-7\n -1.72037e-5   -0.00042145    0.000850665      2.05394e-5   -6.92622e-7\n -2.6875e-5     5.46299e-5   -0.000468521     -2.44053e-5    1.37469e-6\n -0.000143277   3.23757e-5   -7.82234e-6       1.42136e-6   -1.19405e-5\n -4.42848e-5   -7.47998e-5    2.77872e-5   …  -6.01974e-5   -5.10758e-5\n -4.42236e-5    6.33673e-5    4.36921e-5      -7.38967e-5    0.000282224\n -2.73691e-5   -0.000442559   0.000212936      8.73515e-6    2.59293e-6\n  3.47186e-6    0.000213318  -0.000395694     -0.000104387   8.22006e-6\n -2.00307e-5    9.18163e-8    0.000175375      0.000158895  -6.11834e-5\n  7.30162e-5   -2.67196e-5    2.85709e-5   …  -5.78976e-5    8.27887e-5\n  6.89343e-6    8.75214e-7   -2.0532e-6        8.75256e-6   -0.000108189\n  5.14011e-6   -1.95288e-6    7.65873e-6       0.000673769   0.000117964\n -5.22641e-6   -1.56608e-6   -4.64937e-6      -0.00136395    0.000576693\n -1.15171e-6    6.35967e-7   -9.06179e-7       0.000577357  -0.0017145\n -4.61715e-6   -1.20775e-5    3.51919e-7   …   0.00015258   -0.00391901\n -1.28195e-5   -2.83533e-5    2.85131e-5      -0.000140197   0.00179161\n  4.97996e-7    2.85752e-5   -4.69155e-5      -0.0115814    -0.00195382\n  3.88625e-6   -9.28004e-7    2.05394e-5       0.0234048    -0.00998426\n  2.86863e-6   -1.02436e-7   -6.92622e-7      -0.00998426    0.0289529\n\n\n\nvcov(m1; corr=true) # StatsBase.vcov: correlation matrix of fixed-effects coefficients\n\n20×20 Matrix{Float64}:\n  1.0           0.0341237    -0.0307293    …   0.00132339    0.000878293\n  0.0341237     1.0          -0.385657        -0.000161894  -1.60672e-5\n -0.0307293    -0.385657      1.0              0.00460316   -0.000139563\n -0.0468044     0.0487406    -0.537003        -0.00533284    0.000270076\n -0.252376      0.0292155    -0.00906811       0.00031413   -0.00237267\n -0.0423703    -0.0366632     0.0174969    …  -0.00722638   -0.00551271\n -0.0738776     0.0542309     0.0480364       -0.0154889     0.0531858\n -0.0160666    -0.133094      0.0822662        0.000643383   0.00017171\n  0.00221431    0.0696989    -0.16609         -0.00835329    0.000591417\n -0.0135729     3.1873e-5     0.078209         0.013509     -0.00467688\n  0.0446734    -0.00837498    0.0115044    …  -0.00444454    0.00571405\n  0.00703009    0.00045726   -0.00137805       0.00111994   -0.0124466\n  0.00563651   -0.00109708    0.0055272        0.0927014     0.0145925\n -0.00611618   -0.000938893  -0.00358081      -0.200268      0.0761315\n -0.00121163    0.000342757  -0.000627411      0.0762096    -0.203474\n -0.00229461   -0.00307492    0.000115103  …   0.00951414   -0.219712\n -0.00380365   -0.00430979    0.0055678       -0.00521919    0.0599672\n  0.000158841   0.00466927   -0.00984831      -0.463482     -0.0703014\n  0.00132339   -0.000161894   0.00460316       1.0          -0.383546\n  0.000878293  -1.60672e-5   -0.000139563     -0.383546      1.0\n\n\n\nstderror(m1)       # StatsBase.stderror: SE for fixed-effects coefficients\n\n20-element Vector{Float64}:\n 0.019195039092305105\n 0.0374684363562597\n 0.02916616024163202\n 0.029913949773052166\n 0.029576099347489775\n 0.05445076605515375\n 0.031185489686456177\n 0.08874585526972298\n 0.08168372353800525\n 0.07688328620577171\n 0.08514931586998098\n 0.05108409963067973\n 0.04750862039792243\n 0.044517787801048765\n 0.04952022583455841\n 0.10482769052165765\n 0.17558293286303547\n 0.16333341489373127\n 0.15298634639539826\n 0.17015556908802124\n\n\n\npropertynames(m1)  # names of available extractors\n\n(:formula, :reterms, :Xymat, :feterm, :sqrtwts, :parmap, :dims, :A, :L, :optsum, :θ, :theta, :β, :beta, :βs, :betas, :λ, :lambda, :stderror, :σ, :sigma, :σs, :sigmas, :σρs, :sigmarhos, :b, :u, :lowerbd, :X, :y, :corr, :vcov, :PCA, :rePCA, :objective, :pvalues)"
  },
  {
    "objectID": "fggk21.html#covariance-parameter-estimates",
    "href": "fggk21.html#covariance-parameter-estimates",
    "title": "Basics with Emotikon Project",
    "section": "6.3 Covariance parameter estimates",
    "text": "6.3 Covariance parameter estimates\nThese commands inform us about the model parameters associated with the RES.\n+ julia&gt; issingular(m1)        # Test singularity for param. vector m1.theta\n+ julia&gt; VarCorr(m1)           # MixedModels.VarCorr: est. of RES\n+ julia&gt; propertynames(m1)\n+ julia&gt; m1.σ                  # residual; or: m1.sigma\n+ julia&gt; m1.σs                 # VCs; m1.sigmas\n+ julia&gt; m1.θ                  # Parameter vector for RES (w/o residual); m1.theta\n+ julia&gt; MixedModels.sdest(m1) #  prsqrt(MixedModels.varest(m1))\n+ julia&gt; BlockDescription(m1)  #  Description of blocks of A and L in an LMM\n\nissingular(m1) # Test if model is singular for parameter vector m1.theta (default)\n\ntrue\n\n\n\nissingular(m2)\n\ntrue\n\n\n\nVarCorr(m1) # MixedModels.VarCorr: estimates of random-effect structure (RES)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nColumn\nVariance\nStd.Dev\nCorr.\n\n\n\n\n\n\n\n\n\nChild\n(Intercept)\n0.3544881\n0.5953890\n\n\n\n\n\n\n\n\n\nTest: Star_r\n0.6479230\n0.8049366\n+0.13\n\n\n\n\n\n\n\n\nTest: S20_r\n0.4973485\n0.7052294\n+0.00\n-0.52\n\n\n\n\n\n\n\nTest: SLJ\n0.3891358\n0.6238075\n+0.04\n-0.03\n-0.39\n\n\n\n\n\n\nTest: BMT\n0.6154675\n0.7845173\n-0.31\n+0.11\n-0.15\n-0.28\n\n\n\n\nSchool\n(Intercept)\n0.0456848\n0.2137401\n\n\n\n\n\n\n\n\n\nTest: Star_r\n0.1108304\n0.3329121\n-0.06\n\n\n\n\n\n\n\n\nTest: S20_r\n0.1076800\n0.3281462\n-0.08\n-0.39\n\n\n\n\n\n\n\nTest: SLJ\n0.1031693\n0.3211997\n-0.19\n+0.21\n-0.80\n\n\n\n\n\n\nTest: BMT\n0.0980019\n0.3130526\n-0.33\n-0.02\n+0.13\n-0.38\n\n\n\n\n\na1\n0.0797394\n0.2823816\n+0.62\n-0.20\n+0.11\n-0.61\n+0.45\n\n\n\n\nSex: Girls\n0.0206539\n0.1437147\n-0.45\n+0.45\n+0.31\n-0.27\n-0.15\n-0.37\n\n\nCohort\n(Intercept)\n0.0001745\n0.0132087\n\n\n\n\n\n\n\n\n\nTest: Star_r\n0.0040625\n0.0637377\n.\n\n\n\n\n\n\n\n\nTest: S20_r\n0.0000000\n0.0000000\n.\n.\n\n\n\n\n\n\n\nTest: SLJ\n0.0011298\n0.0336125\n.\n.\n.\n\n\n\n\n\n\nTest: BMT\n0.0000000\n0.0000000\n.\n.\n.\n.\n\n\n\n\nResidual\n\n0.2366260\n0.4864422\n\n\n\n\n\n\n\n\n\n\n\n\nVarCorr(m2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nColumn\nVariance\nStd.Dev\nCorr.\n\n\n\n\n\n\n\n\n\nChild\nTest: Run\n0.5590442\n0.7476926\n\n\n\n\n\n\n\n\n\nTest: Star_r\n0.6319425\n0.7949481\n+0.51\n\n\n\n\n\n\n\n\nTest: S20_r\n0.5859061\n0.7654450\n+0.57\n+0.64\n\n\n\n\n\n\n\nTest: SLJ\n0.6057411\n0.7782937\n+0.56\n+0.60\n+0.72\n\n\n\n\n\n\nTest: BMT\n0.4822738\n0.6944594\n+0.20\n+0.41\n+0.37\n+0.49\n\n\n\n\nSchool\nTest: Run\n0.1370711\n0.3702311\n\n\n\n\n\n\n\n\n\nTest: Star_r\n0.0963028\n0.3103270\n+0.53\n\n\n\n\n\n\n\n\nTest: S20_r\n0.1092553\n0.3305378\n+0.47\n+0.48\n\n\n\n\n\n\n\nTest: SLJ\n0.0614022\n0.2477947\n+0.47\n+0.76\n+0.41\n\n\n\n\n\n\nTest: BMT\n0.0387320\n0.1968044\n+0.15\n+0.38\n+0.18\n+0.02\n\n\n\n\n\na1\n0.0801214\n0.2830572\n+0.58\n+0.47\n+0.56\n-0.05\n+0.65\n\n\n\n\nSex: Girls\n0.0205483\n0.1433468\n-0.63\n-0.27\n+0.06\n-0.28\n-0.58\n-0.37\n\n\nCohort\nTest: Run\n0.0030109\n0.0548716\n\n\n\n\n\n\n\n\n\nTest: Star_r\n0.0004327\n0.0208005\n.\n\n\n\n\n\n\n\n\nTest: S20_r\n0.0031715\n0.0563158\n.\n.\n\n\n\n\n\n\n\nTest: SLJ\n0.0002484\n0.0157616\n.\n.\n.\n\n\n\n\n\n\nTest: BMT\n0.0000000\n0.0000000\n.\n.\n.\n.\n\n\n\n\nResidual\n\n0.2654901\n0.5152574\n\n\n\n\n\n\n\n\n\n\n\n\nm1.σs      # VCs; m1.sigmas\n\n(Child = (var\"(Intercept)\" = 0.5953890275925519, var\"Test: Star_r\" = 0.8049366468110731, var\"Test: S20_r\" = 0.7052293867193989, var\"Test: SLJ\" = 0.6238074832962222, var\"Test: BMT\" = 0.7845173367673554), School = (var\"(Intercept)\" = 0.21374013212960472, var\"Test: Star_r\" = 0.33291207058475714, var\"Test: S20_r\" = 0.32814624674860376, var\"Test: SLJ\" = 0.32119971589542495, var\"Test: BMT\" = 0.31305261924546574, a1 = 0.2823815873673048, var\"Sex: Girls\" = 0.1437146853552164), Cohort = (var\"(Intercept)\" = 0.013208735511156039, var\"Test: Star_r\" = 0.06373774288062452, var\"Test: S20_r\" = 0.0, var\"Test: SLJ\" = 0.03361254857775373, var\"Test: BMT\" = 0.0))\n\n\n\nm1.θ       # Parameter vector for RES (w/o residual); m1.theta\n\n48-element Vector{Float64}:\n  1.2239666493516386\n  0.2160434052481848\n  0.0008169302990721254\n  0.05319332147090865\n -0.5050531868226333\n  1.6405786927765886\n -0.7561237904856136\n -0.04961209105316309\n  0.2532788364670923\n  1.2369760227642856\n -0.6092437287568104\n -0.13248197635761508\n  1.126076976808209\n  ⋮\n -0.026657781528939266\n  0.40877075486688047\n  0.12514366094697357\n -0.18131304515737107\n  0.0035233716721180973\n -0.0011006046256771937\n  0.0\n  0.027153761652499598\n  0.13102839987877896\n  0.0\n  0.06909875149233784\n  0.0\n\n\n\nBlockDescription(m1) #  Description of blocks of A and L in a LinearMixedModel\n\n\n\n\nrows\nChild\nSchool\nCohort\nfixed\n\n\n\n\n9930\nBlkDiag\n\n\n\n\n\n3101\nSparse\nBlkDiag\n\n\n\n\n45\nDense\nDense\nBlkDiag/Dense\n\n\n\n21\nDense\nDense\nDense\nDense\n\n\n\n\n\n\nm2.θ\n\n48-element Vector{Float64}:\n  1.4511051185712929\n  0.7797754806652839\n  0.8397427649698777\n  0.8407130390849901\n  0.2632573379428689\n  1.3312537401519589\n  0.6115749639823279\n  0.5644243682102701\n  0.4787253072288131\n  1.0619287044564407\n  0.5420526743752824\n  0.21403875721263715\n  0.981020345431411\n  ⋮\n -0.01806838487749003\n  0.3165904856871388\n  0.13929208239993\n -0.18517109484713315\n  0.0\n  7.311098430498756e-5\n  0.0\n  0.10649353285470316\n  0.04036921598294361\n  0.10929649922489827\n  0.030589747970018153\n  0.0\n\n\n\nBlockDescription(m2)\n\n\n\n\nrows\nChild\nSchool\nCohort\nfixed\n\n\n\n\n9930\nBlkDiag\n\n\n\n\n\n3101\nSparse\nBlkDiag\n\n\n\n\n45\nDense\nDense\nBlkDiag/Dense\n\n\n\n21\nDense\nDense\nDense\nDense"
  },
  {
    "objectID": "fggk21.html#model-predictions",
    "href": "fggk21.html#model-predictions",
    "title": "Basics with Emotikon Project",
    "section": "6.4 Model “predictions”",
    "text": "6.4 Model “predictions”\nThese commands inform us about extracion of conditional modes/means and (co-)variances, that using the model parameters to improve the predictions for units (levels) of the grouping (random) factors. We need this information, e.g., for partial-effect response profiles (e.g., facet plot) or effect profiles (e.g., caterpillar plot), or visualizing the borrowing-strength effect for correlation parameters (e.g., shrinkage plots). We are using the fit of LMM m2.\njulia&gt; condVar(m2)\nSome plotting functions are currently available from the MixedModelsMakie package or via custom functions.\n+ julia&gt; caterpillar!(m2)\n+ julia&gt; shrinkage!(m2)\n\n6.4.1 Conditional covariances\n\ncondVar(m1)\n\n3-element Vector{Array{Float64, 3}}:\n [0.0520247864071823 0.00806414911635219 … 0.0029715968174584425 -0.02016728375583423; 0.00806414911635219 0.28796120060926494 … -0.005166570524619367 0.018767387944463674; … ; 0.0029715968174584425 -0.005166570524619367 … 0.21937144047937782 -0.094022262103809; -0.02016728375583423 0.018767387944463674 … -0.094022262103809 0.2659932576479702;;; 0.05855927229044253 0.010472387331728079 … 0.004001860809597209 -0.02804325258627026; 0.010472387331728079 0.29901806979824563 … -0.003172783667384047 0.019267637754357934; … ; 0.004001860809597209 -0.003172783667384047 … 0.22496480457811205 -0.09559298825697443; -0.02804325258627026 0.019267637754357934 … -0.09559298825697443 0.2772874040422872;;; 0.055644868522513366 0.00956076804026058 … 0.003123542232570369 -0.024479484381002878; 0.00956076804026058 0.29311838288340747 … -0.004373412528848556 0.018744180703860632; … ; 0.003123542232570369 -0.004373412528848556 … 0.22326960903044724 -0.09462424183245854; -0.024479484381002878 0.018744180703860632 … -0.09462424183245854 0.2720445490377345;;; … ;;; 0.08822583964819819 0.009347941519549074 … -0.008263611764646859 -0.02010124233364069; 0.009347941519549074 0.29625598610431486 … -0.004797717769899277 0.020206249190824876; … ; -0.008263611764646859 -0.004797717769899277 … 0.2225041371131604 -0.09251856692546342; -0.02010124233364069 0.020206249190824876 … -0.09251856692546342 0.27195229639046203;;; 0.09495772742366226 0.0062923496325693 … -0.002873105041989705 -0.018784591855405233; 0.0062923496325693 0.29211112791382093 … -0.0052201710558901155 0.020688810816668197; … ; -0.002873105041989705 -0.0052201710558901155 … 0.22136046397246056 -0.09294837238720362; -0.018784591855405233 0.020688810816668197 … -0.09294837238720362 0.26835254167609185;;; 0.08111305899187102 0.00648062764650958 … -0.001528468225030826 -0.015597275691991465; 0.00648062764650958 0.28863604893086997 … -0.00554103816008476 0.020118561986334233; … ; -0.001528468225030826 -0.00554103816008476 … 0.2195574211884858 -0.09301915526723269; -0.015597275691991465 0.020118561986334233 … -0.09301915526723269 0.2651565784987209]\n [0.03736685286464265 -0.0017774763348050034 … 0.03000250326752936 -0.012397559396104747; -0.0017774763348050034 0.0904721080879684 … -0.012477188079513661 0.017400849303029774; … ; 0.03000250326752936 -0.012477188079513661 … 0.06541717227973572 -0.01280315072586248; -0.012397559396104747 0.017400849303029774 … -0.01280315072586248 0.018321913100336137;;; 0.03216333640395021 0.00021604561305795504 … 0.02631511256000991 -0.010623243056437155; 0.00021604561305795504 0.07636315461297444 … -0.007810058939639041 0.01421630995642533; … ; 0.02631511256000991 -0.007810058939639041 … 0.05724848557062279 -0.010798720356306083; -0.010623243056437155 0.01421630995642533 … -0.010798720356306083 0.015890956491284327;;; 0.029297490046210597 -0.003146504230605966 … 0.02159196544931729 -0.010427145876834376; -0.003146504230605966 0.08775018676671936 … -0.013338363999412068 0.018119242856884304; … ; 0.02159196544931729 -0.013338363999412068 … 0.051705402075806414 -0.010414532387749188; -0.010427145876834376 0.018119242856884304 … -0.010414532387749188 0.016770286409506927;;; … ;;; 0.026748574751573596 0.0029135176932135283 … 0.019082610653035136 -0.0068274214685979445; 0.0029135176932135283 0.07611111784481811 … -0.005681814161900429 0.01291537179396916; … ; 0.019082610653035136 -0.005681814161900429 … 0.05073326313046171 -0.006157197448554141; -0.0068274214685979445 0.01291537179396916 … -0.006157197448554141 0.013703291972097938;;; 0.02721914929082198 0.0035247726084060354 … 0.02036073696955497 -0.006132944894486206; 0.0035247726084060354 0.0760314066737002 … -0.005153021112316635 0.01261649944688352; … ; 0.02036073696955497 -0.005153021112316635 … 0.05286936820145979 -0.006338275097575239; -0.006132944894486206 0.01261649944688352 … -0.006338275097575239 0.01372947198177809;;; 0.04332090073095136 -0.0030543720439346693 … 0.036459565304006715 -0.013618032236208182; -0.0030543720439346693 0.09940119556233214 … -0.015252413186102775 0.019157697030230042; … ; 0.036459565304006715 -0.015252413186102775 … 0.07496997588606188 -0.014404723417442925; -0.013618032236208182 0.019157697030230042 … -0.014404723417442925 0.019497628262637796]\n [0.00016385686406272497 6.8612497891564535e-6 … -4.731546446973684e-6 0.0; 6.8612497891564535e-6 0.002351630119884283 … -0.00015374217123349432 0.0; … ; -4.731546446973684e-6 -0.00015374217123349432 … 0.000851265439055861 0.0; 0.0 0.0 … 0.0 0.0;;; 0.00016044961160532187 8.70009580480665e-6 … -6.021918514256797e-6 0.0; 8.70009580480665e-6 0.0020374962198157873 … -0.00016832089467199048 0.0; … ; -6.021918514256797e-6 -0.00016832089467199048 … 0.0007839576973693307 0.0; 0.0 0.0 … 0.0 0.0;;; 0.00016241709272062882 7.574570524716457e-6 … -5.147080464509524e-6 0.0; 7.574570524716457e-6 0.00223553911940227 … -0.00015947135556164258 0.0; … ; -5.147080464509524e-6 -0.00015947135556164258 … 0.0008235744453745595 0.0; 0.0 0.0 … 0.0 0.0;;; 0.0001618037288544521 8.356036365880003e-6 … -5.468504998170012e-6 0.0; 8.356036365880003e-6 0.002157380074802483 … -0.0001640605465326268 0.0; … ; -5.468504998170012e-6 -0.0001640605465326268 … 0.0008117167352767775 0.0; 0.0 0.0 … 0.0 0.0;;; 0.0001593806265262883 8.604661242930377e-6 … -6.8998220629682774e-6 0.0; 8.604661242930377e-6 0.0019547627015494462 … -0.000170294390628613 0.0; … ; -6.8998220629682774e-6 -0.000170294390628613 … 0.0007630400078798982 0.0; 0.0 0.0 … 0.0 0.0;;; 0.00015860584729254548 7.355427111250937e-6 … -7.340293250662632e-6 0.0; 7.355427111250937e-6 0.0019262219140290667 … -0.00016919086974735194 0.0; … ; -7.340293250662632e-6 -0.00016919086974735194 … 0.0007461156760280915 0.0; 0.0 0.0 … 0.0 0.0;;; 0.0001607036650399537 7.945256060374978e-6 … -6.4061935282497765e-6 0.0; 7.945256060374978e-6 0.0020703258871786192 … -0.00016784904889107172 0.0; … ; -6.4061935282497765e-6 -0.00016784904889107172 … 0.0007882004543855909 0.0; 0.0 0.0 … 0.0 0.0;;; 0.00015836276054894752 8.37460533304594e-6 … -7.910925267268054e-6 0.0; 8.37460533304594e-6 0.0018811147712629059 … -0.0001742521170091487 0.0; … ; -7.910925267268054e-6 -0.0001742521170091487 … 0.0007426686197023693 0.0; 0.0 0.0 … 0.0 0.0;;; 0.0001565446850727776 9.669897405303062e-6 … -7.70285876435866e-6 0.0; 9.669897405303062e-6 0.0017805941331152013 … -0.00017395067091495247 0.0; … ; -7.70285876435866e-6 -0.00017395067091495247 … 0.0007167269572421842 0.0; 0.0 0.0 … 0.0 0.0]\n\n\n\ncondVar(m2)\n\n3-element Vector{Array{Float64, 3}}:\n [0.1716573654521686 0.028016698236938187 … 0.03468306524897078 -0.007271216731970491; 0.028016698236938187 0.174315695209237 … 0.037934109860844306 0.01923883389438937; … ; 0.03468306524897078 0.037934109860844306 … 0.16027560546268294 0.029395284108574015; -0.007271216731970491 0.01923883389438937 … 0.029395284108574015 0.16139411075051363;;; 0.18276263006507637 0.03478539868575472 … 0.041970876393990525 -0.00910133674185354; 0.03478539868575472 0.18600937701782716 … 0.048989159115419255 0.0217907227690581; … ; 0.041970876393990525 0.048989159115419255 … 0.17250554008459493 0.03182105814484668; -0.00910133674185354 0.0217907227690581 … 0.03182105814484668 0.16382552537834547;;; 0.17667137751882914 0.031593933136605044 … 0.038693324220989714 -0.00798581457166768; 0.031593933136605044 0.18076950450246082 … 0.043903974564899355 0.020528172534171607; … ; 0.038693324220989714 0.043903974564899355 … 0.16690078974517686 0.03065684052497917; -0.00798581457166768 0.020528172534171607 … 0.03065684052497917 0.16256301128065784;;; … ;;; 0.21742492055620308 0.0711187481630728 … 0.0679559431729804 0.021187793715072463; 0.0711187481630728 0.221712806524546 … 0.07325587210572022 0.05112609460108065; … ; 0.0679559431729804 0.07325587210572022 … 0.18396222541172894 0.051267223554201484; 0.021187793715072463 0.05112609460108065 … 0.051267223554201484 0.1866511267887683;;; 0.22423106046287505 0.07786835399992705 … 0.07735125185697463 0.03178525778082243; 0.07786835399992705 0.2248563186114507 … 0.07936035042825286 0.05882230728941721; … ; 0.07735125185697463 0.07936035042825286 … 0.19262379436347435 0.06150812385090498; 0.03178525778082243 0.05882230728941721 … 0.06150812385090498 0.19541988999864893;;; 0.20625842931475472 0.061734667428460316 … 0.06273422689812473 0.021778024208207614; 0.061734667428460316 0.20768473972488907 … 0.06450259763799501 0.04804129613623677; … ; 0.06273422689812473 0.06450259763799501 … 0.1794607756064914 0.0522777648783303; 0.021778024208207614 0.04804129613623677 … 0.0522777648783303 0.18716843280034437]\n [0.10769972461146979 0.048410637847954206 … 0.04765615856785282 -0.02775559487534286; 0.048410637847954206 0.07951125857367367 … 0.034908441077306056 -0.010292788437635333; … ; 0.04765615856785282 0.034908441077306056 … 0.06565494521898993 -0.012791347514367545; -0.02775559487534286 -0.010292788437635333 … -0.012791347514367545 0.018224383963561946;;; 0.08791287280141899 0.04040021593163912 … 0.04005096143018161 -0.022686534390826667; 0.04040021593163912 0.06915594077953924 … 0.03203799857908201 -0.008413281864543865; … ; 0.04005096143018161 0.03203799857908201 … 0.057412708561706355 -0.010798261424779488; -0.022686534390826667 -0.008413281864543865 … -0.010798261424779488 0.01580801878135513;;; 0.09637601123507496 0.03589456389756267 … 0.038326911556305666 -0.025402823845251898; 0.03589456389756267 0.06309695671764155 … 0.02474630818640006 -0.007226647216429437; … ; 0.038326911556305666 0.02474630818640006 … 0.05190182567131017 -0.010425366032840198; -0.025402823845251898 -0.007226647216429437 … -0.010425366032840198 0.01668640249560667;;; … ;;; 0.07442246275580312 0.030540467389456115 … 0.02936292841187446 -0.017019334381608173; 0.030540467389456115 0.06268958750938752 … 0.02346424355353961 -0.004047186959904146; … ; 0.02936292841187446 0.02346424355353961 … 0.050927855872033814 -0.00616619556722447; -0.017019334381608173 -0.004047186959904146 … -0.00616619556722447 0.013633700797821799;;; 0.0735758168661645 0.030549787480355847 … 0.03009245307624193 -0.01605837600786653; 0.030549787480355847 0.06348029403779122 … 0.02473696381450468 -0.0033824882778251764; … ; 0.03009245307624193 0.02473696381450468 … 0.05305116014873455 -0.006351913234660337; -0.01605837600786653 -0.0033824882778251764 … -0.006351913234660337 0.013657191317016574;;; 0.12475531416678319 0.057980323991242325 … 0.05754685504210366 -0.030987802302217794; 0.057980323991242325 0.09052601669135373 … 0.04198816558881446 -0.011763809044547746; … ; 0.05754685504210366 0.04198816558881446 … 0.07525867380823965 -0.014395087802056122; -0.030987802302217794 -0.011763809044547746 … -0.014395087802056122 0.019398066332050413]\n [0.0018755627275214876 3.089435540918309e-5 … 2.240445843689437e-5 0.0; 3.089435540918309e-5 0.0003965875942257697 … 5.707699981391102e-6 0.0; … ; 2.240445843689437e-5 5.707699981391102e-6 … 0.00023447996250939153 0.0; 0.0 0.0 … 0.0 0.0;;; 0.001646376970663637 3.720954962274924e-5 … 2.7417888277286936e-5 0.0; 3.720954962274924e-5 0.00038538833817743535 … 7.814064228083367e-6 0.0; … ; 2.7417888277286936e-5 7.814064228083367e-6 … 0.000229982924148501 0.0; 0.0 0.0 … 0.0 0.0;;; 0.0017898412273782147 3.236105265738958e-5 … 2.4400334475545568e-5 0.0; 3.236105265738958e-5 0.00039253001485496647 … 6.5222106687580066e-6 0.0; … ; 2.4400334475545568e-5 6.5222106687580066e-6 … 0.00023265031515805362 0.0; 0.0 0.0 … 0.0 0.0;;; 0.0017328616069526477 3.428572805234312e-5 … 2.5422158736825393e-5 0.0; 3.428572805234312e-5 0.0003904519899866648 … 6.73975625908976e-6 0.0; … ; 2.5422158736825393e-5 6.73975625908976e-6 … 0.00023198045879735532 0.0; 0.0 0.0 … 0.0 0.0;;; 0.001588090384384114 3.879528823765752e-5 … 2.8207422860090018e-5 0.0; 3.879528823765752e-5 0.0003823415034034787 … 8.170895209024192e-6 0.0; … ; 2.8207422860090018e-5 8.170895209024192e-6 … 0.00022854142369010057 0.0; 0.0 0.0 … 0.0 0.0;;; 0.0015700608699872338 3.991473369249928e-5 … 2.8878104726035227e-5 0.0; 3.991473369249928e-5 0.00037892295177568964 … 8.988726486139536e-6 0.0; … ; 2.8878104726035227e-5 8.988726486139536e-6 … 0.00022729625363972344 0.0; 0.0 0.0 … 0.0 0.0;;; 0.0016718855317398285 3.6846188009940684e-5 … 2.6424255078286086e-5 0.0; 3.6846188009940684e-5 0.00038579674854786783 … 7.565891802326375e-6 0.0; … ; 2.6424255078286086e-5 7.565891802326375e-6 … 0.00023023332546893613 0.0; 0.0 0.0 … 0.0 0.0;;; 0.001534819444237953 4.080527891291748e-5 … 2.943685366995836e-5 0.0; 4.080527891291748e-5 0.0003787821214818895 … 8.890033569166902e-6 0.0; … ; 2.943685366995836e-5 8.890033569166902e-6 … 0.00022701264662666613 0.0; 0.0 0.0 … 0.0 0.0;;; 0.0014547512252280732 4.275856306522743e-5 … 3.105369942845664e-5 0.0; 4.275856306522743e-5 0.0003727782989429866 … 1.0068949364309914e-5 0.0; … ; 3.105369942845664e-5 1.0068949364309914e-5 … 0.0002250526805839134 0.0; 0.0 0.0 … 0.0 0.0]\n\n\nThey are hard to look at. Let’s take pictures.\n\n\n6.4.2 Caterpillar plots\n\n\nCode\ncaterpillar!(\n  Figure(; resolution=(800, 400)), ranefinfo(m1, :Cohort)\n)\n\n\n\n\n\n\n\n\nFigure 3: Prediction intervals of the random effects for Cohort in model m1\n\n\n\n\n\n\n\n6.4.3 Shrinkage plots\nThese are just teasers. We will pick this up in a separate tutorial. Enjoy!\n\n\nCode\nshrinkageplot!(Figure(; resolution=(800, 800)), m1, :Cohort)\n\n\n\n\n\n\n\n\nFigure 4: Shrinkage plot of the random effects for Cohort in model m1\n\n\n\n\n\n\n\nCode\nshrinkageplot!(Figure(; resolution=(800, 800)), m2, :Cohort)\n\n\n\n\n\n\n\n\nFigure 5: Shrinkage plot of the random effects for Cohort in model m2\n\n\n\n\n\n\n\nFühner, T., Granacher, U., Golle, K., & Kliegl, R. (2021). Age and sex effects in physical fitness components of 108,295 third graders including 515 primary schools and 9 cohorts. Scientific Reports, 11(1). https://doi.org/10.1038/s41598-021-97000-4\n\n\nSchielzeth, H., & Forstmeier, W. (2008). Conclusions beyond support: Overconfident estimates in mixed models. Behavioral Ecology, 20(2), 416–420. https://doi.org/10.1093/beheco/arn145"
  },
  {
    "objectID": "largescaledesigned.html",
    "href": "largescaledesigned.html",
    "title": "1 A large-scale designed experiment",
    "section": "",
    "text": "Load the packages to be used.\n\n\nCode\nusing AlgebraOfGraphics\nusing Arrow\nusing CairoMakie\nusing Chain\nusing DataFrameMacros\nusing DataFrames\nusing Effects\nusing MixedModels\nusing MixedModelsMakie\nusing SMLP2023: dataset\nusing StandardizedPredictors\nusing StatsBase\n\nCairoMakie.activate!(; type=\"svg\")\nimport ProgressMeter\nProgressMeter.ijulia_behavior(:clear)\n\n\nThe English Lexicon Project (Balota et al., 2007) was a large-scale multicenter study to examine properties of English words. It incorporated both a lexical decision task and a word recognition task. Different groups of subjects participated in the different tasks.",
    "crumbs": [
      "Worked examples",
      "A large-scale designed experiment"
    ]
  },
  {
    "objectID": "largescaledesigned.html#trial-level-data-from-the-ldt",
    "href": "largescaledesigned.html#trial-level-data-from-the-ldt",
    "title": "1 A large-scale designed experiment",
    "section": "2.1 Trial-level data from the LDT",
    "text": "2.1 Trial-level data from the LDT\nIn the lexical decision task the study participant is shown a character string, under carefully controlled conditions, and responds according to whether they identify the string as a word or not. Two responses are recorded: whether the choice of word/non-word is correct and the time that elapsed between exposure to the string and registering a decision.\nSeveral covariates, some relating to the subject and some relating to the target, were recorded. Initially we consider only the trial-level data.\n\nldttrial = dataset(:ELP_ldt_trial)\n\nArrow.Table with 2745952 rows, 5 columns, and schema:\n :subj  Int16\n :seq   Int16\n :acc   Union{Missing, Bool}\n :rt    Int16\n :item  String\n\nwith metadata given by a Base.ImmutableDict{String, String} with 3 entries:\n  \"title\"     =&gt; \"Trial-level data from Lexical Discrimination Task in the Engl…\n  \"reference\" =&gt; \"Balota et al. (2007), The English Lexicon Project, Behavior R…\n  \"source\"    =&gt; \"https://osf.io/n63s2\"\n\n\nThe two response variables are acc - the accuracy of the response - and rt, the response time in milliseconds. There is one trial-level covariate, seq, the sequence number of the trial within subj. Each subject participated in two sessions on different days, with 2000 trials recorded on the first day.\nNotice the metadata with a citation and a URL for the OSF project.\nWe convert to a DataFrame and add a Boolean column s2 which is true for trials in the second session.\n\nldttrial = @transform(DataFrame(ldttrial), :s2 = :seq &gt; 2000)\ndescribe(ldttrial)\n\n6×7 DataFrame\n\n\n\nRow\nvariable\nmean\nmin\nmedian\nmax\nnmissing\neltype\n\n\n\nSymbol\nUnion…\nAny\nUnion…\nAny\nInt64\nType\n\n\n\n\n1\nsubj\n409.31\n1\n409.0\n816\n0\nInt16\n\n\n2\nseq\n1687.21\n1\n1687.0\n3374\n0\nInt16\n\n\n3\nacc\n0.85604\nfalse\n1.0\ntrue\n1370\nUnion{Missing, Bool}\n\n\n4\nrt\n846.325\n-16160\n732.0\n32061\n0\nInt16\n\n\n5\nitem\n\nAarod\n\nzuss\n0\nString\n\n\n6\ns2\n0.407128\nfalse\n0.0\ntrue\n0\nBool",
    "crumbs": [
      "Worked examples",
      "A large-scale designed experiment"
    ]
  },
  {
    "objectID": "largescaledesigned.html#sec-ldtinitialexplore",
    "href": "largescaledesigned.html#sec-ldtinitialexplore",
    "title": "1 A large-scale designed experiment",
    "section": "2.2 Initial data exploration",
    "text": "2.2 Initial data exploration\nFrom the basic summary of ldttrial we can see that there are some questionable response times — negative values and values over 32 seconds.\nBecause of obvious outliers we will use the median response time, which is not strongly influenced by outliers, rather than the mean response time when summarizing by item or by subject.\nAlso, there are missing values of the accuracy. We should check if these are associated with particular subjects or particular items.\n\n2.2.1 Summaries by item\nTo summarize by item we group the trials by item and use combine to produce the various summary statistics. As we will create similar summaries by subject, we incorporate an ‘i’ in the names of these summaries (and an ‘s’ in the name of the summaries by subject) to be able to identify the grouping used.\n\nbyitem = @chain ldttrial begin\n  groupby(:item)\n  @combine(\n    :ni = length(:acc),               # no. of obs\n    :imiss = count(ismissing, :acc),  # no. of missing acc\n    :iacc = count(skipmissing(:acc)), # no. of accurate\n    :imedianrt = median(:rt),\n  )\n  @transform!(\n    :wrdlen = Int8(length(:item)),\n    :ipropacc = :iacc / :ni\n  )\nend\n\n80962×7 DataFrame80937 rows omitted\n\n\n\nRow\nitem\nni\nimiss\niacc\nimedianrt\nwrdlen\nipropacc\n\n\n\nString\nInt64\nInt64\nInt64\nFloat64\nInt8\nFloat64\n\n\n\n\n1\na\n35\n0\n26\n743.0\n1\n0.742857\n\n\n2\ne\n35\n0\n19\n824.0\n1\n0.542857\n\n\n3\naah\n34\n0\n21\n770.5\n3\n0.617647\n\n\n4\naal\n34\n0\n32\n702.5\n3\n0.941176\n\n\n5\nAaron\n33\n0\n31\n625.0\n5\n0.939394\n\n\n6\nAarod\n33\n0\n23\n810.0\n5\n0.69697\n\n\n7\naback\n34\n0\n15\n710.0\n5\n0.441176\n\n\n8\nahack\n34\n0\n34\n662.0\n5\n1.0\n\n\n9\nabacus\n34\n0\n17\n671.5\n6\n0.5\n\n\n10\nalacus\n34\n0\n29\n640.0\n6\n0.852941\n\n\n11\nabandon\n34\n0\n32\n641.0\n7\n0.941176\n\n\n12\nacandon\n34\n0\n33\n725.5\n7\n0.970588\n\n\n13\nabandoned\n34\n0\n31\n667.5\n9\n0.911765\n\n\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n\n\n80951\nzoology\n33\n0\n32\n623.0\n7\n0.969697\n\n\n80952\npoology\n33\n0\n32\n757.0\n7\n0.969697\n\n\n80953\nzoom\n35\n0\n34\n548.0\n4\n0.971429\n\n\n80954\nzool\n35\n0\n30\n633.0\n4\n0.857143\n\n\n80955\nzooming\n33\n0\n29\n617.0\n7\n0.878788\n\n\n80956\nsooming\n33\n0\n30\n721.0\n7\n0.909091\n\n\n80957\nzooms\n33\n0\n30\n598.0\n5\n0.909091\n\n\n80958\ncooms\n33\n0\n31\n660.0\n5\n0.939394\n\n\n80959\nzucchini\n34\n0\n29\n781.5\n8\n0.852941\n\n\n80960\nhucchini\n34\n0\n32\n727.5\n8\n0.941176\n\n\n80961\nZurich\n34\n0\n21\n731.5\n6\n0.617647\n\n\n80962\nZurach\n34\n0\n26\n811.0\n6\n0.764706\n\n\n\n\n\n\nIt can be seen that the items occur in word/nonword pairs and the pairs are sorted alphabetically by the word in the pair (ignoring case). We can add the word/nonword status for the items as\n\nbyitem.isword = isodd.(eachindex(byitem.item))\ndescribe(byitem)\n\n8×7 DataFrame\n\n\n\nRow\nvariable\nmean\nmin\nmedian\nmax\nnmissing\neltype\n\n\n\nSymbol\nUnion…\nAny\nUnion…\nAny\nInt64\nDataType\n\n\n\n\n1\nitem\n\nAarod\n\nzuss\n0\nString\n\n\n2\nni\n33.9166\n30\n34.0\n37\n0\nInt64\n\n\n3\nimiss\n0.0169215\n0\n0.0\n2\n0\nInt64\n\n\n4\niacc\n29.0194\n0\n31.0\n37\n0\nInt64\n\n\n5\nimedianrt\n753.069\n458.0\n737.5\n1691.0\n0\nFloat64\n\n\n6\nwrdlen\n7.9988\n1\n8.0\n21\n0\nInt8\n\n\n7\nipropacc\n0.855616\n0.0\n0.911765\n1.0\n0\nFloat64\n\n\n8\nisword\n0.5\nfalse\n0.5\ntrue\n0\nBool\n\n\n\n\n\n\nThis table shows that some of the items were never identified correctly. These are\n\nfilter(:iacc =&gt; iszero, byitem)\n\n9×8 DataFrame\n\n\n\nRow\nitem\nni\nimiss\niacc\nimedianrt\nwrdlen\nipropacc\nisword\n\n\n\nString\nInt64\nInt64\nInt64\nFloat64\nInt8\nFloat64\nBool\n\n\n\n\n1\nbaobab\n34\n0\n0\n616.5\n6\n0.0\ntrue\n\n\n2\nhaulage\n34\n0\n0\n708.5\n7\n0.0\ntrue\n\n\n3\nleitmotif\n35\n0\n0\n688.0\n9\n0.0\ntrue\n\n\n4\nmiasmal\n35\n0\n0\n774.0\n7\n0.0\ntrue\n\n\n5\npeahen\n34\n0\n0\n684.0\n6\n0.0\ntrue\n\n\n6\nplosive\n34\n0\n0\n663.0\n7\n0.0\ntrue\n\n\n7\nplugugly\n33\n0\n0\n709.0\n8\n0.0\ntrue\n\n\n8\nposhest\n34\n0\n0\n740.0\n7\n0.0\ntrue\n\n\n9\nservo\n33\n0\n0\n697.0\n5\n0.0\ntrue\n\n\n\n\n\n\nNotice that these are all words but somewhat obscure words such that none of the subjects exposed to the word identified it correctly.\nWe can incorporate characteristics like wrdlen and isword back into the original trial table with a “left join”. This operation joins two tables by values in a common column. It is called a left join because the left (or first) table takes precedence, in the sense that every row in the left table is present in the result. If there is no matching row in the second table then missing values are inserted for the columns from the right table in the result.\n\ndescribe(\n  leftjoin!(\n    ldttrial,\n    select(byitem, :item, :wrdlen, :isword);\n    on=:item,\n  ),\n)\n\n8×7 DataFrame\n\n\n\nRow\nvariable\nmean\nmin\nmedian\nmax\nnmissing\neltype\n\n\n\nSymbol\nUnion…\nAny\nUnion…\nAny\nInt64\nType\n\n\n\n\n1\nsubj\n409.31\n1\n409.0\n816\n0\nInt16\n\n\n2\nseq\n1687.21\n1\n1687.0\n3374\n0\nInt16\n\n\n3\nacc\n0.85604\nfalse\n1.0\ntrue\n1370\nUnion{Missing, Bool}\n\n\n4\nrt\n846.325\n-16160\n732.0\n32061\n0\nInt16\n\n\n5\nitem\n\nAarod\n\nzuss\n0\nString\n\n\n6\ns2\n0.407128\nfalse\n0.0\ntrue\n0\nBool\n\n\n7\nwrdlen\n7.99835\n1\n8.0\n21\n0\nUnion{Missing, Int8}\n\n\n8\nisword\n0.499995\nfalse\n0.0\ntrue\n0\nUnion{Missing, Bool}\n\n\n\n\n\n\nNotice that the wrdlen and isword variables in this table allow for missing values, because they are derived from the second argument, but there are no missing values for these variables. If there is no need to allow for missing values, there is a slight advantage in disallowing them in the element type, because the code to check for and handle missing values is not needed.\nThis could be done separately for each column or for the whole data frame, as in\n\ndescribe(disallowmissing!(ldttrial; error=false))\n\n8×7 DataFrame\n\n\n\nRow\nvariable\nmean\nmin\nmedian\nmax\nnmissing\neltype\n\n\n\nSymbol\nUnion…\nAny\nUnion…\nAny\nInt64\nType\n\n\n\n\n1\nsubj\n409.31\n1\n409.0\n816\n0\nInt16\n\n\n2\nseq\n1687.21\n1\n1687.0\n3374\n0\nInt16\n\n\n3\nacc\n0.85604\nfalse\n1.0\ntrue\n1370\nUnion{Missing, Bool}\n\n\n4\nrt\n846.325\n-16160\n732.0\n32061\n0\nInt16\n\n\n5\nitem\n\nAarod\n\nzuss\n0\nString\n\n\n6\ns2\n0.407128\nfalse\n0.0\ntrue\n0\nBool\n\n\n7\nwrdlen\n7.99835\n1\n8.0\n21\n0\nInt8\n\n\n8\nisword\n0.499995\nfalse\n0.0\ntrue\n0\nBool\n\n\n\n\n\n\n\n\n\n\n\n\nNamed argument “error”\n\n\n\n\n\nThe named argument error=false is required because there is one column, acc, that does incorporate missing values. If error=false were not given then the error thrown when trying to disallowmissing on the acc column would be propagated and the top-level call would fail.\n\n\n\nA barchart of the word length counts, Figure 1, shows that the majority of the items are between 3 and 14 characters.\n\n\nCode\nlet\n  wlen = 1:21\n  draw(\n    data((; wrdlen=wlen, count=counts(byitem.wrdlen, wlen))) *\n    mapping(:wrdlen =&gt; \"Length of word\", :count) *\n    visual(BarPlot),\n  )\nend\n\n\n\n\n\n\n\n\nFigure 1: Barchart of word lengths in the items used in the lexical decision task.\n\n\n\n\n\nTo examine trends in accuracy by word length we plot the proportion accurate versus word-length separately for words and non-words with the area of each marker proportional to the number of observations for that combination (Figure 2).\n\n\nCode\nlet\n  itemsummry = combine(\n    groupby(byitem, [:wrdlen, :isword]),\n    :ni =&gt; sum,\n    :imiss =&gt; sum,\n    :iacc =&gt; sum,\n  )\n  @transform!(\n    itemsummry,\n    :iacc_mean = :iacc_sum / (:ni_sum - :imiss_sum)\n  )\n  @transform!(itemsummry, :msz = sqrt((:ni_sum - :imiss_sum) / 800))\n  draw(\n    data(itemsummry) * mapping(\n      :wrdlen =&gt; \"Word length\",\n      :iacc_mean =&gt; \"Proportion accurate\";\n      color=:isword,\n      markersize=:msz,\n    );\n    figure=(; resolution=(800, 450)),\n  )\nend\n\n\n\n\n\n\n\n\nFigure 2: Proportion of accurate trials in the LDT versus word length separately for words and non-words. The area of the marker is proportional to the number of observations represented.\n\n\n\n\n\nThe pattern in the range of word lengths with non-negligible counts (there are points in the plot down to word lengths of 1 and up to word lengths of 21 but these points are very small) is that the accuracy for words is nearly constant at about 84% and the accuracy for nonwords is slightly higher until lengths of 13, at which point it falls off a bit.\n\n\n2.2.2 Summaries by subject\nA summary of accuracy and median response time by subject\n\nbysubj = @chain ldttrial begin\n  groupby(:subj)\n  @combine(\n    :ns = length(:acc),               # no. of obs\n    :smiss = count(ismissing, :acc),  # no. of missing acc\n    :sacc = count(skipmissing(:acc)), # no. of accurate\n    :smedianrt = median(:rt),\n  )\n  @transform!(:spropacc = :sacc / :ns)\nend\n\n814×6 DataFrame789 rows omitted\n\n\n\nRow\nsubj\nns\nsmiss\nsacc\nsmedianrt\nspropacc\n\n\n\nInt16\nInt64\nInt64\nInt64\nFloat64\nFloat64\n\n\n\n\n1\n1\n3374\n0\n3158\n554.0\n0.935981\n\n\n2\n2\n3372\n1\n3031\n960.0\n0.898873\n\n\n3\n3\n3372\n3\n3006\n813.0\n0.891459\n\n\n4\n4\n3374\n1\n3062\n619.0\n0.907528\n\n\n5\n5\n3374\n0\n2574\n677.0\n0.762893\n\n\n6\n6\n3374\n0\n2927\n855.0\n0.867516\n\n\n7\n7\n3374\n4\n2877\n918.5\n0.852697\n\n\n8\n8\n3372\n1\n2731\n1310.0\n0.809905\n\n\n9\n9\n3374\n13\n2669\n657.0\n0.791049\n\n\n10\n10\n3374\n0\n2722\n757.0\n0.806758\n\n\n11\n11\n3374\n0\n2894\n632.0\n0.857736\n\n\n12\n12\n3374\n4\n2979\n692.0\n0.882928\n\n\n13\n13\n3374\n2\n2980\n1114.0\n0.883225\n\n\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n\n\n803\n805\n3374\n5\n2881\n534.0\n0.853883\n\n\n804\n806\n3374\n1\n3097\n841.5\n0.917902\n\n\n805\n807\n3374\n3\n2994\n704.0\n0.887374\n\n\n806\n808\n3374\n2\n2751\n630.5\n0.815353\n\n\n807\n809\n3372\n4\n2603\n627.0\n0.771945\n\n\n808\n810\n3374\n1\n3242\n603.5\n0.960877\n\n\n809\n811\n3374\n2\n2861\n827.0\n0.847955\n\n\n810\n812\n3372\n6\n3012\n471.0\n0.893238\n\n\n811\n813\n3372\n4\n2932\n823.0\n0.869514\n\n\n812\n814\n3374\n1\n3070\n773.0\n0.909899\n\n\n813\n815\n3374\n1\n3024\n602.0\n0.896266\n\n\n814\n816\n3374\n0\n2950\n733.0\n0.874333\n\n\n\n\n\n\nshows some anomalies\n\ndescribe(bysubj)\n\n6×7 DataFrame\n\n\n\nRow\nvariable\nmean\nmin\nmedian\nmax\nnmissing\neltype\n\n\n\nSymbol\nFloat64\nReal\nFloat64\nReal\nInt64\nDataType\n\n\n\n\n1\nsubj\n409.311\n1\n409.5\n816\n0\nInt16\n\n\n2\nns\n3373.41\n3370\n3374.0\n3374\n0\nInt64\n\n\n3\nsmiss\n1.68305\n0\n1.0\n22\n0\nInt64\n\n\n4\nsacc\n2886.33\n1727\n2928.0\n3286\n0\nInt64\n\n\n5\nsmedianrt\n760.992\n205.0\n735.0\n1804.0\n0\nFloat64\n\n\n6\nspropacc\n0.855613\n0.511855\n0.868031\n0.973918\n0\nFloat64\n\n\n\n\n\n\nFirst, some subjects are accurate on only about half of their trials, which is the proportion that would be expected from random guessing. A plot of the median response time versus proportion accurate, Figure 3, shows that the subjects with lower accuracy are some of the fastest responders, further indicating that these subjects are sacrificing accuracy for speed.\n\n\nCode\ndraw(\n  data(bysubj) *\n  mapping(\n    :spropacc =&gt; \"Proportion accurate\",\n    :smedianrt =&gt; \"Median response time (ms)\",\n  ) *\n  (visual(Scatter) + smooth())\n)\n\n\n\n\n\n\n\n\nFigure 3: Median response time versus proportion accurate by subject in the LDT.\n\n\n\n\n\nAs described in Balota et al. (2007), the participants performed the trials in blocks of 250 followed by a short break. During the break they were given feedback concerning accuracy and response latency in the previous block of trials. If the accuracy was less than 80% the participant was encouraged to improve their accuracy. Similarly, if the mean response latency was greater than 1000 ms, the participant was encouraged to decrease their response time. During the trials immediate feedback was given if the response was incorrect.\nNevertheless, approximately 15% of the subjects were unable to maintain 80% accuracy on their trials\n\ncount(&lt;(0.8), bysubj.spropacc) / nrow(bysubj)\n\n0.15233415233415235\n\n\nand there is some association of faster response times with low accuracy. The majority of the subjects whose median response time is less than 500 ms. are accurate on less than 75% of their trials. Another way of characterizing the relationship is that none of the subjects with 90% accuracy or greater had a median response time less than 500 ms.\n\nminimum(@subset(bysubj, :spropacc &gt; 0.9).smedianrt)\n\n505.0\n\n\nIt is common in analyses of response latency in a lexical discrimination task to consider only the latencies on correct identifications and to trim outliers. In Balota et al. (2007) a two-stage outlier removal strategy was used; first removing responses less than 200 ms or greater than 3000 ms then removing responses more than three standard deviations from the participant’s mean response.\nAs described in Section 2.2.3 we will analyze these data on a speed scale (the inverse of response time) using only the first-stage outlier removal of response latencies less than 200 ms or greater than 3000 ms. On the speed scale the limits are 0.333 per second up to 5 per second.\nTo examine the effects of the fast but inaccurate responders we will fit models to the data from all the participants and to the data from the 85% of participants who maintained an overall accuracy of 80% or greater.\n\npruned = @chain ldttrial begin\n  @subset(!ismissing(:acc), 200 ≤ :rt ≤ 3000,)\n  leftjoin!(select(bysubj, :subj, :spropacc); on=:subj)\n  dropmissing!\nend\nsize(pruned)\n\n(2714311, 9)\n\n\n\ndescribe(pruned)\n\n9×7 DataFrame\n\n\n\nRow\nvariable\nmean\nmin\nmedian\nmax\nnmissing\neltype\n\n\n\nSymbol\nUnion…\nAny\nUnion…\nAny\nInt64\nDataType\n\n\n\n\n1\nsubj\n409.802\n1\n410.0\n816\n0\nInt16\n\n\n2\nseq\n1684.56\n1\n1684.0\n3374\n0\nInt16\n\n\n3\nacc\n0.859884\nfalse\n1.0\ntrue\n0\nBool\n\n\n4\nrt\n838.712\n200\n733.0\n3000\n0\nInt16\n\n\n5\nitem\n\nAarod\n\nzuss\n0\nString\n\n\n6\ns2\n0.40663\nfalse\n0.0\ntrue\n0\nBool\n\n\n7\nwrdlen\n7.99244\n1\n8.0\n21\n0\nInt8\n\n\n8\nisword\n0.500126\nfalse\n1.0\ntrue\n0\nBool\n\n\n9\nspropacc\n0.857169\n0.511855\n0.869295\n0.973918\n0\nFloat64\n\n\n\n\n\n\n\n\n2.2.3 Choice of response scale\nAs we have indicated, generally the response times are analyzed for the correct identifications only. Furthermore, unrealistically large or small response times are eliminated. For this example we only use the responses between 200 and 3000 ms.\nA density plot of the pruned response times, Figure 4, shows they are skewed to the right.\n\n\nCode\ndraw(\n  data(pruned) *\n  mapping(:rt =&gt; \"Response time (ms.) for correct responses\") *\n  AlgebraOfGraphics.density();\n  figure=(; resolution=(800, 450)),\n)\n\n\n\n\n\n\n\n\nFigure 4: Kernel density plot of the pruned response times (ms.) in the LDT.\n\n\n\n\n\nIn such cases it is common to transform the response to a scale such as the logarithm of the response time or to the speed of the response, which is the inverse of the response time.\nThe density of the response speed, in responses per second, is shown in Figure 5.\n\n\nCode\ndraw(\n  data(pruned) *\n  mapping(\n    :rt =&gt; (x -&gt; 1000 / x) =&gt; \"Response speed (s⁻¹) for correct responses\") *\n  AlgebraOfGraphics.density();\n  figure=(; resolution=(800, 450)),\n)\n\n\n\n\n\n\n\n\nFigure 5: Kernel density plot of the pruned response speed in the LDT.\n\n\n\n\n\nFigure 4 and Figure 5 indicate that it may be more reasonable to establish a lower bound of 1/3 second (333 ms) on the response latency, corresponding to an upper bound of 3 per second on the response speed. However, only about one half of one percent of the correct responses have latencies in the range of 200 ms. to 333 ms.\n\ncount(\n  r -&gt; !ismissing(r.acc) && 200 &lt; r.rt &lt; 333,\n  eachrow(ldttrial),\n) / count(!ismissing, ldttrial.acc)\n\n0.005867195806137328\n\n\nso the exact position of the lower cut-off point on the response latencies is unlikely to be very important.\n\n\n\n\n\n\nUsing inline transformations vs defining new columns\n\n\n\n\n\nIf you examine the code for (fit-elpldtspeeddens?), you will see that the conversion from rt to speed is done inline rather than creating and storing a new variable in the DataFrame.\nI prefer to keep the DataFrame simple with the integer variables (e.g. :rt) if possible.\nI recommend using the StandardizedPredictors.jl capabilities to center numeric variables or convert to zscores.\n\n\n\n\n\n2.2.4 Transformation of response and the form of the model\nAs noted in Box & Cox (1964), a transformation of the response that produces a more Gaussian distribution often will also produce a simpler model structure. For example, Figure 6 shows the smoothed relationship between word length and response time for words and non-words separately,\n\n\nCode\ndraw(\n  data(pruned) *\n  mapping(\n    :wrdlen =&gt; \"Word length\",\n    :rt =&gt; \"Response time (ms)\";\n    :color =&gt; :isword,\n  ) * smooth()\n)\n\n\n\n\n\n\n\n\nFigure 6: Scatterplot smooths of response time versus word length in the LDT.\n\n\n\n\n\nand Figure 7 shows the similar relationships for speed\n\n\nCode\ndraw(\n  data(pruned) *\n  mapping(\n    :wrdlen =&gt; \"Word length\",\n    :rt =&gt; (x -&gt; 1000/x) =&gt; \"Speed of response (s⁻¹)\";\n    :color =&gt; :isword,\n  ) * smooth()\n)\n\n\n\n\n\n\n\n\nFigure 7: Scatterplot smooths of response speed versus word length in the LDT.\n\n\n\n\n\nFor the most part the smoother lines in Figure 7 are reasonably straight. The small amount of curvature is associated with short word lengths, say less than 4 characters, of which there are comparatively few in the study.\nFigure 8 shows a “violin plot” - the empirical density of the response speed by word length separately for words and nonwords. The lines on the plot are fit by linear regression.\n\n\nCode\nlet\n  plt = data(@subset(pruned, :wrdlen &gt; 3, :wrdlen &lt; 14))\n  plt *= mapping(\n    :wrdlen =&gt; \"Word length\",\n    :rt =&gt; (x -&gt; 1000/x) =&gt; \"Speed of response (s⁻¹)\",\n    color=:isword,\n    side=:isword,\n  )\n  plt *= visual(Violin)\n  draw(plt, axis=(; limits=(nothing, (0.0, 2.8))))\nend\n\n\n\n\n\n\n\n\nFigure 8: Empirical density of response speed versus word length by word/non-word status.",
    "crumbs": [
      "Worked examples",
      "A large-scale designed experiment"
    ]
  },
  {
    "objectID": "largescaledesigned.html#sec-ldtinitialmodel",
    "href": "largescaledesigned.html#sec-ldtinitialmodel",
    "title": "1 A large-scale designed experiment",
    "section": "2.3 Models with scalar random effects",
    "text": "2.3 Models with scalar random effects\nA major purpose of the English Lexicon Project is to characterize the items (words or nonwords) according to the observed accuracy of identification and to response latency, taking into account subject-to-subject variability, and to relate these to lexical characteristics of the items.\nIn Balota et al. (2007) the item response latency is characterized by the average response latency from the correct trials after outlier removal.\nMixed-effects models allow us greater flexibility and, we hope, precision in characterizing the items by controlling for subject-to-subject variability and for item characteristics such as word/nonword and item length.\nWe begin with a model that has scalar random effects for item and for subject and incorporates fixed-effects for word/nonword and for item length and for the interaction of these terms.\n\n2.3.1 Establish the contrasts\nBecause there are a large number of items in the data set it is important to assign a Grouping() contrast to item (and, less importantly, to subj). For the isword factor we will use an EffectsCoding contrast with the base level as false. The non-words are assigned -1 in this contrast and the words are assigned +1. The wrdlen covariate is on its original scale but centered at 8 characters.\nThus the (Intercept) coefficient is the predicted speed of response for a typical subject and typical item (without regard to word/non-word status) of 8 characters.\nSet these contrasts\n\ncontrasts = Dict(\n  :subj =&gt; Grouping(),\n  :item =&gt; Grouping(),\n  :isword =&gt; EffectsCoding(; base=false),\n  :wrdlen =&gt; Center(8),\n)\n\nDict{Symbol, Any} with 4 entries:\n  :item   =&gt; Grouping()\n  :wrdlen =&gt; Center(8)\n  :isword =&gt; EffectsCoding(false, nothing)\n  :subj   =&gt; Grouping()\n\n\nand fit a first model with simple, scalar, random effects for subj and item.\n\nelm01 = let\n  form = @formula(\n    1000 / rt ~ 1 + isword * wrdlen + (1 | item) + (1 | subj)\n  )\n  fit(MixedModel, form, pruned; contrasts)\nend\n\nMinimizing 53    Time: 0:00:06 ( 0.12  s/it)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEst.\nSE\nz\np\nσ_item\nσ_subj\n\n\n\n\n(Intercept)\n1.3758\n0.0090\n153.69\n&lt;1e-99\n0.1185\n0.2550\n\n\nisword: true\n0.0625\n0.0005\n131.35\n&lt;1e-99\n\n\n\n\nwrdlen(centered: 8)\n-0.0436\n0.0002\n-225.38\n&lt;1e-99\n\n\n\n\nisword: true & wrdlen(centered: 8)\n-0.0056\n0.0002\n-28.83\n&lt;1e-99\n\n\n\n\nResidual\n0.3781\n\n\n\n\n\n\n\n\n\n\nThe predicted response speed by word length and word/nonword status can be summarized as\n\neffects(Dict(:isword =&gt; [false, true], :wrdlen =&gt; 4:2:12), elm01)\n\n10×6 DataFrame\n\n\n\nRow\nwrdlen\nisword\n1000 / rt\nerr\nlower\nupper\n\n\n\nInt64\nBool\nFloat64\nFloat64\nFloat64\nFloat64\n\n\n\n\n1\n4\nfalse\n1.46555\n0.00903111\n1.45652\n1.47458\n\n\n2\n6\nfalse\n1.38947\n0.00898124\n1.38049\n1.39845\n\n\n3\n8\nfalse\n1.31338\n0.00896459\n1.30442\n1.32235\n\n\n4\n10\nfalse\n1.2373\n0.00898134\n1.22832\n1.24628\n\n\n5\n12\nfalse\n1.16121\n0.00903129\n1.15218\n1.17025\n\n\n6\n4\ntrue\n1.6351\n0.0090311\n1.62607\n1.64413\n\n\n7\n6\ntrue\n1.5367\n0.00898124\n1.52772\n1.54569\n\n\n8\n8\ntrue\n1.43831\n0.00896459\n1.42934\n1.44727\n\n\n9\n10\ntrue\n1.33991\n0.00898133\n1.33092\n1.34889\n\n\n10\n12\ntrue\n1.24151\n0.00903128\n1.23248\n1.25054\n\n\n\n\n\n\nIf we restrict to only those subjects with 80% accuracy or greater the model becomes\n\nelm02 = let\n  form = @formula(\n    1000 / rt ~ 1 + isword * wrdlen + (1 | item) + (1 | subj)\n  )\n  dat = @subset(pruned, :spropacc &gt; 0.8)\n  fit(MixedModel, form, dat; contrasts)\nend\n\nMinimizing 54    Time: 0:00:04 (78.69 ms/it)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEst.\nSE\nz\np\nσ_item\nσ_subj\n\n\n\n\n(Intercept)\n1.3611\n0.0088\n153.98\n&lt;1e-99\n0.1247\n0.2318\n\n\nisword: true\n0.0656\n0.0005\n133.73\n&lt;1e-99\n\n\n\n\nwrdlen(centered: 8)\n-0.0444\n0.0002\n-222.65\n&lt;1e-99\n\n\n\n\nisword: true & wrdlen(centered: 8)\n-0.0057\n0.0002\n-28.73\n&lt;1e-99\n\n\n\n\nResidual\n0.3342\n\n\n\n\n\n\n\n\n\n\n\neffects(Dict(:isword =&gt; [false, true], :wrdlen =&gt; 4:2:12), elm02)\n\n10×6 DataFrame\n\n\n\nRow\nwrdlen\nisword\n1000 / rt\nerr\nlower\nupper\n\n\n\nInt64\nBool\nFloat64\nFloat64\nFloat64\nFloat64\n\n\n\n\n1\n4\nfalse\n1.45036\n0.00892461\n1.44144\n1.45929\n\n\n2\n6\nfalse\n1.37297\n0.00887095\n1.3641\n1.38184\n\n\n3\n8\nfalse\n1.29557\n0.00885302\n1.28672\n1.30443\n\n\n4\n10\nfalse\n1.21818\n0.00887105\n1.20931\n1.22705\n\n\n5\n12\nfalse\n1.14078\n0.0089248\n1.13186\n1.14971\n\n\n6\n4\ntrue\n1.62735\n0.0089246\n1.61842\n1.63627\n\n\n7\n6\ntrue\n1.52702\n0.00887095\n1.51815\n1.53589\n\n\n8\n8\ntrue\n1.4267\n0.00885302\n1.41784\n1.43555\n\n\n9\n10\ntrue\n1.32637\n0.00887104\n1.3175\n1.33524\n\n\n10\n12\ntrue\n1.22605\n0.00892478\n1.21712\n1.23497\n\n\n\n\n\n\nThe differences in the fixed-effects parameter estimates between a model fit to the full data set and one fit to the data from accurate responders only, are small.\nHowever, the random effects for the item, while highly correlated, are not perfectly correlated.\n\n\nCode\nCairoMakie.activate!(; type=\"png\")\ndisallowmissing!(\n  leftjoin!(\n    byitem,\n    leftjoin!(\n      rename!(DataFrame(raneftables(elm01)[:item]), [:item, :elm01]),\n      rename!(DataFrame(raneftables(elm02)[:item]), [:item, :elm02]);\n      on=:item,\n    ),\n    on=:item,\n  ),\n)\ndisallowmissing!(\n  leftjoin!(\n    bysubj,\n    leftjoin!(\n      rename!(DataFrame(raneftables(elm01)[:subj]), [:subj, :elm01]),\n      rename!(DataFrame(raneftables(elm02)[:subj]), [:subj, :elm02]);\n      on=:subj,\n    ),\n    on=:subj,\n  ); error=false,\n)\ndraw(\n  data(byitem) * mapping(\n    :elm01 =&gt; \"Conditional means of item random effects for model elm01\",\n    :elm02 =&gt; \"Conditional means of item random effects for model elm02\";\n    color=:isword,\n  ) * visual(Scatter; alpha=0.2);\n  axis=(; width=600, height=600),\n)\n\n\n\n\n\n\n\n\nFigure 9: Conditional means of scalar random effects for item in model elm01, fit to the pruned data, versus those for model elm02, fit to the pruned data with inaccurate subjects removed.\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nAdjust the alpha on Figure 9.\n\n\nFigure 9 is exactly of the form that would be expected in a sample from a correlated multivariate Gaussian distribution. The correlation of the two sets of conditional means is about 96%.\n\ncor(Matrix(select(byitem, :elm01, :elm02)))\n\n2×2 Matrix{Float64}:\n 1.0       0.958655\n 0.958655  1.0\n\n\nThese models take only a few seconds to fit on a modern laptop computer, which is quite remarkable given the size of the data set and the number of random effects.\nThe amount of time to fit more complex models will be much greater so we may want to move those fits to more powerful server computers. We can split the tasks of fitting and analyzing a model between computers by saving the optimization summary after the model fit and later creating the MixedModel object followed by restoring the optsum object.\n\nsaveoptsum(\"./fits/elm01.json\", elm01);\n\n\nelm01a = restoreoptsum!(\n  let\n    form = @formula(\n      1000 / rt ~ 1 + isword * wrdlen + (1 | item) + (1 | subj)\n    )\n    MixedModel(form, pruned; contrasts)\n  end,\n  \"./fits/elm01.json\",\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEst.\nSE\nz\np\nσ_item\nσ_subj\n\n\n\n\n(Intercept)\n1.3758\n0.0090\n153.69\n&lt;1e-99\n0.1185\n0.2550\n\n\nisword: true\n0.0625\n0.0005\n131.35\n&lt;1e-99\n\n\n\n\nwrdlen(centered: 8)\n-0.0436\n0.0002\n-225.38\n&lt;1e-99\n\n\n\n\nisword: true & wrdlen(centered: 8)\n-0.0056\n0.0002\n-28.83\n&lt;1e-99\n\n\n\n\nResidual\n0.3781\n\n\n\n\n\n\n\n\n\n\nOther covariates associated with the item are available as\n\nelpldtitem = DataFrame(dataset(\"ELP_ldt_item\"))\ndescribe(elpldtitem)\n\n9×7 DataFrame\n\n\n\nRow\nvariable\nmean\nmin\nmedian\nmax\nnmissing\neltype\n\n\n\nSymbol\nUnion…\nAny\nUnion…\nAny\nInt64\nType\n\n\n\n\n1\nitem\n\nAarod\n\nzuss\n0\nString\n\n\n2\nOrtho_N\n1.53309\n0\n1.0\n25\n0\nInt8\n\n\n3\nBG_Sum\n13938.4\n11\n13026.0\n59803\n177\nUnion{Missing, Int32}\n\n\n4\nBG_Mean\n1921.25\n5.5\n1907.0\n6910.0\n177\nUnion{Missing, Float32}\n\n\n5\nBG_Freq_By_Pos\n2043.08\n0\n1928.0\n6985\n4\nUnion{Missing, Int16}\n\n\n6\nitemno\n40481.5\n1\n40481.5\n80962\n0\nInt32\n\n\n7\nisword\n0.5\nfalse\n0.5\ntrue\n0\nBool\n\n\n8\nwrdlen\n7.9988\n1\n8.0\n21\n0\nInt8\n\n\n9\npairno\n20241.0\n1\n20241.0\n40481\n0\nInt32\n\n\n\n\n\n\nand those associated with the subject are\n\nelpldtsubj = DataFrame(dataset(\"ELP_ldt_subj\"))\ndescribe(elpldtsubj)\n\n20×7 DataFrame\n\n\n\nRow\nvariable\nmean\nmin\nmedian\nmax\nnmissing\neltype\n\n\n\nSymbol\nUnion…\nAny\nAny\nAny\nInt64\nType\n\n\n\n\n1\nsubj\n409.311\n1\n409.5\n816\n0\nInt16\n\n\n2\nuniv\n\nKansas\n\nWayne State\n0\nString\n\n\n3\nsex\n\nf\n\nm\n8\nUnion{Missing, String}\n\n\n4\nDOB\n\n1938-06-07\n\n1984-11-14\n0\nDate\n\n\n5\nMEQ\n44.4932\n19.0\n44.0\n75.0\n8\nUnion{Missing, Float32}\n\n\n6\nvision\n5.51169\n0\n6.0\n7\n1\nUnion{Missing, Int8}\n\n\n7\nhearing\n5.86101\n0\n6.0\n7\n1\nUnion{Missing, Int8}\n\n\n8\neducatn\n8.89681\n1\n12.0\n28\n0\nInt8\n\n\n9\nncorrct\n29.8505\n5\n30.0\n40\n18\nUnion{Missing, Int8}\n\n\n10\nrawscor\n31.9925\n13\n32.0\n40\n18\nUnion{Missing, Int8}\n\n\n11\nvocabAge\n17.8123\n10.3\n17.8\n21.0\n19\nUnion{Missing, Float32}\n\n\n12\nshipTime\n3.0861\n0\n3.0\n9\n1\nUnion{Missing, Int8}\n\n\n13\nreadTime\n2.50215\n0.0\n2.0\n15.0\n1\nUnion{Missing, Float32}\n\n\n14\npreshlth\n5.48708\n0\n6.0\n7\n1\nUnion{Missing, Int8}\n\n\n15\npasthlth\n4.92989\n0\n5.0\n7\n1\nUnion{Missing, Int8}\n\n\n16\nS1start\n\n2001-03-16T13:49:27\n2001-10-16T11:38:28.500\n2003-07-29T18:48:44\n0\nDateTime\n\n\n17\nS2start\n\n2001-03-19T10:00:35\n2001-10-19T14:24:19.500\n2003-07-30T13:07:45\n0\nDateTime\n\n\n18\nMEQstrt\n\n2001-03-22T18:32:00\n2001-10-23T11:26:13\n2003-07-30T14:30:49\n7\nUnion{Missing, DateTime}\n\n\n19\nfilename\n\n101DATA.LDT\n\nData998.LDT\n0\nString\n\n\n20\nfrstLang\n\nEnglish\n\nother\n8\nUnion{Missing, String}\n\n\n\n\n\n\nFor the simple model elm01 the estimated standard deviation of the random effects for subject is greater than that of the random effects for item, a common occurrence. A caterpillar plot, Figure 10,\n\n\nCode\nqqcaterpillar!(\n  Figure(resolution=(800, 650)),\n  ranefinfo(elm01, :subj),\n)\n\n\n\n\n\n\n\n\nFigure 10: Conditional means and 95% prediction intervals for subject random effects in elm01.\n\n\n\n\n\nshows definite distinctions between subjects because the widths of the prediction intervals are small compared to the range of the conditional modes. Also, there is at least one outlier with a conditional mode over 1.0.\nFigure 11 is the corresponding caterpillar plot for model elm02 fit to the data with inaccurate responders eliminated.\n\n\nCode\nqqcaterpillar!(\n  Figure(resolution=(800, 650)),\n  ranefinfo(elm02, :subj),\n)\n\n\n\n\n\n\n\n\nFigure 11: Conditional means and 95% prediction intervals for subject random effects in elm02.",
    "crumbs": [
      "Worked examples",
      "A large-scale designed experiment"
    ]
  },
  {
    "objectID": "largescaledesigned.html#random-effects-from-the-simple-model-related-to-covariates",
    "href": "largescaledesigned.html#random-effects-from-the-simple-model-related-to-covariates",
    "title": "1 A large-scale designed experiment",
    "section": "2.4 Random effects from the simple model related to covariates",
    "text": "2.4 Random effects from the simple model related to covariates\nThe random effects “estimates” (technically they are “conditional means”) from the simple model elm01 provide a measure of how much the item or subject differs from the population. (We use elm01 because the main difference between elm01 and elm02 are that some subjects were dropped before fitting elm02.)\nFor the item its length and word/non-word status have already been incorporated in the model. At this point the subjects are just being treated as a homogeneous population.\nThe random effects conditional means have been extracted and incorporated in the byitem and bysubj tables. Now add selected demographic and item-specific measures.\n\nitemextended = leftjoin(\n  byitem,\n  select(elpldtitem, 1:5);\n  on = :item,\n)\nsubjextended = leftjoin(\n  bysubj,\n  select(elpldtsubj, 1:3, :vocabAge);\n  on=:subj,\n)\n\n814×11 DataFrame789 rows omitted\n\n\n\nRow\nsubj\nns\nsmiss\nsacc\nsmedianrt\nspropacc\nelm01\nelm02\nuniv\nsex\nvocabAge\n\n\n\nInt16\nInt64\nInt64\nInt64\nFloat64\nFloat64\nFloat64\nFloat64?\nString?\nString?\nFloat32?\n\n\n\n\n1\n1\n3374\n0\n3158\n554.0\n0.935981\n0.411459\n0.426624\nMorehead\nm\n19.8\n\n\n2\n2\n3372\n1\n3031\n960.0\n0.898873\n-0.30907\n-0.293732\nMorehead\nf\n17.8\n\n\n3\n3\n3372\n3\n3006\n813.0\n0.891459\n-0.153078\n-0.139436\nMorehead\nf\n18.2\n\n\n4\n4\n3374\n1\n3062\n619.0\n0.907528\n0.213047\n0.22754\nMorehead\nf\n18.6\n\n\n5\n5\n3374\n0\n2574\n677.0\n0.762893\n0.0850349\nmissing\nMorehead\nf\n16.2\n\n\n6\n6\n3374\n0\n2927\n855.0\n0.867516\n-0.207356\n-0.192651\nMorehead\nf\n17.8\n\n\n7\n7\n3374\n4\n2877\n918.5\n0.852697\n-0.182201\n-0.166357\nMorehead\nf\n17.4\n\n\n8\n8\n3372\n1\n2731\n1310.0\n0.809905\n-0.541434\n-0.526828\nMorehead\nm\n16.2\n\n\n9\n9\n3374\n13\n2669\n657.0\n0.791049\n0.154926\nmissing\nMorehead\nf\n16.6\n\n\n10\n10\n3374\n0\n2722\n757.0\n0.806758\n-0.0541104\n-0.0403266\nMorehead\nf\n17.0\n\n\n11\n11\n3374\n0\n2894\n632.0\n0.857736\n0.217734\n0.231618\nMorehead\nf\n17.4\n\n\n12\n12\n3374\n4\n2979\n692.0\n0.882928\n0.062351\n0.0770981\nMorehead\nm\n18.2\n\n\n13\n13\n3374\n2\n2980\n1114.0\n0.883225\n-0.409761\n-0.3956\nMorehead\nf\n18.2\n\n\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n\n\n803\n805\n3374\n5\n2881\n534.0\n0.853883\n0.480461\n0.495683\nWash. Univ\nm\n19.0\n\n\n804\n806\n3374\n1\n3097\n841.5\n0.917902\n-0.1888\n-0.173376\nWash. Univ\nm\n19.8\n\n\n805\n807\n3374\n3\n2994\n704.0\n0.887374\n0.01919\n0.0338241\nWash. Univ\nm\n17.4\n\n\n806\n808\n3374\n2\n2751\n630.5\n0.815353\n0.199416\n0.214299\nWash. Univ\nf\n18.6\n\n\n807\n809\n3372\n4\n2603\n627.0\n0.771945\n0.2277\nmissing\nWash. Univ\nm\n15.1\n\n\n808\n810\n3374\n1\n3242\n603.5\n0.960877\n0.252522\n0.266822\nWash. Univ\nm\n19.8\n\n\n809\n811\n3374\n2\n2861\n827.0\n0.847955\n-0.158097\n-0.143568\nWash. Univ\nf\n16.2\n\n\n810\n812\n3372\n6\n3012\n471.0\n0.893238\n0.748427\n0.76354\nWash. Univ\nf\n19.8\n\n\n811\n813\n3372\n4\n2932\n823.0\n0.869514\n-0.167166\n-0.153846\nWash. Univ\nm\n17.4\n\n\n812\n814\n3374\n1\n3070\n773.0\n0.909899\n-0.0753662\n-0.0606956\nWash. Univ\nf\n18.6\n\n\n813\n815\n3374\n1\n3024\n602.0\n0.896266\n0.249134\n0.2643\nWash. Univ\nf\n18.6\n\n\n814\n816\n3374\n0\n2950\n733.0\n0.874333\n-0.0364596\n-0.0222916\nWash. Univ\nf\n17.8\n\n\n\n\n\n\nAs shown in Figure 12, there does not seem to be a strong relationship between vocabulary age and speed of response by subject.\n\n\nCode\ndraw(\n  data(dropmissing(select(subjextended, :elm01, :vocabAge, :sex))) *\n  mapping(\n    :vocabAge =&gt; \"Vocabulary age (yr) of subject\",\n    :elm01 =&gt; \"Random effect in model elm01\";\n    color=:sex,\n  ) * visual(Scatter; alpha=0.6)\n)\n\n\n\n\n\n\n\n\nFigure 12: Random effect for subject in model elm01 versus vocabulary age\n\n\n\n\n\n\n\nCode\ndraw(\n  data(dropmissing(select(subjextended, :elm01, :univ))) *\n  mapping(\n    :elm01 =&gt; \"Random effect in model elm01\";\n    color=:univ =&gt; \"University\",\n  ) * AlgebraOfGraphics.density()\n)\n\n\n\n\n\n\n\n\nFigure 13: Estimated density of random effects for subject in model elm01 by university\n\n\n\n\n\n\n\nCode\ndraw(\n  data(dropmissing(select(subjextended, :elm02, :univ))) *\n  mapping(\n    :elm02 =&gt; \"Random effect in model elm02 (accurate responders only)\";\n    color=:univ =&gt; \"University\",\n  ) * AlgebraOfGraphics.density()\n)\n\n\n\n\n\n\n\n\nFigure 14: Estimated density of random effects for subject in model elm02, fit to accurate responders only, by university\n\n\n\n\n\n\n\nCode\nCairoMakie.activate!(; type=\"png\")\ndraw(\n  data(dropmissing(select(itemextended, :elm01, :BG_Mean, :isword))) *\n  mapping(\n    :BG_Mean =&gt; \"Mean bigram frequency\",\n    :elm01 =&gt; \"Random effect in model elm01\";\n    color=:isword,\n  ) * visual(Scatter; alpha=0.2)\n)\n\n\n\n\n\n\n\n\nFigure 15: Random effect in model elm01 versus mean bigram frequency, by word/nonword status",
    "crumbs": [
      "Worked examples",
      "A large-scale designed experiment"
    ]
  },
  {
    "objectID": "shrinkageplot.html",
    "href": "shrinkageplot.html",
    "title": "More on shrinkage plots",
    "section": "",
    "text": "I have stated that the likelihood criterion used to fit linear mixed-effects can be considered as balancing fidelity to the data (i.e. fits the observed data well) versus model complexity.\nThis is similar to some of the criterion used in Machine Learning (ML), except that the criterion for LMMs has a rigorous mathematical basis.\nIn the shrinkage plot we consider the values of the random-effects coefficients for the fitted values of the model versus those from a model in which there is no penalty for model complexity.\nIf there is strong subject-to-subject variation then the model fit will tend to values of the random effects similar to those without a penalty on complexity.\nIf the random effects term is not contributing much (i.e. it is “inert”) then the random effects will be shrunk considerably towards zero in some directions.\nCode\nusing CairoMakie\nusing DataFrames\nusing LinearAlgebra\nusing MixedModels\nusing MixedModelsMakie\nusing Random\nusing ProgressMeter\n\nProgressMeter.ijulia_behavior(:clear);\nLoad the kb07 data set (don’t tell Reinhold that I used these data).\nkb07 = MixedModels.dataset(:kb07)\n\nArrow.Table with 1789 rows, 7 columns, and schema:\n :subj      String\n :item      String\n :spkr      String\n :prec      String\n :load      String\n :rt_trunc  Int16\n :rt_raw    Int16\ncontrasts = Dict(\n  :subj =&gt; Grouping(),\n  :item =&gt; Grouping(),\n  :spkr =&gt; HelmertCoding(),\n  :prec =&gt; HelmertCoding(),\n  :load =&gt; HelmertCoding(),\n)\nm1 = let\n  form = @formula(\n    rt_trunc ~\n      1 +\n      spkr * prec * load +\n      (1 + spkr + prec + load | subj) +\n      (1 + spkr + prec + load | item)\n  )\n  fit(MixedModel, form, kb07; contrasts)\nend\n\nMinimizing 874    Time: 0:00:01 ( 1.43 ms/it)\n  objective:  28637.971010073215\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEst.\nSE\nz\np\nσ_subj\nσ_item\n\n\n\n\n(Intercept)\n2181.6424\n77.3515\n28.20\n&lt;1e-99\n301.8721\n362.4695\n\n\nspkr: old\n67.7496\n17.9607\n3.77\n0.0002\n33.0588\n41.1159\n\n\nprec: maintain\n-333.9200\n47.1563\n-7.08\n&lt;1e-11\n58.8512\n247.3299\n\n\nload: yes\n78.8007\n19.7269\n3.99\n&lt;1e-04\n66.9526\n43.3991\n\n\nspkr: old & prec: maintain\n-21.9960\n15.8191\n-1.39\n0.1644\n\n\n\n\nspkr: old & load: yes\n18.3832\n15.8191\n1.16\n0.2452\n\n\n\n\nprec: maintain & load: yes\n4.5327\n15.8191\n0.29\n0.7745\n\n\n\n\nspkr: old & prec: maintain & load: yes\n23.6377\n15.8191\n1.49\n0.1351\n\n\n\n\nResidual\n669.0515\nVarCorr(m1)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nColumn\nVariance\nStd.Dev\nCorr.\n\n\n\n\n\n\nsubj\n(Intercept)\n91126.7545\n301.8721\n\n\n\n\n\n\nspkr: old\n1092.8862\n33.0588\n+1.00\n\n\n\n\n\nprec: maintain\n3463.4603\n58.8512\n-0.62\n-0.62\n\n\n\n\nload: yes\n4482.6493\n66.9526\n+0.36\n+0.36\n+0.51\n\n\nitem\n(Intercept)\n131384.1084\n362.4695\n\n\n\n\n\n\nspkr: old\n1690.5210\n41.1159\n+0.42\n\n\n\n\n\nprec: maintain\n61172.0781\n247.3299\n-0.69\n+0.37\n\n\n\n\nload: yes\n1883.4785\n43.3991\n+0.29\n+0.14\n-0.13\n\n\nResidual\n\n447629.9270\n669.0515\nissingular(m1)\n\ntrue\nprint(m1)\n\nLinear mixed model fit by maximum likelihood\n rt_trunc ~ 1 + spkr + prec + load + spkr & prec + spkr & load + prec & load + spkr & prec & load + (1 + spkr + prec + load | subj) + (1 + spkr + prec + load | item)\n    logLik   -2 logLik      AIC         AICc        BIC     \n -14318.9855  28637.9710  28695.9710  28696.9602  28855.1640\n\nVariance components:\n             Column       Variance  Std.Dev.   Corr.\nsubj     (Intercept)      91126.7545 301.8721\n         spkr: old         1092.8862  33.0588 +1.00\n         prec: maintain    3463.4603  58.8512 -0.62 -0.62\n         load: yes         4482.6493  66.9526 +0.36 +0.36 +0.51\nitem     (Intercept)     131384.1084 362.4695\n         spkr: old         1690.5210  41.1159 +0.42\n         prec: maintain   61172.0781 247.3299 -0.69 +0.37\n         load: yes         1883.4785  43.3991 +0.29 +0.14 -0.13\nResidual                 447629.9270 669.0515\n Number of obs: 1789; levels of grouping factors: 56, 32\n\n  Fixed-effects parameters:\n───────────────────────────────────────────────────────────────────────────────\n                                             Coef.  Std. Error      z  Pr(&gt;|z|)\n───────────────────────────────────────────────────────────────────────────────\n(Intercept)                             2181.64        77.3515  28.20    &lt;1e-99\nspkr: old                                 67.7496      17.9607   3.77    0.0002\nprec: maintain                          -333.92        47.1563  -7.08    &lt;1e-11\nload: yes                                 78.8007      19.7269   3.99    &lt;1e-04\nspkr: old & prec: maintain               -21.996       15.8191  -1.39    0.1644\nspkr: old & load: yes                     18.3832      15.8191   1.16    0.2452\nprec: maintain & load: yes                 4.53273     15.8191   0.29    0.7745\nspkr: old & prec: maintain & load: yes    23.6377      15.8191   1.49    0.1351\n───────────────────────────────────────────────────────────────────────────────",
    "crumbs": [
      "Visualizations and diagnostics",
      "More on shrinkage plots"
    ]
  },
  {
    "objectID": "shrinkageplot.html#expressing-the-covariance-of-random-effects",
    "href": "shrinkageplot.html#expressing-the-covariance-of-random-effects",
    "title": "More on shrinkage plots",
    "section": "1 Expressing the covariance of random effects",
    "text": "1 Expressing the covariance of random effects\nEarlier today we mentioned that the parameters being optimized are from a “matrix square root” of the covariance matrix for the random effects. There is one such lower triangular matrix for each grouping factor.\n\nl1 = first(m1.λ)   # Cholesky factor of relative covariance for subj\n\n4×4 LowerTriangular{Float64, Matrix{Float64}}:\n  0.451194     ⋅          ⋅          ⋅ \n  0.0494115   0.0         ⋅          ⋅ \n -0.0547474  -0.0357256  0.0588535   ⋅ \n  0.0359075  -0.0484787  0.0798414  0.0\n\n\nNotice the zero on the diagonal. A triangular matrix with zeros on the diagonal is singular.\n\nl2 = last(m1.λ)    # this one is also singular\n\n4×4 LowerTriangular{Float64, Matrix{Float64}}:\n  0.541766    ⋅            ⋅          ⋅ \n  0.0259255  0.0557178     ⋅          ⋅ \n -0.253795   0.26783      0.0226493   ⋅ \n  0.0189316  0.000867924  0.0620364  0.0\n\n\nTo regenerate the covariance matrix we need to know that the covariance is not the square of l1, it is l1 * l1' (so that the result is symmetric) and multiplied by σ̂²\n\nΣ₁ = varest(m1) .* (l1 * l1')\n\n4×4 Matrix{Float64}:\n  91126.8    9979.54   -11057.2   7252.16\n   9979.54   1092.89    -1210.91   794.203\n -11057.2   -1210.91     3463.46  1998.68\n   7252.16    794.203    1998.68  4482.65\n\n\n\ndiag(Σ₁)  # compare to the variance column in the VarCorr output\n\n4-element Vector{Float64}:\n 91126.75451781915\n  1092.8862206704753\n  3463.4602904990634\n  4482.649272250181\n\n\n\nsqrt.(diag(Σ₁))\n\n4-element Vector{Float64}:\n 301.8720830381954\n  33.05882969299542\n  58.85117068078649\n  66.95258973520129",
    "crumbs": [
      "Visualizations and diagnostics",
      "More on shrinkage plots"
    ]
  },
  {
    "objectID": "shrinkageplot.html#shrinkage-plots",
    "href": "shrinkageplot.html#shrinkage-plots",
    "title": "More on shrinkage plots",
    "section": "2 Shrinkage plots",
    "text": "2 Shrinkage plots\n\n\nCode\nshrinkageplot(m1)\n\n\n\n\n\n\n\n\nFigure 1: Shrinkage plot of model m1\n\n\n\n\n\nThe upper left panel shows the perfect negative correlation for those two components of the random effects.\n\nshrinkageplot(m1, :item)\n\n\n\n\n\n\n\n\n\nX1 = Int.(m1.X')\n\n8×1789 Matrix{Int64}:\n  1   1   1   1   1  1   1   1   1   1  …   1   1   1   1   1   1   1  1   1\n -1   1   1  -1  -1  1   1  -1  -1   1      1  -1  -1   1   1  -1  -1  1   1\n -1   1  -1   1  -1  1  -1   1  -1   1     -1   1  -1   1  -1   1  -1  1  -1\n  1  -1  -1  -1  -1  1   1   1   1  -1      1   1   1  -1  -1  -1  -1  1   1\n  1   1  -1  -1   1  1  -1  -1   1   1     -1  -1   1   1  -1  -1   1  1  -1\n -1  -1  -1   1   1  1   1  -1  -1  -1  …   1  -1  -1  -1  -1   1   1  1   1\n -1  -1   1  -1   1  1  -1   1  -1  -1     -1   1  -1  -1   1  -1   1  1  -1\n  1  -1   1   1  -1  1  -1  -1   1  -1     -1  -1   1  -1   1   1  -1  1  -1\n\n\n\nX1 * X1'\n\n8×8 Matrix{Int64}:\n 1789    -1    -1     3    -3     1     1     3\n   -1  1789    -3     1    -1     3     3     1\n   -1    -3  1789     1    -1     3     3     1\n    3     1     1  1789     3    -1    -1    -3\n   -3    -1    -1     3  1789     1     1     3\n    1     3     3    -1     1  1789    -3    -1\n    1     3     3    -1     1    -3  1789    -1\n    3     1     1    -3     3    -1    -1  1789",
    "crumbs": [
      "Visualizations and diagnostics",
      "More on shrinkage plots"
    ]
  },
  {
    "objectID": "shrinkageplot.html#how-to-interpret-a-shrinkage-plot",
    "href": "shrinkageplot.html#how-to-interpret-a-shrinkage-plot",
    "title": "More on shrinkage plots",
    "section": "3 How to interpret a shrinkage plot",
    "text": "3 How to interpret a shrinkage plot\n\nExtreme shrinkage (shrunk to a line or to a point) is easy to interpret - the term is not providing benefit and can be removed.\nWhen the range of the blue dots (shrunk values) is comparable to those of the red dots (unshrunk) it indicates that the term after shrinkage is about as strong as without shrinkage.\nBy itself, this doesn’t mean that the term is important. In some ways you need to get a feeling for the absolute magnitude of the random effects in addition to the relative magnitude.\nSmall magnitude and small relative magnitude indicate you can drop that term",
    "crumbs": [
      "Visualizations and diagnostics",
      "More on shrinkage plots"
    ]
  },
  {
    "objectID": "shrinkageplot.html#conclusions-from-these-plots",
    "href": "shrinkageplot.html#conclusions-from-these-plots",
    "title": "More on shrinkage plots",
    "section": "4 Conclusions from these plots",
    "text": "4 Conclusions from these plots\n\nOnly the intercept for the subj appears to be contributing explanatory power\nFor the item both the intercept and the spkr appear to be contributing\n\n\nm2 = let\n  form = @formula(\n    rt_trunc ~\n      1 + prec * spkr * load + (1 | subj) + (1 + prec | item)\n  )\n  fit(MixedModel, form, kb07; contrasts)\nend\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEst.\nSE\nz\np\nσ_item\nσ_subj\n\n\n\n\n(Intercept)\n2181.7582\n77.4709\n28.16\n&lt;1e-99\n364.7286\n298.1109\n\n\nprec: maintain\n-333.8582\n47.4629\n-7.03\n&lt;1e-11\n252.6687\n\n\n\nspkr: old\n67.8114\n16.0526\n4.22\n&lt;1e-04\n\n\n\n\nload: yes\n78.6849\n16.0525\n4.90\n&lt;1e-06\n\n\n\n\nprec: maintain & spkr: old\n-21.8802\n16.0525\n-1.36\n0.1729\n\n\n\n\nprec: maintain & load: yes\n4.4710\n16.0526\n0.28\n0.7806\n\n\n\n\nspkr: old & load: yes\n18.3214\n16.0526\n1.14\n0.2537\n\n\n\n\nprec: maintain & spkr: old & load: yes\n23.5219\n16.0525\n1.47\n0.1428\n\n\n\n\nResidual\n678.9318\n\n\n\n\n\n\n\n\n\n\n\nVarCorr(m2)\n\n\n\n\n\nColumn\nVariance\nStd.Dev\nCorr.\n\n\n\n\nitem\n(Intercept)\n133026.918\n364.729\n\n\n\n\nprec: maintain\n63841.496\n252.669\n-0.70\n\n\nsubj\n(Intercept)\n88870.080\n298.111\n\n\n\nResidual\n\n460948.432\n678.932\n\n\n\n\n\n\n\n\nCode\nshrinkageplot(m2)\n\n\n\n\n\n\n\n\nFigure 2: Shrinkage plot of model m2\n\n\n\n\n\n\nm3 = let\n  form = @formula(\n    rt_trunc ~\n      1 + prec + spkr + load + (1 | subj) + (1 + prec | item)\n  )\n  fit(MixedModel, form, kb07; contrasts)\nend\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEst.\nSE\nz\np\nσ_item\nσ_subj\n\n\n\n\n(Intercept)\n2181.8526\n77.4681\n28.16\n&lt;1e-99\n364.7126\n298.0259\n\n\nprec: maintain\n-333.7906\n47.4472\n-7.03\n&lt;1e-11\n252.5212\n\n\n\nspkr: old\n67.8790\n16.0785\n4.22\n&lt;1e-04\n\n\n\n\nload: yes\n78.5904\n16.0785\n4.89\n&lt;1e-05\n\n\n\n\nResidual\n680.0319\n\n\n\n\n\n\n\n\n\n\n\nVarCorr(m3)\n\n\n\n\n\nColumn\nVariance\nStd.Dev\nCorr.\n\n\n\n\nitem\n(Intercept)\n133015.244\n364.713\n\n\n\n\nprec: maintain\n63766.937\n252.521\n-0.70\n\n\nsubj\n(Intercept)\n88819.436\n298.026\n\n\n\nResidual\n\n462443.388\n680.032\n\n\n\n\n\n\n\nrng = Random.seed!(1234321);\n\n\nm3btstrp = parametricbootstrap(rng, 2000, m3);\n\n\nDataFrame(shortestcovint(m3btstrp))\n\n9×5 DataFrame\n\n\n\nRow\ntype\ngroup\nnames\nlower\nupper\n\n\n\nString\nString?\nString?\nFloat64\nFloat64\n\n\n\n\n1\nβ\nmissing\n(Intercept)\n2013.95\n2319.53\n\n\n2\nβ\nmissing\nprec: maintain\n-429.807\n-241.429\n\n\n3\nβ\nmissing\nspkr: old\n35.3339\n95.7272\n\n\n4\nβ\nmissing\nload: yes\n47.067\n111.04\n\n\n5\nσ\nitem\n(Intercept)\n267.788\n452.9\n\n\n6\nσ\nitem\nprec: maintain\n171.547\n314.7\n\n\n7\nρ\nitem\n(Intercept), prec: maintain\n-0.893081\n-0.457084\n\n\n8\nσ\nsubj\n(Intercept)\n235.921\n364.717\n\n\n9\nσ\nresidual\nmissing\n657.736\n703.054\n\n\n\n\n\n\n\nridgeplot(m3btstrp)\n\n\n\n\n\n\n\nFigure 3: Ridge plot of the fixed-effects coefficients from the bootstrap sample\n\n\n\n\n\n\nridgeplot(m3btstrp; show_intercept=false)\n\n\n\n\n\n\n\nFigure 4: Ridge plot of the fixed-effects coefficients from the bootstrap sample (with the intercept)\n\n\n\n\n\n\nm4 = let\n  form = @formula(\n    rt_trunc ~\n      1 + prec + spkr + load + (1 + prec | item) + (1 | subj)\n  )\n  fit(MixedModel, form, kb07; contrasts)\nend\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEst.\nSE\nz\np\nσ_item\nσ_subj\n\n\n\n\n(Intercept)\n2181.8526\n77.4681\n28.16\n&lt;1e-99\n364.7126\n298.0259\n\n\nprec: maintain\n-333.7906\n47.4472\n-7.03\n&lt;1e-11\n252.5212\n\n\n\nspkr: old\n67.8790\n16.0785\n4.22\n&lt;1e-04\n\n\n\n\nload: yes\n78.5904\n16.0785\n4.89\n&lt;1e-05\n\n\n\n\nResidual\n680.0319\n\n\n\n\n\n\n\n\n\n\n\nm4bstrp = parametricbootstrap(rng, 2000, m4);\n\n\nridgeplot(m4bstrp; show_intercept=false)\n\n\n\n\n\n\n\n\n\nDataFrame(shortestcovint(m4bstrp))\n\n9×5 DataFrame\n\n\n\nRow\ntype\ngroup\nnames\nlower\nupper\n\n\n\nString\nString?\nString?\nFloat64\nFloat64\n\n\n\n\n1\nβ\nmissing\n(Intercept)\n2030.75\n2342.66\n\n\n2\nβ\nmissing\nprec: maintain\n-432.619\n-249.063\n\n\n3\nβ\nmissing\nspkr: old\n35.4729\n97.9576\n\n\n4\nβ\nmissing\nload: yes\n47.0469\n108.272\n\n\n5\nσ\nitem\n(Intercept)\n261.52\n444.426\n\n\n6\nσ\nitem\nprec: maintain\n177.803\n318.382\n\n\n7\nρ\nitem\n(Intercept), prec: maintain\n-0.904897\n-0.477346\n\n\n8\nσ\nsubj\n(Intercept)\n234.301\n361.964\n\n\n9\nσ\nresidual\nmissing\n657.11\n701.879\n\n\n\n\n\n\n\nVarCorr(m4)\n\n\n\n\n\nColumn\nVariance\nStd.Dev\nCorr.\n\n\n\n\nitem\n(Intercept)\n133015.244\n364.713\n\n\n\n\nprec: maintain\n63766.937\n252.521\n-0.70\n\n\nsubj\n(Intercept)\n88819.436\n298.026\n\n\n\nResidual\n\n462443.388\n680.032\n\n\n\n\n\n\n\n\nCode\nlet mods = [m1, m2, m4]\n  DataFrame(;\n    geomdof=(sum ∘ leverage).(mods),\n    npar=dof.(mods),\n    deviance=deviance.(mods),\n    AIC=aic.(mods),\n    BIC=bic.(mods),\n    AICc=aicc.(mods),\n  )\nend\n\n\n3×6 DataFrame\n\n\n\nRow\ngeomdof\nnpar\ndeviance\nAIC\nBIC\nAICc\n\n\n\nFloat64\nInt64\nFloat64\nFloat64\nFloat64\nFloat64\n\n\n\n\n1\n130.126\n29\n28638.0\n28696.0\n28855.2\n28697.0\n\n\n2\n107.543\n13\n28658.5\n28684.5\n28755.8\n28684.7\n\n\n3\n103.478\n9\n28663.9\n28681.9\n28731.3\n28682.0\n\n\n\n\n\n\n\nscatter(fitted(m4), residuals(m4))\n\n\n\n\n\n\n\nFigure 5: Residuals versus fitted values for model m4",
    "crumbs": [
      "Visualizations and diagnostics",
      "More on shrinkage plots"
    ]
  },
  {
    "objectID": "glmm.html",
    "href": "glmm.html",
    "title": "Generalized linear mixed models",
    "section": "",
    "text": "Load the packages to be used\nCode\nusing AlgebraOfGraphics\nusing CairoMakie\nusing DataFrameMacros\nusing DataFrames\nusing MixedModels\nusing MixedModelsMakie\nusing SMLP2023: dataset\n\nCairoMakie.activate!(; type=\"svg\")\n\nimport ProgressMeter\nProgressMeter.ijulia_behavior(:clear)",
    "crumbs": [
      "Generalized linear mixed models"
    ]
  },
  {
    "objectID": "glmm.html#matrix-notation-for-the-sleepstudy-model",
    "href": "glmm.html#matrix-notation-for-the-sleepstudy-model",
    "title": "Generalized linear mixed models",
    "section": "1 Matrix notation for the sleepstudy model",
    "text": "1 Matrix notation for the sleepstudy model\n\nsleepstudy = DataFrame(dataset(:sleepstudy))\n\n180×3 DataFrame155 rows omitted\n\n\n\nRow\nsubj\ndays\nreaction\n\n\n\nString\nInt8\nFloat64\n\n\n\n\n1\nS308\n0\n249.56\n\n\n2\nS308\n1\n258.705\n\n\n3\nS308\n2\n250.801\n\n\n4\nS308\n3\n321.44\n\n\n5\nS308\n4\n356.852\n\n\n6\nS308\n5\n414.69\n\n\n7\nS308\n6\n382.204\n\n\n8\nS308\n7\n290.149\n\n\n9\nS308\n8\n430.585\n\n\n10\nS308\n9\n466.353\n\n\n11\nS309\n0\n222.734\n\n\n12\nS309\n1\n205.266\n\n\n13\nS309\n2\n202.978\n\n\n⋮\n⋮\n⋮\n⋮\n\n\n169\nS371\n8\n350.781\n\n\n170\nS371\n9\n369.469\n\n\n171\nS372\n0\n269.412\n\n\n172\nS372\n1\n273.474\n\n\n173\nS372\n2\n297.597\n\n\n174\nS372\n3\n310.632\n\n\n175\nS372\n4\n287.173\n\n\n176\nS372\n5\n329.608\n\n\n177\nS372\n6\n334.482\n\n\n178\nS372\n7\n343.22\n\n\n179\nS372\n8\n369.142\n\n\n180\nS372\n9\n364.124\n\n\n\n\n\n\n\ncontrasts = Dict(:subj =&gt; Grouping())\nm1 = let f = @formula reaction ~ 1 + days + (1 + days | subj)\n  fit(MixedModel, f, sleepstudy; contrasts)\nend\nprintln(m1)\n\nMinimizing 57    Time: 0:00:00 ( 3.78 ms/it)\n\n\nLinear mixed model fit by maximum likelihood\n reaction ~ 1 + days + (1 + days | subj)\n   logLik   -2 logLik     AIC       AICc        BIC    \n  -875.9697  1751.9393  1763.9393  1764.4249  1783.0971\n\nVariance components:\n            Column    Variance Std.Dev.   Corr.\nsubj     (Intercept)  565.51067 23.78047\n         days          32.68212  5.71683 +0.08\nResidual              654.94145 25.59182\n Number of obs: 180; levels of grouping factors: 18\n\n  Fixed-effects parameters:\n──────────────────────────────────────────────────\n                Coef.  Std. Error      z  Pr(&gt;|z|)\n──────────────────────────────────────────────────\n(Intercept)  251.405      6.63226  37.91    &lt;1e-99\ndays          10.4673     1.50224   6.97    &lt;1e-11\n──────────────────────────────────────────────────\n\n\nThe response vector, y, has 180 elements. The fixed-effects coefficient vector, β, has 2 elements and the fixed-effects model matrix, X, is of size 180 × 2.\n\nm1.y\n\n180-element view(::Matrix{Float64}, :, 3) with eltype Float64:\n 249.56\n 258.7047\n 250.8006\n 321.4398\n 356.8519\n 414.6901\n 382.2038\n 290.1486\n 430.5853\n 466.3535\n 222.7339\n 205.2658\n 202.9778\n   ⋮\n 350.7807\n 369.4692\n 269.4117\n 273.474\n 297.5968\n 310.6316\n 287.1726\n 329.6076\n 334.4818\n 343.2199\n 369.1417\n 364.1236\n\n\n\nm1.β\n\n2-element Vector{Float64}:\n 251.4051048484836\n  10.467285959597044\n\n\n\nm1.X\n\n180×2 Matrix{Float64}:\n 1.0  0.0\n 1.0  1.0\n 1.0  2.0\n 1.0  3.0\n 1.0  4.0\n 1.0  5.0\n 1.0  6.0\n 1.0  7.0\n 1.0  8.0\n 1.0  9.0\n 1.0  0.0\n 1.0  1.0\n 1.0  2.0\n ⋮    \n 1.0  8.0\n 1.0  9.0\n 1.0  0.0\n 1.0  1.0\n 1.0  2.0\n 1.0  3.0\n 1.0  4.0\n 1.0  5.0\n 1.0  6.0\n 1.0  7.0\n 1.0  8.0\n 1.0  9.0\n\n\nThe second column of X is just the days vector and the first column is all 1’s.\nThere are 36 random effects, 2 for each of the 18 levels of subj. The “estimates” (technically, the conditional means or conditional modes) are returned as a vector of matrices, one matrix for each grouping factor. In this case there is only one grouping factor for the random effects so there is one one matrix which contains 18 intercept random effects and 18 slope random effects.\n\nm1.b\n\n1-element Vector{Matrix{Float64}}:\n [2.8158198796389 -40.04844205814212 … 0.7232619479594231 12.118907795182105; 9.075511561300857 -8.644079376096018 … -0.9710526156139855 1.3106980690771255]\n\n\n\nonly(m1.b)   # only one grouping factor\n\n2×18 Matrix{Float64}:\n 2.81582  -40.0484   -38.4331  22.8321   …  -24.7101   0.723262  12.1189\n 9.07551   -8.64408   -5.5134  -4.65872       4.6597  -0.971053   1.3107\n\n\nThere is a model matrix, Z, for the random effects. In general it has one chunk of columns for the first grouping factor, a chunk of columns for the second grouping factor, etc.\nIn this case there is only one grouping factor.\n\nInt.(first(m1.reterms))\n\n180×36 Matrix{Int64}:\n 1  0  0  0  0  0  0  0  0  0  0  0  0  …  0  0  0  0  0  0  0  0  0  0  0  0\n 1  1  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  0  0\n 1  2  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  0  0\n 1  3  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  0  0\n 1  4  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  0  0\n 1  5  0  0  0  0  0  0  0  0  0  0  0  …  0  0  0  0  0  0  0  0  0  0  0  0\n 1  6  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  0  0\n 1  7  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  0  0\n 1  8  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  0  0\n 1  9  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  0  0\n 0  0  1  0  0  0  0  0  0  0  0  0  0  …  0  0  0  0  0  0  0  0  0  0  0  0\n 0  0  1  1  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  0  0\n 0  0  1  2  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  0  0\n ⋮              ⋮              ⋮        ⋱     ⋮              ⋮              ⋮\n 0  0  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  1  8  0  0\n 0  0  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  1  9  0  0\n 0  0  0  0  0  0  0  0  0  0  0  0  0  …  0  0  0  0  0  0  0  0  0  0  1  0\n 0  0  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  1  1\n 0  0  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  1  2\n 0  0  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  1  3\n 0  0  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  1  4\n 0  0  0  0  0  0  0  0  0  0  0  0  0  …  0  0  0  0  0  0  0  0  0  0  1  5\n 0  0  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  1  6\n 0  0  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  1  7\n 0  0  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  1  8\n 0  0  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  1  9\n\n\nThe defining property of a linear model or linear mixed model is that the fitted values are linear combinations of the fixed-effects parameters and the random effects. We can write the fitted values as\n\nm1.X * m1.β + only(m1.reterms) * vec(only(m1.b))\n\n180-element Vector{Float64}:\n 254.2209247281225\n 273.7637222490204\n 293.30651976991834\n 312.8493172908162\n 332.3921148117141\n 351.93491233261204\n 371.4777098535099\n 391.02050737440777\n 410.5633048953057\n 430.1061024162036\n 211.3566627903415\n 213.1798693738425\n 215.00307595734353\n   ⋮\n 328.0982335483075\n 337.59446689229054\n 263.5240126436657\n 275.3019966723399\n 287.07998070101405\n 298.8579647296882\n 310.63594875836236\n 322.4139327870366\n 334.1919168157107\n 345.9699008443849\n 357.7478848730591\n 369.5258689017332\n\n\n\nfitted(m1)   # just to check that these are indeed the same as calculated above\n\n180-element Vector{Float64}:\n 254.2209247281225\n 273.7637222490204\n 293.30651976991834\n 312.8493172908162\n 332.3921148117141\n 351.93491233261204\n 371.4777098535099\n 391.0205073744078\n 410.56330489530575\n 430.1061024162036\n 211.3566627903415\n 213.1798693738425\n 215.00307595734355\n   ⋮\n 328.0982335483075\n 337.59446689229054\n 263.5240126436657\n 275.3019966723399\n 287.07998070101405\n 298.8579647296882\n 310.63594875836236\n 322.4139327870366\n 334.1919168157107\n 345.9699008443849\n 357.7478848730591\n 369.5258689017332\n\n\nIn symbols we would write the linear predictor expression as \\[\n\\boldsymbol{\\eta} = \\mathbf{X}\\boldsymbol{\\beta} +\\mathbf{Z b}\n\\] where \\(\\boldsymbol{\\eta}\\) has 180 elements, \\(\\boldsymbol{\\beta}\\) has 2 elements, \\(\\bf b\\) has 36 elements, \\(\\bf X\\) is of size 180 × 2 and \\(\\bf Z\\) is of size 180 × 36.\nFor a linear model or linear mixed model the linear predictor is the mean response, \\(\\boldsymbol\\mu\\). That is, we can write the probability model in terms of a 180-dimensional random variable, \\(\\mathcal Y\\), for the response and a 36-dimensional random variable, \\(\\mathcal B\\), for the random effects as \\[\n\\begin{aligned}\n(\\mathcal{Y} | \\mathcal{B}=\\bf{b}) &\\sim\\mathcal{N}(\\bf{ X\\boldsymbol\\beta + Z b},\\sigma^2\\bf{I})\\\\\\\\\n\\mathcal{B}&\\sim\\mathcal{N}(\\bf{0},\\boldsymbol{\\Sigma}_{\\boldsymbol\\theta}) .\n\\end{aligned}\n\\] where \\(\\boldsymbol{\\Sigma}_\\boldsymbol{\\theta}\\) is a 36 × 36 symmetric covariance matrix that has a special form - it consists of 18 diagonal blocks, each of size 2 × 2 and all the same.\nRecall that this symmetric matrix can be constructed from the parameters \\(\\boldsymbol\\theta\\), which generate the lower triangular matrix \\(\\boldsymbol\\lambda\\), and the estimate \\(\\widehat{\\sigma^2}\\).\n\nm1.θ\n\n3-element Vector{Float64}:\n 0.9292213124664976\n 0.01816838673115054\n 0.22264486480463275\n\n\n\nλ = only(m1.λ)  # with multiple grouping factors there will be multiple λ's\n\n2×2 LinearAlgebra.LowerTriangular{Float64, Matrix{Float64}}:\n 0.929221    ⋅ \n 0.0181684  0.222645\n\n\n\nΣ = varest(m1) * (λ * λ')\n\n2×2 Matrix{Float64}:\n 565.511  11.057\n  11.057  32.6821\n\n\nCompare the diagonal elements to the Variance column of\n\nVarCorr(m1)\n\n\n\n\n\nColumn\nVariance\nStd.Dev\nCorr.\n\n\n\n\nsubj\n(Intercept)\n565.51067\n23.78047\n\n\n\n\ndays\n32.68212\n5.71683\n+0.08\n\n\nResidual\n\n654.94145\n25.59182",
    "crumbs": [
      "Generalized linear mixed models"
    ]
  },
  {
    "objectID": "glmm.html#linear-predictors-in-lmms-and-glmms",
    "href": "glmm.html#linear-predictors-in-lmms-and-glmms",
    "title": "Generalized linear mixed models",
    "section": "2 Linear predictors in LMMs and GLMMs",
    "text": "2 Linear predictors in LMMs and GLMMs\nWriting the model for \\(\\mathcal Y\\) as \\[\n(\\mathcal{Y} | \\mathcal{B}=\\bf{b})\\sim\\mathcal{N}(\\bf{ X\\boldsymbol\\beta + Z b},\\sigma^2\\bf{I})\n\\] may seem like over-mathematization (or “overkill”, if you prefer) relative to expressions like \\[\ny_i = \\beta_1 x_{i,1} + \\beta_2 x_{i,2}+ b_1 z_{i,1} +\\dots+b_{36} z_{i,36}+\\epsilon_i\n\\] but this more abstract form is necessary for generalizations.\nThe way that I read the first form is\n\n\n\n\n\n\nThe conditional distribution of the response vector, \\(\\mathcal Y\\), given that the random effects vector, \\(\\mathcal B =\\bf b\\), is a multivariate normal (or Gaussian) distribution whose mean, \\(\\boldsymbol\\mu\\), is the linear predictor, \\(\\boldsymbol\\eta=\\bf{X\\boldsymbol\\beta+Zb}\\), and whose covariance matrix is \\(\\sigma^2\\bf I\\). That is, conditional on \\(\\bf b\\), the elements of \\(\\mathcal Y\\) are independent normal random variables with constant variance, \\(\\sigma^2\\), and means of the form \\(\\boldsymbol\\mu = \\boldsymbol\\eta = \\bf{X\\boldsymbol\\beta+Zb}\\).\n\n\n\nSo the only things that differ in the distributions of the \\(y_i\\)’s are the means and they are determined by this linear predictor, \\(\\boldsymbol\\eta = \\bf{X\\boldsymbol\\beta+Zb}\\).",
    "crumbs": [
      "Generalized linear mixed models"
    ]
  },
  {
    "objectID": "glmm.html#generalized-linear-mixed-models",
    "href": "glmm.html#generalized-linear-mixed-models",
    "title": "Generalized linear mixed models",
    "section": "3 Generalized Linear Mixed Models",
    "text": "3 Generalized Linear Mixed Models\nConsider first a GLMM for a vector, \\(\\bf y\\), of binary (i.e. yes/no) responses. The probability model for the conditional distribution \\(\\mathcal Y|\\mathcal B=\\bf b\\) consists of independent Bernoulli distributions where the mean, \\(\\mu_i\\), for the i’th response is again determined by the i’th element of a linear predictor, \\(\\boldsymbol\\eta = \\mathbf{X}\\boldsymbol\\beta+\\mathbf{Z b}\\).\nHowever, in this case we will run into trouble if we try to make \\(\\boldsymbol\\mu=\\boldsymbol\\eta\\) because \\(\\mu_i\\) is the probability of “success” for the i’th response and must be between 0 and 1. We can’t guarantee that the i’th component of \\(\\boldsymbol\\eta\\) will be between 0 and 1. To get around this problem we apply a transformation to take \\(\\eta_i\\) to \\(\\mu_i\\). For historical reasons this transformation is called the inverse link, written \\(g^{-1}\\), and the opposite transformation - from the probability scale to an unbounded scale - is called the link, g.\nEach probability distribution in the exponential family (which is most of the important ones), has a canonical link which comes from the form of the distribution itself. The details aren’t as important as recognizing that the distribution itself determines a preferred link function.\nFor the Bernoulli distribution, the canonical link is the logit or log-odds function, \\[\n\\eta = g(\\mu) = \\log\\left(\\frac{\\mu}{1-\\mu}\\right),\n\\] (it’s called log-odds because it is the logarithm of the odds ratio, \\(p/(1-p)\\)) and the canonical inverse link is the logistic \\[\n\\mu=g^{-1}(\\eta)=\\frac{1}{1+\\exp(-\\eta)}.\n\\] This is why fitting a binary response is sometimes called logistic regression.\nFor later use we define a Julia logistic function. See this presentation for more information than you could possibly want to know on how Julia converts code like this to run on the processor.\n\nincrement(x) = x + one(x)\nlogistic(η) = inv(increment(exp(-η)))\n\nlogistic (generic function with 1 method)\n\n\nTo reiterate, the probability model for a Generalized Linear Mixed Model (GLMM) is \\[\n\\begin{aligned}\n(\\mathcal{Y} | \\mathcal{B}=\\bf{b}) &\\sim\\mathcal{D}(\\bf{g^{-1}(X\\boldsymbol\\beta + Z b)},\\phi)\\\\\\\\\n\\mathcal{B}&\\sim\\mathcal{N}(\\bf{0},\\Sigma_{\\boldsymbol\\theta}) .\n\\end{aligned}\n\\] where \\(\\mathcal{D}\\) is the distribution family (such as Bernoulli or Poisson), \\(g^{-1}\\) is the inverse link and \\(\\phi\\) is a scale parameter for \\(\\mathcal{D}\\) if it has one. The important cases of the Bernoulli and Poisson distributions don’t have a scale parameter - once you know the mean you know everything you need to know about the distribution. (For those following the presentation, this poem by John Keats is the one with the couplet “Beauty is truth, truth beauty - that is all ye know on earth and all ye need to know.”)\n\n3.1 An example of a Bernoulli GLMM\nThe contra dataset in the MixedModels package is from a survey on the use of artificial contraception by women in Bangladesh.\n\ncontra = DataFrame(dataset(:contra))\n\n1934×5 DataFrame1909 rows omitted\n\n\n\nRow\ndist\nurban\nlivch\nage\nuse\n\n\n\nString\nString\nString\nFloat64\nString\n\n\n\n\n1\nD01\nY\n3+\n18.44\nN\n\n\n2\nD01\nY\n0\n-5.56\nN\n\n\n3\nD01\nY\n2\n1.44\nN\n\n\n4\nD01\nY\n3+\n8.44\nN\n\n\n5\nD01\nY\n0\n-13.56\nN\n\n\n6\nD01\nY\n0\n-11.56\nN\n\n\n7\nD01\nY\n3+\n18.44\nN\n\n\n8\nD01\nY\n3+\n-3.56\nN\n\n\n9\nD01\nY\n1\n-5.56\nN\n\n\n10\nD01\nY\n3+\n1.44\nN\n\n\n11\nD01\nY\n0\n-11.56\nY\n\n\n12\nD01\nY\n0\n-2.56\nN\n\n\n13\nD01\nY\n1\n-4.56\nN\n\n\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n\n\n1923\nD61\nN\n0\n-11.56\nY\n\n\n1924\nD61\nN\n3+\n1.44\nN\n\n\n1925\nD61\nN\n1\n-5.56\nN\n\n\n1926\nD61\nN\n3+\n14.44\nN\n\n\n1927\nD61\nN\n3+\n19.44\nN\n\n\n1928\nD61\nN\n2\n-9.56\nY\n\n\n1929\nD61\nN\n2\n-2.56\nN\n\n\n1930\nD61\nN\n3+\n14.44\nN\n\n\n1931\nD61\nN\n2\n-4.56\nN\n\n\n1932\nD61\nN\n3+\n14.44\nN\n\n\n1933\nD61\nN\n0\n-13.56\nN\n\n\n1934\nD61\nN\n3+\n10.44\nN\n\n\n\n\n\n\n\ncombine(groupby(contra, :dist), nrow)\n\n60×2 DataFrame35 rows omitted\n\n\n\nRow\ndist\nnrow\n\n\n\nString\nInt64\n\n\n\n\n1\nD01\n117\n\n\n2\nD02\n20\n\n\n3\nD03\n2\n\n\n4\nD04\n30\n\n\n5\nD05\n39\n\n\n6\nD06\n65\n\n\n7\nD07\n18\n\n\n8\nD08\n37\n\n\n9\nD09\n23\n\n\n10\nD10\n13\n\n\n11\nD11\n21\n\n\n12\nD12\n29\n\n\n13\nD13\n24\n\n\n⋮\n⋮\n⋮\n\n\n49\nD49\n4\n\n\n50\nD50\n19\n\n\n51\nD51\n37\n\n\n52\nD52\n61\n\n\n53\nD53\n19\n\n\n54\nD55\n6\n\n\n55\nD56\n45\n\n\n56\nD57\n27\n\n\n57\nD58\n33\n\n\n58\nD59\n10\n\n\n59\nD60\n32\n\n\n60\nD61\n42\n\n\n\n\n\n\nThe information recorded included woman’s age, the number of live children she has, whether she lives in an urban or rural setting, and the political district in which she lives.\nThe age was centered. Unfortunately, the version of the data to which I had access did not record what the centering value was.\nA data plot, Figure 1, shows that the probability of contraception use is not linear in age - it is low for younger women, higher for women in the middle of the range (assumed to be women in late 20’s to early 30’s) and low again for older women (late 30’s to early 40’s in this survey).\nIf we fit a model with only the age term in the fixed effects, that term will not be significant. This doesn’t mean that there is no “age effect”, it only means that there is no significant linear effect for age.\n\n\nCode\ndraw(\n  data(\n    @transform(\n      contra,\n      :numuse = Int(:use == \"Y\"),\n      :urb = ifelse(:urban == \"Y\", \"Urban\", \"Rural\")\n    )\n  ) *\n  mapping(\n    :age =&gt; \"Centered age (yr)\",\n    :numuse =&gt; \"Frequency of contraception use\";\n    col=:urb,\n    color=:livch,\n  ) *\n  smooth();\n  figure=(; resolution=(800, 450)),\n)\n\n\n\n\n\n\n\n\nFigure 1: Smoothed relative frequency of contraception use versus centered age for women in the 1989 Bangladesh Fertility Survey\n\n\n\n\n\n\ncontrasts = Dict(\n  :dist =&gt; Grouping(),\n  :urban =&gt; HelmertCoding(),\n  :livch =&gt; DummyCoding(), # default, but no harm in being explicit\n)\nnAGQ = 9\ndist = Bernoulli()\ngm1 = let\n  form = @formula(\n    use ~ 1 + age + abs2(age) + urban + livch + (1 | dist)\n  )\n  fit(MixedModel, form, contra, dist; nAGQ, contrasts)\nend\n\nMinimizing 205    Time: 0:00:00 ( 1.93 ms/it)\n\n\n\n\n\n\nEst.\nSE\nz\np\nσ_dist\n\n\n\n\n(Intercept)\n-0.6871\n0.1686\n-4.08\n&lt;1e-04\n0.4786\n\n\nage\n0.0035\n0.0092\n0.38\n0.7022\n\n\n\nabs2(age)\n-0.0046\n0.0007\n-6.29\n&lt;1e-09\n\n\n\nurban: Y\n0.3484\n0.0600\n5.81\n&lt;1e-08\n\n\n\nlivch: 1\n0.8151\n0.1622\n5.02\n&lt;1e-06\n\n\n\nlivch: 2\n0.9165\n0.1851\n4.95\n&lt;1e-06\n\n\n\nlivch: 3+\n0.9154\n0.1858\n4.93\n&lt;1e-06\n\n\n\n\n\n\nNotice that the linear term for age is not significant but the quadratic term for age is highly significant. We usually retain the lower order term, even if it is not significant, if the higher order term is significant.\nNotice also that the parameter estimates for the treatment contrasts for livch are similar. Thus the distinction of 1, 2, or 3+ children is not as important as the contrast between having any children and not having any. Those women who already have children are more likely to use artificial contraception.\nFurthermore, the women without children have a different probability vs age profile than the women with children. To allow for this we define a binary children factor and incorporate an age&children interaction.\n\nVarCorr(gm1)\n\n\n\n\n\nColumn\nVariance\nStd.Dev\n\n\n\n\ndist\n(Intercept)\n0.229095\n0.478639\n\n\n\n\n\nNotice that there is no “residual” variance being estimated. This is because the Bernoulli distribution doesn’t have a scale parameter.\n\n\n3.2 Convert livch to a binary factor\n\n@transform!(contra, :children = :livch ≠ \"0\")\n# add the associated contrast specifier\ncontrasts[:children] = EffectsCoding()\n\nEffectsCoding(nothing, nothing)\n\n\n\ngm2 = let\n  form = @formula(\n    use ~\n      1 +\n      age * children +\n      abs2(age) +\n      children +\n      urban +\n      (1 | dist)\n  )\n  fit(MixedModel, form, contra, dist; nAGQ, contrasts)\nend\n\nMinimizing 125    Time: 0:00:00 ( 0.96 ms/it)\n\n\n\n\n\n\nEst.\nSE\nz\np\nσ_dist\n\n\n\n\n(Intercept)\n-0.3614\n0.1275\n-2.83\n0.0046\n0.4757\n\n\nage\n-0.0131\n0.0110\n-1.19\n0.2353\n\n\n\nchildren: true\n0.6054\n0.1035\n5.85\n&lt;1e-08\n\n\n\nabs2(age)\n-0.0058\n0.0008\n-6.89\n&lt;1e-11\n\n\n\nurban: Y\n0.3567\n0.0602\n5.93\n&lt;1e-08\n\n\n\nage & children: true\n0.0342\n0.0127\n2.69\n0.0072\n\n\n\n\n\n\n\n\nCode\nlet\n  mods = [gm2, gm1]\n  DataFrame(;\n    model=[:gm2, :gm1],\n    npar=dof.(mods),\n    deviance=deviance.(mods),\n    AIC=aic.(mods),\n    BIC=bic.(mods),\n    AICc=aicc.(mods),\n  )\nend\n\n\n2×6 DataFrame\n\n\n\nRow\nmodel\nnpar\ndeviance\nAIC\nBIC\nAICc\n\n\n\nSymbol\nInt64\nFloat64\nFloat64\nFloat64\nFloat64\n\n\n\n\n1\ngm2\n7\n2364.92\n2379.18\n2418.15\n2379.24\n\n\n2\ngm1\n8\n2372.46\n2388.73\n2433.27\n2388.81\n\n\n\n\n\n\nBecause these models are not nested, we cannot do a likelihood ratio test. Nevertheless we see that the deviance is much lower in the model with age & children even though the 3 levels of livch have been collapsed into a single level of children. There is a substantial decrease in the deviance even though there are fewer parameters in model gm2 than in gm1. This decrease is because the flexibility of the model - its ability to model the behavior of the response - is being put to better use in gm2 than in gm1.\nAt present the calculation of the geomdof as sum(influence(m)) is not correctly defined in our code for a GLMM so we need to do some more work before we can examine those values.\n\n\n3.3 Using urban&dist as a grouping factor\nIt turns out that there can be more difference between urban and rural settings within the same political district than there is between districts. To model this difference we build a model with urban&dist as a grouping factor.\n\ngm3 = let\n  form = @formula(\n    use ~\n      1 +\n      age * children +\n      abs2(age) +\n      children +\n      urban +\n      (1 | urban & dist)\n  )\n  fit(MixedModel, form, contra, dist; nAGQ, contrasts)\nend\n\nMinimizing 149    Time: 0:00:00 ( 0.96 ms/it)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEst.\nSE\nz\np\nσ_urban & dist\n\n\n\n\n(Intercept)\n-0.3415\n0.1269\n-2.69\n0.0071\n0.5761\n\n\nage\n-0.0129\n0.0112\n-1.16\n0.2472\n\n\n\nchildren: true\n0.6064\n0.1045\n5.80\n&lt;1e-08\n\n\n\nabs2(age)\n-0.0056\n0.0008\n-6.66\n&lt;1e-10\n\n\n\nurban: Y\n0.3936\n0.0859\n4.58\n&lt;1e-05\n\n\n\nage & children: true\n0.0332\n0.0128\n2.59\n0.0096\n\n\n\n\n\n\n\n\nCode\nlet\n  mods = [gm3, gm2, gm1]\n  DataFrame(;\n    model=[:gm3, :gm2, :gm1],\n    npar=dof.(mods),\n    deviance=deviance.(mods),\n    AIC=aic.(mods),\n    BIC=bic.(mods),\n    AICc=aicc.(mods),\n  )\nend\n\n\n3×6 DataFrame\n\n\n\nRow\nmodel\nnpar\ndeviance\nAIC\nBIC\nAICc\n\n\n\nSymbol\nInt64\nFloat64\nFloat64\nFloat64\nFloat64\n\n\n\n\n1\ngm3\n7\n2353.82\n2368.48\n2407.46\n2368.54\n\n\n2\ngm2\n7\n2364.92\n2379.18\n2418.15\n2379.24\n\n\n3\ngm1\n8\n2372.46\n2388.73\n2433.27\n2388.81\n\n\n\n\n\n\nNotice that the parameter count in gm3 is the same as that of gm2 - the thing that has changed is the number of levels of the grouping factor- resulting in a much lower deviance for gm3. This reinforces the idea that a simple count of the number of parameters to be estimated does not always reflect the complexity of the model.\n\ngm2\n\n\n\n\n\nEst.\nSE\nz\np\nσ_dist\n\n\n\n\n(Intercept)\n-0.3614\n0.1275\n-2.83\n0.0046\n0.4757\n\n\nage\n-0.0131\n0.0110\n-1.19\n0.2353\n\n\n\nchildren: true\n0.6054\n0.1035\n5.85\n&lt;1e-08\n\n\n\nabs2(age)\n-0.0058\n0.0008\n-6.89\n&lt;1e-11\n\n\n\nurban: Y\n0.3567\n0.0602\n5.93\n&lt;1e-08\n\n\n\nage & children: true\n0.0342\n0.0127\n2.69\n0.0072\n\n\n\n\n\n\n\ngm3\n\n\n\n\n\n\n\n\n\n\n\n\n\nEst.\nSE\nz\np\nσ_urban & dist\n\n\n\n\n(Intercept)\n-0.3415\n0.1269\n-2.69\n0.0071\n0.5761\n\n\nage\n-0.0129\n0.0112\n-1.16\n0.2472\n\n\n\nchildren: true\n0.6064\n0.1045\n5.80\n&lt;1e-08\n\n\n\nabs2(age)\n-0.0056\n0.0008\n-6.66\n&lt;1e-10\n\n\n\nurban: Y\n0.3936\n0.0859\n4.58\n&lt;1e-05\n\n\n\nage & children: true\n0.0332\n0.0128\n2.59\n0.0096\n\n\n\n\n\n\nThe coefficient for age may be regarded as insignificant but we retain it for two reasons: we have a term of age² (written abs2(age)) in the model and we have a significant interaction age & children in the model.\n\n\n3.4 Predictions for some subgroups\nFor a “typical” district (random effect near zero) the predictions on the linear predictor scale for a woman whose age is near the centering value (i.e. centered age of zero) are:\n\nusing Effects\ndesign = Dict(\n  :children =&gt; [true, false], :urban =&gt; [\"Y\", \"N\"], :age =&gt; [0.0]\n)\npreds = effects(design, gm3; invlink=AutoInvLink())\n\n4×7 DataFrame\n\n\n\nRow\nchildren\nage\nurban\nuse: Y\nerr\nlower\nupper\n\n\n\nBool\nFloat64\nString\nFloat64\nFloat64\nFloat64\nFloat64\n\n\n\n\n1\ntrue\n0.0\nY\n0.658941\n0.0338299\n0.625111\n0.692771\n\n\n2\nfalse\n0.0\nY\n0.364868\n0.0534103\n0.311458\n0.418279\n\n\n3\ntrue\n0.0\nN\n0.46789\n0.0281378\n0.439752\n0.496028\n\n\n4\nfalse\n0.0\nN\n0.207265\n0.0364059\n0.170859\n0.243671",
    "crumbs": [
      "Generalized linear mixed models"
    ]
  },
  {
    "objectID": "glmm.html#summarizing-the-results",
    "href": "glmm.html#summarizing-the-results",
    "title": "Generalized linear mixed models",
    "section": "4 Summarizing the results",
    "text": "4 Summarizing the results\n\nFrom the data plot we can see a quadratic trend in the probability by age.\nThe patterns for women with children are similar and we do not need to distinguish between 1, 2, and 3+ children.\nWe do distinguish between those women who do not have children and those with children. This shows up in a significant age & children interaction term.",
    "crumbs": [
      "Generalized linear mixed models"
    ]
  },
  {
    "objectID": "bootstrap.html",
    "href": "bootstrap.html",
    "title": "Parametric bootstrap for mixed-effects models",
    "section": "",
    "text": "The speed of MixedModels.jl relative to its predecessors makes the parametric bootstrap much more computationally tractable. This is valuable because the parametric bootstrap can be used to produce more accurate confidence intervals than methods based on standard errors or profiling of the likelihood surface.\nThis page is adapted from the MixedModels.jl docs\n\n1 The parametric bootstrap\nBootstrapping is a family of procedures for generating sample values of a statistic, allowing for visualization of the distribution of the statistic or for inference from this sample of values. Bootstrapping also belongs to a larger family of procedures called resampling, which are based on creating new samples of data from an existing one, then computing statistics on the new samples, in order to examine the distribution of the relevant statistics.\nA parametric bootstrap is used with a parametric model, m, that has been fit to data. The procedure is to simulate n response vectors from m using the estimated parameter values and refit m to these responses in turn, accumulating the statistics of interest at each iteration.\nThe parameters of a LinearMixedModel object are the fixed-effects parameters, β, the standard deviation, σ, of the per-observation noise, and the covariance parameter, θ, that defines the variance-covariance matrices of the random effects. A technical description of the covariance parameter can be found in the MixedModels.jl docs. Lisa Schwetlick and Daniel Backhaus have provided a more beginner-friendly description of the covariance parameter in the documentation for MixedModelsSim.jl. For today’s purposes – looking at the uncertainty in the estimates from a fitted model – we can simply use values from the fitted model, but we will revisit the parametric bootstrap as a convenient way to simulate new data, potentially with different parameter values, for power analysis.\nAttach the packages to be used\n\n\nCode\nusing AlgebraOfGraphics\nusing CairoMakie\nusing DataFrames\nusing MixedModels\nusing MixedModelsMakie\nusing Random\nusing SMLP2023: dataset\n\nusing AlgebraOfGraphics: AlgebraOfGraphics as AoG\nCairoMakie.activate!(; type=\"svg\") # use SVG (other options include PNG)\n\nimport ProgressMeter\nProgressMeter.ijulia_behavior(:clear);\n\n\nNote that the precise stream of random numbers generated for a given seed can change between Julia versions. For exact reproducibility, you either need to have the exact same Julia version or use the StableRNGs package.\n\n\n2 A model of moderate complexity\nThe kb07 data (Kronmüller & Barr, 2007) are one of the datasets provided by the MixedModels package.\n\nkb07 = dataset(:kb07)\n\nArrow.Table with 1789 rows, 7 columns, and schema:\n :subj      String\n :item      String\n :spkr      String\n :prec      String\n :load      String\n :rt_trunc  Int16\n :rt_raw    Int16\n\n\nConvert the table to a DataFrame for summary.\n\nkb07 = DataFrame(kb07)\ndescribe(kb07)\n\n7×7 DataFrame\n\n\n\nRow\nvariable\nmean\nmin\nmedian\nmax\nnmissing\neltype\n\n\n\nSymbol\nUnion…\nAny\nUnion…\nAny\nInt64\nDataType\n\n\n\n\n1\nsubj\n\nS030\n\nS103\n0\nString\n\n\n2\nitem\n\nI01\n\nI32\n0\nString\n\n\n3\nspkr\n\nnew\n\nold\n0\nString\n\n\n4\nprec\n\nbreak\n\nmaintain\n0\nString\n\n\n5\nload\n\nno\n\nyes\n0\nString\n\n\n6\nrt_trunc\n2182.2\n579\n1940.0\n5171\n0\nInt16\n\n\n7\nrt_raw\n2226.24\n579\n1940.0\n15923\n0\nInt16\n\n\n\n\n\n\nThe experimental factors; spkr, prec, and load, are two-level factors.\n\ncontrasts = Dict(:spkr =&gt; EffectsCoding(),\n                 :prec =&gt; EffectsCoding(),\n                 :load =&gt; EffectsCoding(),\n                 :subj =&gt; Grouping(),\n                 :item =&gt; Grouping())\n\nThe EffectsCoding contrast is used with these to create a ±1 encoding. Furthermore, Grouping contrasts are assigned to the subj and item factors. This is not a contrast per-se but an indication that these factors will be used as grouping factors for random effects and, therefore, there is no need to create a contrast matrix. For large numbers of levels in a grouping factor, an attempt to create a contrast matrix may cause memory overflow.\nIt is not important in these cases but a good practice in any case.\nWe can look at an initial fit of moderate complexity:\n\nform = @formula(rt_trunc ~ 1 + spkr * prec * load +\n                          (1 + spkr + prec + load | subj) +\n                          (1 + spkr + prec + load | item))\nm0 = fit(MixedModel, form, kb07; contrasts)\n\nMinimizing 874    Time: 0:00:01 ( 1.27 ms/it)\n  objective:  28637.971010073215\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEst.\nSE\nz\np\nσ_subj\nσ_item\n\n\n\n\n(Intercept)\n2181.6424\n77.3515\n28.20\n&lt;1e-99\n301.8721\n362.4695\n\n\nspkr: old\n67.7496\n17.9607\n3.77\n0.0002\n33.0588\n41.1159\n\n\nprec: maintain\n-333.9200\n47.1563\n-7.08\n&lt;1e-11\n58.8512\n247.3299\n\n\nload: yes\n78.8007\n19.7269\n3.99\n&lt;1e-04\n66.9526\n43.3991\n\n\nspkr: old & prec: maintain\n-21.9960\n15.8191\n-1.39\n0.1644\n\n\n\n\nspkr: old & load: yes\n18.3832\n15.8191\n1.16\n0.2452\n\n\n\n\nprec: maintain & load: yes\n4.5327\n15.8191\n0.29\n0.7745\n\n\n\n\nspkr: old & prec: maintain & load: yes\n23.6377\n15.8191\n1.49\n0.1351\n\n\n\n\nResidual\n669.0515\n\n\n\n\n\n\n\n\n\n\nThe default display in Quarto uses the pretty MIME show method for the model and omits the estimated correlations of the random effects.\nThe VarCorr extractor displays these.\n\nVarCorr(m0)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nColumn\nVariance\nStd.Dev\nCorr.\n\n\n\n\n\n\nsubj\n(Intercept)\n91126.7545\n301.8721\n\n\n\n\n\n\nspkr: old\n1092.8862\n33.0588\n+1.00\n\n\n\n\n\nprec: maintain\n3463.4603\n58.8512\n-0.62\n-0.62\n\n\n\n\nload: yes\n4482.6493\n66.9526\n+0.36\n+0.36\n+0.51\n\n\nitem\n(Intercept)\n131384.1084\n362.4695\n\n\n\n\n\n\nspkr: old\n1690.5210\n41.1159\n+0.42\n\n\n\n\n\nprec: maintain\n61172.0781\n247.3299\n-0.69\n+0.37\n\n\n\n\nload: yes\n1883.4785\n43.3991\n+0.29\n+0.14\n-0.13\n\n\nResidual\n\n447629.9270\n669.0515\n\n\n\n\n\n\n\n\nNone of the two-factor or three-factor interaction terms in the fixed-effects are significant. In the random-effects terms only the scalar random effects and the prec random effect for item appear to be warranted, leading to the reduced formula\n\n# formula f4 from https://doi.org/10.33016/nextjournal.100002\nform = @formula(rt_trunc ~ 1 + spkr * prec * load + (1 | subj) + (1 + prec | item))\n\nm1 = fit(MixedModel, form, kb07; contrasts)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEst.\nSE\nz\np\nσ_item\nσ_subj\n\n\n\n\n(Intercept)\n2181.7582\n77.4709\n28.16\n&lt;1e-99\n364.7286\n298.1109\n\n\nspkr: old\n67.8114\n16.0526\n4.22\n&lt;1e-04\n\n\n\n\nprec: maintain\n-333.8582\n47.4629\n-7.03\n&lt;1e-11\n252.6687\n\n\n\nload: yes\n78.6849\n16.0525\n4.90\n&lt;1e-06\n\n\n\n\nspkr: old & prec: maintain\n-21.8802\n16.0525\n-1.36\n0.1729\n\n\n\n\nspkr: old & load: yes\n18.3214\n16.0526\n1.14\n0.2537\n\n\n\n\nprec: maintain & load: yes\n4.4710\n16.0526\n0.28\n0.7806\n\n\n\n\nspkr: old & prec: maintain & load: yes\n23.5219\n16.0525\n1.47\n0.1428\n\n\n\n\nResidual\n678.9318\n\n\n\n\n\n\n\n\n\n\n\nVarCorr(m1)\n\n\n\n\n\nColumn\nVariance\nStd.Dev\nCorr.\n\n\n\n\nitem\n(Intercept)\n133026.918\n364.729\n\n\n\n\nprec: maintain\n63841.496\n252.669\n-0.70\n\n\nsubj\n(Intercept)\n88870.081\n298.111\n\n\n\nResidual\n\n460948.432\n678.932\n\n\n\n\n\n\nThese two models are nested and can be compared with a likelihood-ratio test.\n\nMixedModels.likelihoodratiotest(m0, m1)\n\n\n\n\n\n\n\n\n\n\n\n\n\nmodel-dof\ndeviance\nχ²\nχ²-dof\nP(&gt;χ²)\n\n\n\n\nrt_trunc ~ 1 + spkr + prec + load + spkr & prec + spkr & load + prec & load + spkr & prec & load + (1 | subj) + (1 + prec | item)\n13\n28658\n\n\n\n\n\nrt_trunc ~ 1 + spkr + prec + load + spkr & prec + spkr & load + prec & load + spkr & prec & load + (1 + spkr + prec + load | subj) + (1 + spkr + prec + load | item)\n29\n28638\n21\n16\n0.1979\n\n\n\n\n\nThe p-value of approximately 20% leads us to prefer the simpler model, m1, to the more complex, m0.\n\n\n3 Bootstrap basics\nTo bootstrap the model parameters, first initialize a random number generator then create a bootstrap sample and extract the table of parameter estimates from it.\n\nconst RNG = MersenneTwister(42)\nsamp = parametricbootstrap(RNG, 5_000, m1)\ntbl = samp.tbl\n\nTable with 18 columns and 5000 rows:\n      obj      β1       β2       β3        β4       β5        β6        ⋯\n    ┌────────────────────────────────────────────────────────────────────\n 1  │ 28691.9  2049.88  71.6398  -268.333  75.1566  -17.8459  -1.90968  ⋯\n 2  │ 28642.6  2197.6   73.0832  -313.409  68.018   -18.062   30.2431   ⋯\n 3  │ 28748.1  2175.11  85.4784  -311.786  48.8342  -27.3483  4.97874   ⋯\n 4  │ 28683.7  2158.48  86.0498  -301.013  71.0117  -45.3329  27.3747   ⋯\n 5  │ 28692.6  2342.6   81.9722  -305.764  78.9518  -37.0137  -5.59876  ⋯\n 6  │ 28691.5  2198.64  90.4229  -387.855  93.4767  -37.2244  3.13351   ⋯\n 7  │ 28687.7  2239.49  74.5619  -364.151  93.9154  -43.441   19.1572   ⋯\n 8  │ 28673.4  2217.89  76.4077  -409.228  52.5047  -11.5777  26.4348   ⋯\n 9  │ 28688.6  2184.89  81.3252  -298.693  89.8482  -5.99902  21.3162   ⋯\n 10 │ 28597.6  2227.36  57.3436  -332.635  102.565  -3.85194  7.23222   ⋯\n 11 │ 28700.8  2131.61  58.1253  -363.018  88.0576  -24.1592  2.26268   ⋯\n 12 │ 28644.8  2145.05  94.7216  -242.292  103.158  -27.2788  27.0664   ⋯\n 13 │ 28678.8  2113.56  55.4491  -382.425  89.8252  -12.9669  -2.74384  ⋯\n 14 │ 28773.5  2123.38  117.383  -258.883  78.8998  -63.4576  26.0778   ⋯\n 15 │ 28566.3  2168.89  52.8287  -289.968  71.5994  -14.7045  23.0165   ⋯\n 16 │ 28764.1  2344.66  54.5199  -356.183  54.3082  -36.4798  32.3478   ⋯\n 17 │ 28547.7  2230.3   56.2337  -329.448  88.8918  -27.7721  23.5031   ⋯\n 18 │ 28729.3  2268.71  77.0147  -328.11   73.1191  -15.0292  17.2227   ⋯\n 19 │ 28686.4  2138.38  88.8036  -273.697  68.418   -40.8522  17.7392   ⋯\n 20 │ 28620.7  1955.46  97.2794  -334.827  92.7866  -1.49191  33.0718   ⋯\n 21 │ 28664.1  2115.45  71.7916  -375.888  76.0984  -27.8142  24.5494   ⋯\n 22 │ 28742.2  2173.11  83.8428  -351.655  76.0148  -15.7201  -8.95264  ⋯\n 23 │ 28720.3  2192.85  62.7893  -327.012  47.9618  -23.188   16.7164   ⋯\n ⋮  │    ⋮        ⋮        ⋮        ⋮         ⋮        ⋮         ⋮      ⋱\n\n\nAn empirical density plot of the estimates of the residual standard deviation is obtained as\n\nplt = data(tbl) * mapping(:σ) * AoG.density()\ndraw(plt; axis=(;title=\"Parametric bootstrap estimates of σ\"))\n\n\n\n\n\n\n\n\nA density plot of the estimates of the standard deviation of the random effects is obtained as\n\nplt = data(tbl) * mapping(\n  [:σ1, :σ2, :σ3] .=&gt; \"Bootstrap replicates of standard deviations\";\n  color=dims(1) =&gt; renamer([\"Item intercept\", \"Item speaker\", \"Subj\"])\n) * AoG.density()\ndraw(plt; figure=(;supertitle=\"Parametric bootstrap estimates of variance components\"))\n\n\n\n\n\n\n\n\nThe bootstrap sample can be used to generate intervals that cover a certain percentage of the bootstrapped values. We refer to these as “coverage intervals”, similar to a confidence interval. The shortest such intervals, obtained with the shortestcovint extractor, correspond to a highest posterior density interval in Bayesian inference.\nWe generate these for all random and fixed effects:\n\nconfint(samp)\n\nDictTable with 2 columns and 13 rows:\n par   lower      upper\n ────┬─────────────────────\n β1  │ 2032.48    2340.42\n β2  │ 34.3982    97.8079\n β3  │ -428.09    -244.474\n β4  │ 48.642     110.179\n β5  │ -53.4336   8.42791\n β6  │ -11.3574   51.5299\n β7  │ -27.1799   36.3332\n β8  │ -9.28905   53.5917\n ρ1  │ -0.912778  -0.471128\n σ   │ 654.965    700.928\n σ1  │ 265.276    456.967\n σ2  │ 179.402    321.017\n σ3  │ 232.096    359.744\n\n\n\ndraw(\n  data(samp.β) * mapping(:β; color=:coefname) * AoG.density();\n  figure=(; resolution=(800, 450)),\n)\n\n\n\n\n\n\n\n\nFor the fixed effects, MixedModelsMakie provides a convenience interface to plot the combined coverage intervals and density plots\n\nridgeplot(samp)\n\n\n\n\n\n\n\n\nOften the intercept will be on a different scale and potentially less interesting, so we can stop it from being included in the plot:\n\nridgeplot(samp; show_intercept=false, xlabel=\"Bootstrap density and 95%CI\")\n\n\n\n\n\n\n\n\n\n\n4 Singularity\nLet’s consider the classic dysetuff dataset:\n\ndyestuff = dataset(:dyestuff)\nmdye = fit(MixedModel, @formula(yield ~ 1 + (1 | batch)), dyestuff)\n\n\n\n\n\nEst.\nSE\nz\np\nσ_batch\n\n\n\n\n(Intercept)\n1527.5000\n17.6946\n86.33\n&lt;1e-99\n37.2603\n\n\nResidual\n49.5101\n\n\n\n\n\n\n\n\n\n\nsampdye = parametricbootstrap(MersenneTwister(1234321), 10_000, mdye)\ntbldye = sampdye.tbl\n\nTable with 5 columns and 10000 rows:\n      obj      β1       σ        σ1        θ1\n    ┌────────────────────────────────────────────────\n 1  │ 339.022  1509.13  67.4315  14.312    0.212245\n 2  │ 322.689  1538.08  47.9831  25.5673   0.53284\n 3  │ 324.002  1508.02  50.1346  21.7622   0.434076\n 4  │ 331.887  1538.47  53.2238  41.0559   0.771383\n 5  │ 317.771  1520.62  45.2975  19.1802   0.423428\n 6  │ 315.181  1536.94  36.7556  49.1832   1.33812\n 7  │ 333.641  1519.88  53.8161  46.712    0.867993\n 8  │ 325.729  1528.43  47.8989  37.6367   0.785752\n 9  │ 311.601  1497.46  41.4     15.1257   0.365355\n 10 │ 335.244  1532.65  64.616   0.0       0.0\n 11 │ 327.935  1552.54  57.2036  0.485275  0.00848329\n 12 │ 323.861  1519.28  49.355   24.3703   0.493776\n 13 │ 332.736  1509.04  59.6272  18.2905   0.306747\n 14 │ 328.243  1531.7   51.5431  32.4743   0.630042\n 15 │ 336.186  1536.17  64.0205  15.243    0.238096\n 16 │ 329.468  1526.42  58.6856  0.0       0.0\n 17 │ 320.086  1517.67  43.218   35.9663   0.832207\n 18 │ 325.887  1497.86  50.8753  25.9059   0.509205\n 19 │ 311.31   1529.24  33.8976  49.6557   1.46487\n 20 │ 309.404  1549.71  33.987   41.1105   1.20959\n 21 │ 327.973  1512.27  51.9135  29.8809   0.57559\n 22 │ 310.973  1523.54  40.5191  16.8259   0.415258\n 23 │ 323.794  1545.11  47.1842  32.9669   0.698686\n ⋮  │    ⋮        ⋮        ⋮        ⋮          ⋮\n\n\n\nplt = data(tbldye) * mapping(:σ1) * AoG.density()\ndraw(plt; axis=(;title=\"Parametric bootstrap estimates of σ_batch\"))\n\n\n\n\n\n\n\n\nNotice that this density plot has a spike, or mode, at zero. Although this mode appears to be diffuse, this is an artifact of the way that density plots are created. In fact, it is a pulse, as can be seen from a histogram.\n\nplt = data(tbldye) * mapping(:σ1) * AoG.histogram(;bins=100)\ndraw(plt; axis=(;title=\"Parametric bootstrap estimates of σ_batch\"))\n\n\n\n\n\n\n\n\nA value of zero for the standard deviation of the random effects is an example of a singular covariance. It is easy to detect the singularity in the case of a scalar random-effects term. However, it is not as straightforward to detect singularity in vector-valued random-effects terms.\nFor example, if we bootstrap a model fit to the sleepstudy data\n\nsleepstudy = dataset(:sleepstudy)\nmsleep = fit(MixedModel, @formula(reaction ~ 1 + days + (1 + days | subj)),\n             sleepstudy)\n\n\n\n\n\nEst.\nSE\nz\np\nσ_subj\n\n\n\n\n(Intercept)\n251.4051\n6.6323\n37.91\n&lt;1e-99\n23.7805\n\n\ndays\n10.4673\n1.5022\n6.97\n&lt;1e-11\n5.7168\n\n\nResidual\n25.5918\n\n\n\n\n\n\n\n\n\n\nsampsleep = parametricbootstrap(MersenneTwister(666), 10_000, msleep)\ntblsleep = sampsleep.tbl\n\nTable with 10 columns and 10000 rows:\n      obj      β1       β2       σ        σ1       σ2       ρ1         ⋯\n    ┌───────────────────────────────────────────────────────────────────\n 1  │ 1721.95  252.488  11.0328  22.4544  29.6185  6.33343  0.233383   ⋯\n 2  │ 1760.85  260.763  8.55352  27.3836  20.806   4.32887  0.914702   ⋯\n 3  │ 1750.88  246.709  12.4613  25.9951  15.8702  6.33404  0.200358   ⋯\n 4  │ 1777.33  247.683  12.9824  27.7966  27.5413  4.9878   0.121411   ⋯\n 5  │ 1738.05  245.649  10.5792  25.3596  21.5208  4.26131  0.052677   ⋯\n 6  │ 1751.25  255.669  10.1984  26.1432  22.5389  4.58209  0.225968   ⋯\n 7  │ 1727.51  248.986  7.62095  24.6451  19.0858  4.34881  0.212916   ⋯\n 8  │ 1754.18  246.075  11.0469  26.9407  19.8341  4.55961  -0.202146  ⋯\n 9  │ 1757.47  245.407  13.7475  25.8265  20.0014  7.7647   -0.266385  ⋯\n 10 │ 1752.8   253.911  11.4977  25.7077  20.6409  6.27298  0.171494   ⋯\n 11 │ 1707.8   248.887  10.1608  23.9684  10.5923  4.32041  1.0        ⋯\n 12 │ 1773.69  252.542  10.7379  26.8795  27.7956  6.20553  0.156472   ⋯\n 13 │ 1761.27  254.712  11.0373  25.7998  23.2005  7.30831  0.368175   ⋯\n 14 │ 1737.0   260.299  10.5659  24.6504  29.0113  4.26877  -0.078572  ⋯\n 15 │ 1760.12  258.949  10.1464  27.2088  8.02851  7.01925  0.727257   ⋯\n 16 │ 1723.7   249.204  11.7868  24.9861  18.6887  3.08433  0.633219   ⋯\n 17 │ 1734.14  262.586  8.96611  24.0011  26.7969  5.37598  0.297089   ⋯\n 18 │ 1788.8   260.376  11.658   28.6099  26.1245  5.85587  0.0323618  ⋯\n 19 │ 1752.44  239.962  11.0195  26.2388  23.2242  4.45586  0.482511   ⋯\n 20 │ 1752.92  258.171  11.6339  25.7146  27.3026  4.87036  0.22569    ⋯\n 21 │ 1740.81  254.09   7.91985  25.2195  16.2247  6.08679  0.462549   ⋯\n 22 │ 1756.6   245.791  10.3434  26.2627  23.289   5.50225  -0.143374  ⋯\n 23 │ 1759.01  256.131  9.10794  27.136   27.5008  3.51226  1.0        ⋯\n ⋮  │    ⋮        ⋮        ⋮        ⋮        ⋮        ⋮         ⋮      ⋱\n\n\nthe singularity can be exhibited as a standard deviation of zero or as a correlation of ±1.\n\nconfint(sampsleep)\n\nDictTable with 2 columns and 6 rows:\n par   lower      upper\n ────┬───────────────────\n β1  │ 237.905    264.231\n β2  │ 7.44545    13.3516\n ρ1  │ -0.409926  1.0\n σ   │ 22.6345    28.5125\n σ1  │ 10.6756    33.3567\n σ2  │ 3.07053    7.82206\n\n\nA histogram of the estimated correlations from the bootstrap sample has a spike at +1.\n\nplt = data(tblsleep) * mapping(:ρ1) * AoG.histogram(;bins=100)\ndraw(plt; axis=(;title=\"Parametric bootstrap samples of correlation of random effects\"))\n\n\n\n\n\n\n\n\nor, as a count,\n\ncount(tblsleep.ρ1  .≈ 1)\n\n290\n\n\nClose examination of the histogram shows a few values of -1.\n\ncount(tblsleep.ρ1 .≈ -1)\n\n2\n\n\nFurthermore there are even a few cases where the estimate of the standard deviation of the random effect for the intercept is zero.\n\ncount(tblsleep.σ1 .≈ 0)\n\n7\n\n\nThere is a general condition to check for singularity of an estimated covariance matrix or matrices in a bootstrap sample. The parameter optimized in the estimation is θ, the relative covariance parameter. Some of the elements of this parameter vector must be non-negative and, when one of these components is approximately zero, one of the covariance matrices will be singular.\nThe issingular method for a MixedModel object that tests if a parameter vector θ corresponds to a boundary or singular fit.\nThis operation is encapsulated in a method for the issingular function that works on MixedModelBootstrap objects.\n\ncount(issingular(sampsleep))\n\n299\n\n\n\n\n5 References\n\n\nKronmüller, E., & Barr, D. J. (2007). Perspective-free pragmatics: Broken precedents and the recovery-from-preemption hypothesis. Journal of Memory and Language, 56(3), 436–455. https://doi.org/10.1016/j.jml.2006.05.002\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Bootstrap and profiling",
      "Parametric bootstrap for mixed-effects models"
    ]
  },
  {
    "objectID": "sleepstudy.html",
    "href": "sleepstudy.html",
    "title": "Analysis of the sleepstudy data",
    "section": "",
    "text": "The sleepstudy data are from a study of the effects of sleep deprivation on response time reported in Balkin et al. (2000) and in Belenky et al. (2003). Eighteen subjects were allowed only 3 hours of time to sleep each night for 9 successive nights. Their reaction time was measured each day, starting the day before the first night of sleep deprivation, when the subjects were on their regular sleep schedule.\n\n\n\n\n\n\nNote\n\n\n\nThis description is inaccurate. In fact the first two days were acclimatization, the third was a baseline and sleep deprivation was only enforced after day 2. To allow for comparison with earlier analyses of these data we retain the old data description for this notebook only.\n\n\n\n1 Loading the data\nFirst attach the MixedModels package and other packages for plotting. The CairoMakie package allows the Makie graphics system (Danisch & Krumbiegel, 2021) to generate high quality static images. Activate that package with the SVG (Scalable Vector Graphics) backend.\n\n\nCode\nusing CairoMakie       # graphics back-end\nusing DataFrames\nusing KernelDensity    # density estimation\nusing MixedModels\nusing MixedModelsMakie # diagnostic plots\nusing ProgressMeter\nusing Random           # random number generators\nusing RCall            # call R from Julia\nusing SMLP2023\nusing SMLP2023: dataset\n\nProgressMeter.ijulia_behavior(:clear)\nCairoMakie.activate!(; type=\"svg\")\n\n\nThe sleepstudy data are one of the datasets available with the MixedModels package. It is re-exported by the SMLP2023 package’s dataset function.\n\nsleepstudy = dataset(\"sleepstudy\")\n\nArrow.Table with 180 rows, 3 columns, and schema:\n :subj      String\n :days      Int8\n :reaction  Float64\n\n\nFigure 1 displays the data in a multi-panel plot created with the lattice package in R (Sarkar, 2008), using RCall.jl.\n\n\nCode\nRCall.ijulia_setdevice(MIME(\"image/svg+xml\"); width=10, height=4.5)\nR\"\"\"\nrequire(\"lattice\", quietly=TRUE)\nprint(xyplot(reaction ~ days | subj,\n  $(DataFrame(sleepstudy)),\n  aspect=\"xy\",\n  layout=c(9,2),\n  type=c(\"g\", \"p\", \"r\"),\n  index.cond=function(x,y) coef(lm(y ~ x))[1],\n  xlab = \"Days of sleep deprivation\",\n  ylab = \"Average reaction time (ms)\"\n))\n\"\"\";\n\n\n\n\n\n\n\n\nFigure 1: Average response time versus days of sleep deprivation by subject\n\n\n\n\n\nEach panel shows the data from one subject and a line fit by least squares to that subject’s data. Starting at the lower left panel and proceeding across rows, the panels are ordered by increasing intercept of the least squares line.\nThere are some deviations from linearity within the panels but the deviations are neither substantial nor systematic.\n\n\n2 Fitting an initial model\n\ncontrasts = Dict{Symbol,Any}(:subj =&gt; Grouping())\nm1 = let f = @formula(reaction ~ 1 + days + (1 + days | subj))\n  fit(MixedModel, f, sleepstudy; contrasts)\nend\n\nMinimizing 57    Time: 0:00:00 ( 3.63 ms/it)\n\n\n\n\n\n\nEst.\nSE\nz\np\nσ_subj\n\n\n\n\n(Intercept)\n251.4051\n6.6323\n37.91\n&lt;1e-99\n23.7805\n\n\ndays\n10.4673\n1.5022\n6.97\n&lt;1e-11\n5.7168\n\n\nResidual\n25.5918\n\n\n\n\n\n\n\n\n\nThis model includes fixed effects for the intercept, representing the typical reaction time at the beginning of the experiment with zero days of sleep deprivation, and the slope w.r.t. days of sleep deprivation. The parameter estimates are about 250 ms. typical reaction time without deprivation and a typical increase of 10.5 ms. per day of sleep deprivation.\nThe random effects represent shifts from the typical behavior for each subject. The shift in the intercept has a standard deviation of about 24 ms. which would suggest a range of about 200 ms. to 300 ms. in the intercepts. Similarly within-subject slopes would be expected to have a range of about 0 ms./day up to 20 ms./day.\nThe random effects for the slope and for the intercept are allowed to be correlated within subject. The estimated correlation, 0.08, is small. This estimate is not shown in the default display above but is shown in the output from VarCorr (variance components and correlations).\n\nVarCorr(m1)\n\n\n\n\n\nColumn\nVariance\nStd.Dev\nCorr.\n\n\n\n\nsubj\n(Intercept)\n565.51067\n23.78047\n\n\n\n\ndays\n32.68212\n5.71683\n+0.08\n\n\nResidual\n\n654.94145\n25.59182\n\n\n\n\n\n\nTechnically, the random effects for each subject are unobserved random variables and are not “parameters” in the model per se. Hence we do not report standard errors or confidence intervals for these deviations. However, we can produce prediction intervals on the random effects for each subject. Because the experimental design is balanced, these intervals will have the same width for all subjects.\nA plot of the prediction intervals versus the level of the grouping factor (subj, in this case) is sometimes called a caterpillar plot because it can look like a fuzzy caterpillar if there are many levels of the grouping factor. By default, the levels of the grouping factor are sorted by increasing value of the first random effect.\n\n\nCode\ncaterpillar(m1; vline_at_zero=true)\n\n\n\n\n\n\n\n\nFigure 2: Prediction intervals on random effects for model m1\n\n\n\n\n\nFigure 2 reinforces the conclusion that there is little correlation between the random effect for intercept and the random effect for slope.\n\n\n3 A model with uncorrelated random effects\nThe zerocorr function applied to a random-effects term creates uncorrelated vector-valued per-subject random effects.\n\nm2 = let f = @formula reaction ~ 1 + days + zerocorr(1 + days | subj)\n  fit(MixedModel, f, sleepstudy; contrasts)\nend\n\n\n\n\n\nEst.\nSE\nz\np\nσ_subj\n\n\n\n\n(Intercept)\n251.4051\n6.7077\n37.48\n&lt;1e-99\n24.1714\n\n\ndays\n10.4673\n1.5193\n6.89\n&lt;1e-11\n5.7994\n\n\nResidual\n25.5561\n\n\n\n\n\n\n\n\n\nAgain, the default display doesn’t show that there is no correlation parameter to be estimated in this model, but the VarCorr display does.\n\nVarCorr(m2)\n\n\n\n\n\nColumn\nVariance\nStd.Dev\nCorr.\n\n\n\n\nsubj\n(Intercept)\n584.25897\n24.17145\n\n\n\n\ndays\n33.63281\n5.79938\n.\n\n\nResidual\n\n653.11578\n25.55613\n\n\n\n\n\n\nThis model has a slightly lower log-likelihood than does m1 and one fewer parameter than m1. A likelihood-ratio test can be used to compare these nested models.\n\nMixedModels.likelihoodratiotest(m2, m1)\n\n\n\n\n\n\n\n\n\n\n\n\n\nmodel-dof\ndeviance\nχ²\nχ²-dof\nP(&gt;χ²)\n\n\n\n\nreaction ~ 1 + days + zerocorr(1 + days | subj)\n5\n1752\n\n\n\n\n\nreaction ~ 1 + days + (1 + days | subj)\n6\n1752\n0\n1\n0.8004\n\n\n\n\n\nAlternatively, the AIC or BIC values can be compared.\n\n\nCode\nlet mods = [m2, m1]\n  Table(;\n    model=[:m2, :m1],\n    pars=dof.(mods),\n    geomdof=(sum ∘ leverage).(mods),\n    AIC=aic.(mods),\n    BIC=bic.(mods),\n    AICc=aicc.(mods),\n  )\nend\n\n\nTable with 6 columns and 2 rows:\n     model  pars  geomdof  AIC      BIC      AICc\n   ┌────────────────────────────────────────────────\n 1 │ m2     5     29.045   1762.0   1777.97  1762.35\n 2 │ m1     6     28.6115  1763.94  1783.1   1764.42\n\n\nThe goodness of fit measures: AIC, BIC, and AICc, are all on a “smaller is better” scale and, hence, they all prefer m2.\nThe pars column, which is the same as the model-dof column in the likelihood ratio test output, is simply a count of the number of parameters to be estimated when fitting the model. For example, in m2 there are two fixed-effects parameters and three variance components (including the residual variance).\nAn alternative, more geometrically inspired definition of “degrees of freedom”, is the sum of the leverage values, called geomdof in this table.\nInterestingly, the model with fewer parameters, m2, has a greater sum of the leverage values than the model with more parameters, m1. We’re not sure what to make of that.\nIn both cases the sum of the leverage values is toward the upper end of the range of possible values, which is the rank of the fixed-effects model matrix (2) up to the rank of the fixed-effects plus the random effects model matrix (2 + 36 = 38).\n\n\n\n\n\n\nNote\n\n\n\nI think that the upper bound may be 36, not 38, because the two columns of X lie in the column span of Z\n\n\nThis comparison does show, however, that a simple count of the parameters in a mixed-effects model can underestimate, sometimes drastically, the model complexity. This is because a single variance component or multiple components can add many dimensions to the linear predictor.\n\n\n4 Some diagnostic plots\nIn mixed-effects models the linear predictor expression incorporates fixed-effects parameters, which summarize trends for the population or certain well-defined subpopulations, and random effects which represent deviations associated with the experimental units or observational units - individual subjects, in this case. The random effects are modeled as unobserved random variables.\nThe conditional means of these random variables, sometimes called the BLUPs or Best Linear Unbiased Predictors, are not simply the least squares estimates. They are attenuated or shrunk towards zero to reflect the fact that the individuals are assumed to come from a population. A shrinkage plot, Figure 3, shows the BLUPs from the model fit compared to the values without any shrinkage. If the BLUPs are similar to the unshrunk values then the more complicated model accounting for individual differences is supported. If the BLUPs are strongly shrunk towards zero then the additional complexity in the model to account for individual differences is not providing sufficient increase in fidelity to the data to warrant inclusion.\n\n\nCode\nshrinkageplot!(Figure(; resolution=(500, 500)), m1)\n\n\n\n\n\n\n\n\nFigure 3: Shrinkage plot of means of the random effects in model m1\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThis plot could be drawn as shrinkageplot(m1). The reason for explicitly creating a Figure to be modified by shrinkageplot! is to control the resolution.\n\n\nThis plot shows an intermediate pattern. The random effects are somewhat shrunk toward the origin, a model simplification trend, but not completely shrunk - indicating that fidelity to the data is enhanced with these additional coefficients in the linear predictor.\nIf the shrinkage were primarily in one direction - for example, if the arrows from the unshrunk values to the shrunk values were mostly in the vertical direction - then we would get an indication that we could drop the random effect for slope and revert to a simpler model. This is not the case here.\nAs would be expected, the unshrunk values that are further from the origin tend to be shrunk more toward the origin. That is, the arrows that originate furthest from the origin are longer. However, that is not always the case. The arrow in the upper right corner, from S337, is relatively short. Examination of the panel for S337 in the data plot shows a strong linear trend, even though both the intercept and the slope are unusually large. The neighboring panels in the data plot, S330 and S331, have more variability around the least squares line and are subject to a greater amount of shrinkage in the model. (They correspond to the two arrows on the right hand side of the figure around -5 on the vertical scale.)\n\n\n5 Assessing variability by bootstrapping\nThe speed of fitting linear mixed-effects models using MixedModels.jl allows for using simulation-based approaches to inference instead of relying on approximate standard errors. A parametric bootstrap sample for model m is a collection of models of the same form as m fit to data values simulated from m. That is, we pretend that m and its parameter values are the true parameter values, simulate data from these values, and estimate parameters from the simulated data.\nSimulating and fitting a substantial number of model fits, 5000 in this case, takes only a few seconds, following which we extract a data frame of the parameter estimates and plot densities of some of these estimates.\n\nrng = Random.seed!(42)    # initialize a random number generator\nm1bstp = parametricbootstrap(rng, 5000, m1)\ntbl = m1bstp.tbl\n\nTable with 10 columns and 5000 rows:\n      obj      β1       β2       σ        σ1       σ2       ρ1          ⋯\n    ┌────────────────────────────────────────────────────────────────────\n 1  │ 1717.29  260.712  9.84975  23.4092  15.3317  6.40282  -0.025923   ⋯\n 2  │ 1744.06  262.253  12.3008  25.7047  16.3183  5.54688  0.552607    ⋯\n 3  │ 1714.16  253.149  12.879   22.2753  25.4787  6.1444   0.0691545   ⋯\n 4  │ 1711.54  263.376  11.5798  23.3128  18.8039  4.6557   0.103361    ⋯\n 5  │ 1741.66  248.429  9.39444  25.4355  20.1408  5.27356  -0.163609   ⋯\n 6  │ 1754.81  256.794  8.024    26.5087  10.6784  7.14155  0.335466    ⋯\n 7  │ 1777.73  253.388  8.83556  27.8623  17.8329  7.17384  0.00379206  ⋯\n 8  │ 1768.59  254.441  11.4479  27.4034  16.2483  6.67045  0.725384    ⋯\n 9  │ 1753.56  244.906  11.3423  25.6046  25.3607  5.98654  -0.171821   ⋯\n 10 │ 1722.61  257.088  9.18397  23.3386  24.9274  5.18012  0.181143    ⋯\n 11 │ 1738.16  251.262  11.6568  25.7823  17.6663  4.07213  0.258151    ⋯\n 12 │ 1747.76  258.302  12.8015  26.1085  19.2398  5.06066  0.879712    ⋯\n 13 │ 1745.91  254.57   11.8062  24.8863  24.2513  6.14642  0.0126996   ⋯\n 14 │ 1738.8   251.179  10.3226  24.2672  23.7195  6.32645  0.368592    ⋯\n 15 │ 1724.76  238.603  11.5045  25.23    19.0263  3.64038  -0.34657    ⋯\n 16 │ 1777.7   254.133  8.26398  26.9846  26.3715  7.8283   -0.288773   ⋯\n 17 │ 1748.33  251.571  9.5294   26.2927  21.9611  4.31316  -0.150104   ⋯\n 18 │ 1708.99  245.607  12.8175  24.4135  12.494   5.11304  -0.694452   ⋯\n 19 │ 1732.8   256.87   12.2719  23.9952  20.2665  6.58464  0.147646    ⋯\n 20 │ 1746.12  247.428  10.4695  24.319   32.9351  5.78109  0.0337611   ⋯\n 21 │ 1788.6   254.067  9.11893  27.8716  29.9888  6.74499  -0.0317738  ⋯\n 22 │ 1772.12  245.629  10.4063  28.3482  17.6055  5.1408   -0.0573888  ⋯\n 23 │ 1749.18  245.683  11.673   25.3641  26.4956  5.18958  0.469145    ⋯\n ⋮  │    ⋮        ⋮        ⋮        ⋮        ⋮        ⋮         ⋮       ⋱\n\n\nAn empirical density plot of the estimates for the fixed-effects coefficients, Figure 4, shows the normal distribution, “bell-curve”, shape as we might expect.\n\n\nCode\nbegin\n  f1 = Figure(; resolution=(1000, 400))\n  CairoMakie.density!(\n    Axis(f1[1, 1]; xlabel=\"Intercept [ms]\"), tbl.β1\n  )\n  CairoMakie.density!(\n    Axis(f1[1, 2]; xlabel=\"Coefficient of days [ms/day]\"),\n    tbl.β2\n  )\n  f1\nend\n\n\n\n\n\n\n\n\nFigure 4: Empirical density plots of bootstrap replications of fixed-effects parameter estimates\n\n\n\n\n\nIt is also possible to create interval estimates of the parameters from the bootstrap replicates. We define the 1-α shortestcovint to be the shortest interval that contains a proportion 1-α (defaults to 95%) of the bootstrap estimates of the parameter.\n\nTable(shortestcovint(m1bstp))\n\nTable with 5 columns and 6 rows:\n     type  group     names              lower      upper\n   ┌──────────────────────────────────────────────────────\n 1 │ β     missing   (Intercept)        239.64     265.228\n 2 │ β     missing   days               7.42347    13.1607\n 3 │ σ     subj      (Intercept)        10.1722    33.0876\n 4 │ σ     subj      days               2.99481    7.66134\n 5 │ ρ     subj      (Intercept), days  -0.401353  1.0\n 6 │ σ     residual  missing            22.701     28.5016\n\n\nThe intervals look reasonable except that the upper end point of the interval for ρ1, the correlation coefficient, is 1.0 . It turns out that the estimates of ρ have a great deal of variability.\nBecause there are several values on the boundary (ρ = 1.0) and a pulse like this is not handled well by a density plot, we plot this sample as a histogram, Figure 5.\n\n\nCode\nhist(\n  tbl.ρ1;\n  bins=40,\n  axis=(; xlabel=\"Estimated correlation of the random effects\"),\n  figure=(; resolution=(500, 500)),\n)\n\n\n\n\n\n\n\n\nFigure 5: Histogram of bootstrap replications of the within-subject correlation parameter\n\n\n\n\n\nFinally, density plots for the variance components (but on the scale of the standard deviation), Figure 6, show reasonable symmetry.\n\n\nCode\nbegin\n  f2 = Figure(; resolution=(1000, 300))\n  CairoMakie.density!(\n    Axis(f2[1, 1]; xlabel=\"Residual σ\"),\n    tbl.σ,\n  )\n  CairoMakie.density!(\n    Axis(f2[1, 2]; xlabel=\"subj-Intercept σ\"),\n    tbl.σ1,\n  )\n  CairoMakie.density!(\n    Axis(f2[1, 3]; xlabel=\"subj-slope σ\"),\n    tbl.σ2,\n  )\n  f2\nend\n\n\n\n\n\n\n\n\nFigure 6: Empirical density plots of bootstrap replicates of standard deviation estimates\n\n\n\n\n\nThe estimates of the coefficients, β₁ and β₂, are not highly correlated as shown in a scatterplot of the bootstrap estimates, Figure 7 .\n\nvcov(m1; corr=true)  # correlation estimate from the model\n\n2×2 Matrix{Float64}:\n  1.0       -0.137545\n -0.137545   1.0\n\n\n\n\nCode\nlet\n  scatter(\n    tbl.β1, tbl.β2,\n    color=(:blue, 0.20),\n    axis=(; xlabel=\"Intercept\", ylabel=\"Coefficient of days\"),\n    figure=(; resolution=(500, 500)),\n  )\n  contour!(kde((tbl.β1, tbl.β2)))\n  current_figure()\nend\n\n\n\n\n\n\n\n\nFigure 7: Scatter-plot of bootstrap replicates of fixed-effects estimates with contours\n\n\n\n\n\n\n\n6 References\n\n\nBalkin, T., Thome, D., Sing, H., Thomas, M., Redmond, D., Wesensten, N., Williams, J., Hall, S., & Belenky, G. (2000). Effects of sleep schedules on commercial motor vehicle driver performance (DOT-MC-00-133). Federal Motor Carrier Safety Administration. https://doi.org/10.21949/1503015.\n\n\nBelenky, G., Wesensten, N. J., Thorne, D. R., Thomas, M. L., Sing, H. C., Redmond, D. P., Russo, M. B., & Balkin, T. J. (2003). Patterns of performance degradation and restoration during sleep restriction and subsequent recovery: A sleep dose-response study. Journal of Sleep Research, 12(1), 1–12. https://doi.org/10.1046/j.1365-2869.2003.00337.x\n\n\nDanisch, S., & Krumbiegel, J. (2021). Makie.jl: Flexible high-performance data visualization for julia. Journal of Open Source Software, 6(65), 3349. https://doi.org/10.21105/joss.03349\n\n\nSarkar, D. (2008). Lattice: Mutivariate data visualization with r. Springer-Verlag GmbH. https://www.ebook.de/de/product/11429038/deepayan_sarkar_lattice.html\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Worked examples",
      "Analysis of the sleepstudy data"
    ]
  },
  {
    "objectID": "profiling.html",
    "href": "profiling.html",
    "title": "Confidence intervals from profiled objective",
    "section": "",
    "text": "Statistical methods that are based on probability models can be used to provide us with a “best guess” of the value of parameters, such as the effect of a particular experimental treatment, in the form of a parameter estimate. In addition, the probability model can be used to assess the uncertainty in the estimate.\nOften the information about the uncertainty is reduced to a single number, a p-value for a test of a null hypothesis, such as the effect being zero, versus the alternative of a non-zero effect. But quoting a single number from a model fit to experimental data, which may have required considerable effort and expense to obtain, will often mean discarding a considerable amount of the information in the data. In the days when computing was expensive and labor-intensive this may have been unavoidable. However, modern computing hardware and software systems provide us with the opportunity of much more intensive evaluation of the uncertainty. At a minimum, instead of focussing solely on the question of whether a coefficient could reasonably be zero, we can formulate confidence intervals on individual parameter estimates or confidence regions on groups of parameters.\nWe have seen the used of a parametric bootstrap to create a sample from the distribution of the estimators of the parameters, and how such samples can be used to create coverage intervals. The bootstrap is based on simulating response vectors from the model that has been fit to the observed data and refitting the same model to these simulated responses.\nIn this section we explore another approach based on refitting the model, keeping the same responses but holding one of the parameters fixed at a specified value.\n\n\nLoad the packages to be used\n\n\nCode\nusing AlgebraOfGraphics\nusing CairoMakie\nusing MixedModels\nusing MixedModelsMakie\nusing Random\nusing SMLP2023: dataset\n\nCairoMakie.activate!(; type=\"svg\")\nimport ProgressMeter\nProgressMeter.ijulia_behavior(:clear)\n\n\nLoad the data and define the contrasts so that the coefficients for each of the experimental variables, load, spkr and prec, are positive.\n\ncontrasts = Dict( # base levels so estimates for speed are positive\n  :load =&gt; EffectsCoding(; base=\"yes\"),\n  :prec =&gt; EffectsCoding(; base=\"break\"),\n  :spkr =&gt; EffectsCoding(; base=\"old\"),\n)\nkb07 = Table(dataset(:kb07))\n\nTable with 7 columns and 1789 rows:\n      subj  item  spkr  prec      load  rt_trunc  rt_raw\n    ┌───────────────────────────────────────────────────\n 1  │ S030  I01   new   break     yes   2267      2267\n 2  │ S030  I02   old   maintain  no    3856      3856\n 3  │ S030  I03   old   break     no    1567      1567\n 4  │ S030  I04   new   maintain  no    1732      1732\n 5  │ S030  I05   new   break     no    2660      2660\n 6  │ S030  I06   old   maintain  yes   2763      2763\n 7  │ S030  I07   old   break     yes   3528      3528\n 8  │ S030  I08   new   maintain  yes   1741      1741\n 9  │ S030  I09   new   break     yes   3692      3692\n 10 │ S030  I10   old   maintain  no    1949      1949\n 11 │ S030  I11   old   break     no    2189      2189\n 12 │ S030  I12   new   maintain  no    2207      2207\n 13 │ S030  I13   new   break     no    2078      2078\n 14 │ S030  I14   old   maintain  yes   1901      1901\n 15 │ S030  I15   old   break     yes   4015      4015\n 16 │ S030  I16   new   maintain  yes   1880      1880\n 17 │ S030  I17   new   break     yes   1444      1444\n 18 │ S030  I18   old   maintain  no    1683      1683\n 19 │ S030  I19   old   break     no    2037      2037\n 20 │ S030  I20   new   maintain  no    1168      1168\n 21 │ S030  I21   new   break     no    1930      1930\n 22 │ S030  I22   old   maintain  yes   1843      1843\n 23 │ S030  I23   old   break     yes   4969      4969\n ⋮  │  ⋮     ⋮     ⋮       ⋮       ⋮       ⋮        ⋮\n\n\nNow we fit and profile a model. The response is defined as 1000 / rt_raw where rt_raw is measured in milliseconds. Thus the response being modeled is the speed measured in responses per second.\n\npr01 = let f = @formula 1000 / rt_raw ~\n    1 + load + spkr + prec + (1 + prec | item) + (1 | subj)\n  profile(fit(MixedModel, f, kb07; contrasts))\nend\nprintln(pr01.m) # model is a property of the profile object\n\nMinimizing 67    Time: 0:00:00 (10.77 ms/it)\n\n\nLinear mixed model fit by maximum likelihood\n :(1000 / rt_raw) ~ 1 + load + spkr + prec + (1 + prec | item) + (1 | subj)\n   logLik   -2 logLik     AIC       AICc        BIC    \n   846.2869 -1692.5738 -1674.5738 -1674.4726 -1625.1691\n\nVariance components:\n             Column      Variance  Std.Dev.   Corr.\nitem     (Intercept)     0.0061053 0.0781364\n         prec: maintain  0.0020476 0.0452502 -0.21\nsubj     (Intercept)     0.0054186 0.0736113\nResidual                 0.0194484 0.1394577\n Number of obs: 1789; levels of grouping factors: 32, 56\n\n  Fixed-effects parameters:\n──────────────────────────────────────────────────────\n                    Coef.  Std. Error      z  Pr(&gt;|z|)\n──────────────────────────────────────────────────────\n(Intercept)     0.531523   0.0172749   30.77    &lt;1e-99\nload: no        0.0212959  0.00329731   6.46    &lt;1e-09\nspkr: new       0.011218   0.00329732   3.40    0.0007\nprec: maintain  0.0698293  0.00865212   8.07    &lt;1e-15\n──────────────────────────────────────────────────────\n\n\nEvaluation of pr01 is similar to other model fits in these notes except that the call to fit is wrapped in a call to profile. Because the object returned from profile includes the original model fit as its m property, it is not necessary to save the original model fit separately.\n\n\n\nThe information from the profile is encapsulated in a table.\n\npr01.tbl\n\nTable with 15 columns and 249 rows:\n      p   ζ          β1        β2         β3         β4         σ         ⋯\n    ┌──────────────────────────────────────────────────────────────────────\n 1  │ σ   -4.11624   0.531525  0.021298   0.0112154  0.0698319  0.130088  ⋯\n 2  │ σ   -3.59106   0.531525  0.0212977  0.0112157  0.0698316  0.131224  ⋯\n 3  │ σ   -3.06898   0.531524  0.0212975  0.011216   0.0698313  0.13237   ⋯\n 4  │ σ   -2.54996   0.531524  0.0212972  0.0112164  0.069831   0.133526  ⋯\n 5  │ σ   -2.03399   0.531524  0.021297   0.0112167  0.0698307  0.134692  ⋯\n 6  │ σ   -1.52104   0.531524  0.0212967  0.011217   0.0698303  0.135868  ⋯\n 7  │ σ   -1.01107   0.531523  0.0212964  0.0112173  0.06983    0.137054  ⋯\n 8  │ σ   -0.504067  0.531523  0.0212961  0.0112177  0.0698297  0.138251  ⋯\n 9  │ σ   0.0        0.531523  0.0212959  0.011218   0.0698293  0.139458  ⋯\n 10 │ σ   0.501151   0.531523  0.0212956  0.0112184  0.069829   0.140675  ⋯\n 11 │ σ   0.999416   0.531522  0.0212953  0.0112187  0.0698286  0.141904  ⋯\n 12 │ σ   1.49482    0.531522  0.021295   0.0112191  0.0698282  0.143143  ⋯\n 13 │ σ   1.98739    0.531522  0.0212947  0.0112195  0.0698279  0.144392  ⋯\n 14 │ σ   2.47714    0.531521  0.0212944  0.0112199  0.0698275  0.145653  ⋯\n 15 │ σ   2.9641     0.531521  0.021294   0.0112202  0.0698271  0.146925  ⋯\n 16 │ σ   3.4483     0.531521  0.0212937  0.0112206  0.0698267  0.148208  ⋯\n 17 │ σ   3.92976    0.53152   0.0212934  0.011221   0.0698263  0.149502  ⋯\n 18 │ σ   4.4085     0.53152   0.0212931  0.0112214  0.0698259  0.150807  ⋯\n 19 │ β1  -4.15491   0.453786  0.0212914  0.0112175  0.0763347  0.139442  ⋯\n 20 │ β1  -3.75285   0.462423  0.0212917  0.0112177  0.0755058  0.139444  ⋯\n 21 │ β1  -3.33142   0.471061  0.0212921  0.0112179  0.0747195  0.139446  ⋯\n 22 │ β1  -2.89208   0.479698  0.0212926  0.0112181  0.073965   0.139448  ⋯\n 23 │ β1  -2.43663   0.488335  0.0212931  0.0112182  0.0732385  0.13945   ⋯\n ⋮  │ ⋮       ⋮         ⋮          ⋮          ⋮          ⋮         ⋮      ⋱\n\n\nEach row of the table summarizes a fit of the original model to the original data but with one of the parameters held fixed. For the first 18 rows of the table, the parameter being held fixed is \\(\\sigma\\), as shown in the p column. In the next set of rows the parameter being held fixed will be \\(\\beta_1\\), the intercept.\nThere are blocks of rows for the fixed-effects (\\(\\boldsymbol{\\beta}\\)) parameters, the variance components (on the scale of a standard deviation), and the \\(\\boldsymbol{\\theta}\\) parameters that generate the covariance factor \\(\\boldsymbol{\\Lambda}_{\\boldsymbol{\\theta}}\\). (At present the correlation parameters are not profiled - we may add them later but that computation is rather awkward.)\n\nshow(unique(pr01.tbl.p))\n\n[:σ, :β1, :β2, :β3, :β4, :θ1, :θ2, :θ3, :θ4, :σ1, :σ2, :σ3]\n\n\nTo reiterate, the first row contains the parameter estimates for this model fit to the original response values with the constraint that \\(\\sigma=0.130088\\), instead of the global estimate \\(\\hat{\\sigma}=0.139458\\) in the row for which \\(\\zeta=0.0\\).\nThe global estimates are included in every block at the row for which \\(\\zeta=0.0\\).\n\nfilter(r -&gt; iszero(r.ζ), pr01.tbl)\n\nTable with 15 columns and 12 rows:\n      p   ζ    β1        β2         β3        β4         σ         σ1         ⋯\n    ┌──────────────────────────────────────────────────────────────────────────\n 1  │ σ   0.0  0.531523  0.0212959  0.011218  0.0698293  0.139458  0.0781364  ⋯\n 2  │ β1  0.0  0.531523  0.0212959  0.011218  0.0698293  0.139458  0.0781364  ⋯\n 3  │ β2  0.0  0.531523  0.0212959  0.011218  0.0698293  0.139458  0.0781364  ⋯\n 4  │ β3  0.0  0.531523  0.0212959  0.011218  0.0698293  0.139458  0.0781364  ⋯\n 5  │ β4  0.0  0.531523  0.0212959  0.011218  0.0698293  0.139458  0.0781364  ⋯\n 6  │ θ1  0.0  0.531523  0.0212959  0.011218  0.0698293  0.139458  0.0781364  ⋯\n 7  │ θ2  0.0  0.531523  0.0212959  0.011218  0.0698293  0.139458  0.0781364  ⋯\n 8  │ θ3  0.0  0.531523  0.0212959  0.011218  0.0698293  0.139458  0.0781364  ⋯\n 9  │ θ4  0.0  0.531523  0.0212959  0.011218  0.0698293  0.139458  0.0781364  ⋯\n 10 │ σ1  0.0  0.531523  0.0212959  0.011218  0.0698293  0.139458  0.0781364  ⋯\n 11 │ σ2  0.0  0.531523  0.0212959  0.011218  0.0698293  0.139458  0.0781364  ⋯\n 12 │ σ3  0.0  0.531523  0.0212959  0.011218  0.0698293  0.139458  0.0781364  ⋯\n\n\nThe \\(\\zeta\\) column in this table is a measure of the quality of the fit from the parameters in each row, relative to the global parameter estimates, as measured by the change in the objective (negative twice the log-likelihood).\nThe minimum value for the objective is that at the global parameter estimates. The change in the objective when we constrain one parameter to a particular value has approximately a \\(\\chi^2\\) distribution on 1 degree of freedom, which is the square of a standard normal distribution, \\(\\mathcal{Z}^2\\). We can convert this change in the quality of the fit to the scale of the standard normal distribution by taking the signed square root, which is the square root of the change in the objective with the sign of \\(\\psi-\\hat{\\psi}\\) where \\(\\psi\\) represents the parameter being profiled. This is the value labelled \\(\\zeta\\) in the table.\nTo review:\n\nEach row in the table is the result of re-fitting the original model with the parameter in the p column held fixed at a particular value, as shown in the column for that parameter.\nThe \\(\\zeta\\) column is the signed square root of the change in the objective from the global parameter estimates.\nThus in the block of rows where \\(\\sigma\\) is held fixed, the \\(\\zeta\\) values in rows for which \\(\\sigma&lt;\\hat\\sigma\\) are negative and those for which \\(\\sigma &gt; \\hat\\sigma\\) have positive values of \\(\\zeta\\).\nRows in which \\(\\zeta=0.0\\) are the global parameter estimates.\n\n\n\n\nFigure 1 shows, for each of the fixed effects parameters, \\(\\zeta\\) versus the parameter value.\n\n\nCode\nzetaplot!(Figure(; resolution=(1200, 350)), pr01; ptyp='β')\n\n\n\n\n\n\n\n\nFigure 1: ζ versus the value of the coefficient for the fixed-effects parameters in a model of response speed for the kb07 data.\n\n\n\n\n\nThe lines on these panels are read like normal probability plots, i.e. QQ plots against a standard normal distribution. Those on the \\(\\beta_2\\) and \\(\\beta_3\\) panels are, to the resolution of the plot, straight lines which indicates that the estimators of those parameters are normally distributed over the region of interest.\nThe points in the \\(\\beta_1\\) and \\(\\beta_4\\) panels are slightly over-dispersed relative to the straight line, which means that the estimators of these parameters are distributed like a T-distribution with a moderate number of degrees of freedom.\nThe profile-\\(\\zeta\\) function can be used to generate confidence intervals on the parameters\n\nconfint(pr01)\n\nDictTable with 3 columns and 8 rows:\n par   estimate   lower       upper\n ────┬─────────────────────────────────\n β1  │ 0.531523   0.497103    0.565942\n β2  │ 0.0212959  0.0148295   0.0277621\n β3  │ 0.011218   0.00475174  0.0176844\n β4  │ 0.0698293  0.0523046   0.0873562\n σ   │ 0.139458   0.13486     0.144322\n σ1  │ 0.0781364  0.0612443   0.103257\n σ2  │ 0.0452502  0.0338521   0.0618819\n σ3  │ 0.0736113  0.0600844   0.0916852\n\n\nas shown in Figure 2, which shows the absolute value of \\(\\zeta\\), which is simply the square root of the difference in the objective, versus the parameter being profiled.\n\n\nCode\nzetaplot!(Figure(; resolution=(1200, 330)), pr01; ptyp='β', absv=true)\n\n\n\n\n\n\n\n\nFigure 2: Absolute value of ζ versus value of the coefficient for the fixed-effects parameters in a model of response speed for the kb07 data. The horizontal lines are confidence intervals with nominal 50%, 80%, 90%, 95% and 99% confidence.\n\n\n\n\n\nThe 95% confidence intervals are the second horizontal lines from the top in each panel, at 1.96 on the vertical scale.\n\n\nCode\nzetaplot!(Figure(; resolution=(1200, 330)), pr01; ptyp='σ', absv=true)\n\n\n\n\n\n\n\n\nFigure 3: Absolute value of ζ versus value of the coefficient for the variance component parameters in a model of response speed for the kb07 data. The horizontal lines are confidence intervals with nominal 50%, 80%, 90%, 95% and 99% confidence.\n\n\n\n\n\nFigure 3 shows similar confidence intervals on the parameters representing standard deviations as does Figure 4 for the \\(\\theta\\) parameters.\n\n\nCode\nzetaplot!(Figure(; resolution=(1200, 330)), pr01; ptyp='θ', absv=true)\n\n\n\n\n\n\n\n\nFigure 4: Absolute value of ζ versus parameter value for the θ parameters in a model of response speed for the kb07 data. The horizontal lines are confidence intervals with nominal 50%, 80%, 90%, 95% and 99% confidence.",
    "crumbs": [
      "Bootstrap and profiling",
      "Confidence intervals from profiled objective"
    ]
  },
  {
    "objectID": "profiling.html#profiling-a-model-for-the-kb07-data",
    "href": "profiling.html#profiling-a-model-for-the-kb07-data",
    "title": "Confidence intervals from profiled objective",
    "section": "",
    "text": "Load the packages to be used\n\n\nCode\nusing AlgebraOfGraphics\nusing CairoMakie\nusing MixedModels\nusing MixedModelsMakie\nusing Random\nusing SMLP2023: dataset\n\nCairoMakie.activate!(; type=\"svg\")\nimport ProgressMeter\nProgressMeter.ijulia_behavior(:clear)\n\n\nLoad the data and define the contrasts so that the coefficients for each of the experimental variables, load, spkr and prec, are positive.\n\ncontrasts = Dict( # base levels so estimates for speed are positive\n  :load =&gt; EffectsCoding(; base=\"yes\"),\n  :prec =&gt; EffectsCoding(; base=\"break\"),\n  :spkr =&gt; EffectsCoding(; base=\"old\"),\n)\nkb07 = Table(dataset(:kb07))\n\nTable with 7 columns and 1789 rows:\n      subj  item  spkr  prec      load  rt_trunc  rt_raw\n    ┌───────────────────────────────────────────────────\n 1  │ S030  I01   new   break     yes   2267      2267\n 2  │ S030  I02   old   maintain  no    3856      3856\n 3  │ S030  I03   old   break     no    1567      1567\n 4  │ S030  I04   new   maintain  no    1732      1732\n 5  │ S030  I05   new   break     no    2660      2660\n 6  │ S030  I06   old   maintain  yes   2763      2763\n 7  │ S030  I07   old   break     yes   3528      3528\n 8  │ S030  I08   new   maintain  yes   1741      1741\n 9  │ S030  I09   new   break     yes   3692      3692\n 10 │ S030  I10   old   maintain  no    1949      1949\n 11 │ S030  I11   old   break     no    2189      2189\n 12 │ S030  I12   new   maintain  no    2207      2207\n 13 │ S030  I13   new   break     no    2078      2078\n 14 │ S030  I14   old   maintain  yes   1901      1901\n 15 │ S030  I15   old   break     yes   4015      4015\n 16 │ S030  I16   new   maintain  yes   1880      1880\n 17 │ S030  I17   new   break     yes   1444      1444\n 18 │ S030  I18   old   maintain  no    1683      1683\n 19 │ S030  I19   old   break     no    2037      2037\n 20 │ S030  I20   new   maintain  no    1168      1168\n 21 │ S030  I21   new   break     no    1930      1930\n 22 │ S030  I22   old   maintain  yes   1843      1843\n 23 │ S030  I23   old   break     yes   4969      4969\n ⋮  │  ⋮     ⋮     ⋮       ⋮       ⋮       ⋮        ⋮\n\n\nNow we fit and profile a model. The response is defined as 1000 / rt_raw where rt_raw is measured in milliseconds. Thus the response being modeled is the speed measured in responses per second.\n\npr01 = let f = @formula 1000 / rt_raw ~\n    1 + load + spkr + prec + (1 + prec | item) + (1 | subj)\n  profile(fit(MixedModel, f, kb07; contrasts))\nend\nprintln(pr01.m) # model is a property of the profile object\n\nMinimizing 67    Time: 0:00:00 (10.77 ms/it)\n\n\nLinear mixed model fit by maximum likelihood\n :(1000 / rt_raw) ~ 1 + load + spkr + prec + (1 + prec | item) + (1 | subj)\n   logLik   -2 logLik     AIC       AICc        BIC    \n   846.2869 -1692.5738 -1674.5738 -1674.4726 -1625.1691\n\nVariance components:\n             Column      Variance  Std.Dev.   Corr.\nitem     (Intercept)     0.0061053 0.0781364\n         prec: maintain  0.0020476 0.0452502 -0.21\nsubj     (Intercept)     0.0054186 0.0736113\nResidual                 0.0194484 0.1394577\n Number of obs: 1789; levels of grouping factors: 32, 56\n\n  Fixed-effects parameters:\n──────────────────────────────────────────────────────\n                    Coef.  Std. Error      z  Pr(&gt;|z|)\n──────────────────────────────────────────────────────\n(Intercept)     0.531523   0.0172749   30.77    &lt;1e-99\nload: no        0.0212959  0.00329731   6.46    &lt;1e-09\nspkr: new       0.011218   0.00329732   3.40    0.0007\nprec: maintain  0.0698293  0.00865212   8.07    &lt;1e-15\n──────────────────────────────────────────────────────\n\n\nEvaluation of pr01 is similar to other model fits in these notes except that the call to fit is wrapped in a call to profile. Because the object returned from profile includes the original model fit as its m property, it is not necessary to save the original model fit separately.",
    "crumbs": [
      "Bootstrap and profiling",
      "Confidence intervals from profiled objective"
    ]
  },
  {
    "objectID": "profiling.html#fixing-values-of-parameters",
    "href": "profiling.html#fixing-values-of-parameters",
    "title": "Confidence intervals from profiled objective",
    "section": "",
    "text": "The information from the profile is encapsulated in a table.\n\npr01.tbl\n\nTable with 15 columns and 249 rows:\n      p   ζ          β1        β2         β3         β4         σ         ⋯\n    ┌──────────────────────────────────────────────────────────────────────\n 1  │ σ   -4.11624   0.531525  0.021298   0.0112154  0.0698319  0.130088  ⋯\n 2  │ σ   -3.59106   0.531525  0.0212977  0.0112157  0.0698316  0.131224  ⋯\n 3  │ σ   -3.06898   0.531524  0.0212975  0.011216   0.0698313  0.13237   ⋯\n 4  │ σ   -2.54996   0.531524  0.0212972  0.0112164  0.069831   0.133526  ⋯\n 5  │ σ   -2.03399   0.531524  0.021297   0.0112167  0.0698307  0.134692  ⋯\n 6  │ σ   -1.52104   0.531524  0.0212967  0.011217   0.0698303  0.135868  ⋯\n 7  │ σ   -1.01107   0.531523  0.0212964  0.0112173  0.06983    0.137054  ⋯\n 8  │ σ   -0.504067  0.531523  0.0212961  0.0112177  0.0698297  0.138251  ⋯\n 9  │ σ   0.0        0.531523  0.0212959  0.011218   0.0698293  0.139458  ⋯\n 10 │ σ   0.501151   0.531523  0.0212956  0.0112184  0.069829   0.140675  ⋯\n 11 │ σ   0.999416   0.531522  0.0212953  0.0112187  0.0698286  0.141904  ⋯\n 12 │ σ   1.49482    0.531522  0.021295   0.0112191  0.0698282  0.143143  ⋯\n 13 │ σ   1.98739    0.531522  0.0212947  0.0112195  0.0698279  0.144392  ⋯\n 14 │ σ   2.47714    0.531521  0.0212944  0.0112199  0.0698275  0.145653  ⋯\n 15 │ σ   2.9641     0.531521  0.021294   0.0112202  0.0698271  0.146925  ⋯\n 16 │ σ   3.4483     0.531521  0.0212937  0.0112206  0.0698267  0.148208  ⋯\n 17 │ σ   3.92976    0.53152   0.0212934  0.011221   0.0698263  0.149502  ⋯\n 18 │ σ   4.4085     0.53152   0.0212931  0.0112214  0.0698259  0.150807  ⋯\n 19 │ β1  -4.15491   0.453786  0.0212914  0.0112175  0.0763347  0.139442  ⋯\n 20 │ β1  -3.75285   0.462423  0.0212917  0.0112177  0.0755058  0.139444  ⋯\n 21 │ β1  -3.33142   0.471061  0.0212921  0.0112179  0.0747195  0.139446  ⋯\n 22 │ β1  -2.89208   0.479698  0.0212926  0.0112181  0.073965   0.139448  ⋯\n 23 │ β1  -2.43663   0.488335  0.0212931  0.0112182  0.0732385  0.13945   ⋯\n ⋮  │ ⋮       ⋮         ⋮          ⋮          ⋮          ⋮         ⋮      ⋱\n\n\nEach row of the table summarizes a fit of the original model to the original data but with one of the parameters held fixed. For the first 18 rows of the table, the parameter being held fixed is \\(\\sigma\\), as shown in the p column. In the next set of rows the parameter being held fixed will be \\(\\beta_1\\), the intercept.\nThere are blocks of rows for the fixed-effects (\\(\\boldsymbol{\\beta}\\)) parameters, the variance components (on the scale of a standard deviation), and the \\(\\boldsymbol{\\theta}\\) parameters that generate the covariance factor \\(\\boldsymbol{\\Lambda}_{\\boldsymbol{\\theta}}\\). (At present the correlation parameters are not profiled - we may add them later but that computation is rather awkward.)\n\nshow(unique(pr01.tbl.p))\n\n[:σ, :β1, :β2, :β3, :β4, :θ1, :θ2, :θ3, :θ4, :σ1, :σ2, :σ3]\n\n\nTo reiterate, the first row contains the parameter estimates for this model fit to the original response values with the constraint that \\(\\sigma=0.130088\\), instead of the global estimate \\(\\hat{\\sigma}=0.139458\\) in the row for which \\(\\zeta=0.0\\).\nThe global estimates are included in every block at the row for which \\(\\zeta=0.0\\).\n\nfilter(r -&gt; iszero(r.ζ), pr01.tbl)\n\nTable with 15 columns and 12 rows:\n      p   ζ    β1        β2         β3        β4         σ         σ1         ⋯\n    ┌──────────────────────────────────────────────────────────────────────────\n 1  │ σ   0.0  0.531523  0.0212959  0.011218  0.0698293  0.139458  0.0781364  ⋯\n 2  │ β1  0.0  0.531523  0.0212959  0.011218  0.0698293  0.139458  0.0781364  ⋯\n 3  │ β2  0.0  0.531523  0.0212959  0.011218  0.0698293  0.139458  0.0781364  ⋯\n 4  │ β3  0.0  0.531523  0.0212959  0.011218  0.0698293  0.139458  0.0781364  ⋯\n 5  │ β4  0.0  0.531523  0.0212959  0.011218  0.0698293  0.139458  0.0781364  ⋯\n 6  │ θ1  0.0  0.531523  0.0212959  0.011218  0.0698293  0.139458  0.0781364  ⋯\n 7  │ θ2  0.0  0.531523  0.0212959  0.011218  0.0698293  0.139458  0.0781364  ⋯\n 8  │ θ3  0.0  0.531523  0.0212959  0.011218  0.0698293  0.139458  0.0781364  ⋯\n 9  │ θ4  0.0  0.531523  0.0212959  0.011218  0.0698293  0.139458  0.0781364  ⋯\n 10 │ σ1  0.0  0.531523  0.0212959  0.011218  0.0698293  0.139458  0.0781364  ⋯\n 11 │ σ2  0.0  0.531523  0.0212959  0.011218  0.0698293  0.139458  0.0781364  ⋯\n 12 │ σ3  0.0  0.531523  0.0212959  0.011218  0.0698293  0.139458  0.0781364  ⋯\n\n\nThe \\(\\zeta\\) column in this table is a measure of the quality of the fit from the parameters in each row, relative to the global parameter estimates, as measured by the change in the objective (negative twice the log-likelihood).\nThe minimum value for the objective is that at the global parameter estimates. The change in the objective when we constrain one parameter to a particular value has approximately a \\(\\chi^2\\) distribution on 1 degree of freedom, which is the square of a standard normal distribution, \\(\\mathcal{Z}^2\\). We can convert this change in the quality of the fit to the scale of the standard normal distribution by taking the signed square root, which is the square root of the change in the objective with the sign of \\(\\psi-\\hat{\\psi}\\) where \\(\\psi\\) represents the parameter being profiled. This is the value labelled \\(\\zeta\\) in the table.\nTo review:\n\nEach row in the table is the result of re-fitting the original model with the parameter in the p column held fixed at a particular value, as shown in the column for that parameter.\nThe \\(\\zeta\\) column is the signed square root of the change in the objective from the global parameter estimates.\nThus in the block of rows where \\(\\sigma\\) is held fixed, the \\(\\zeta\\) values in rows for which \\(\\sigma&lt;\\hat\\sigma\\) are negative and those for which \\(\\sigma &gt; \\hat\\sigma\\) have positive values of \\(\\zeta\\).\nRows in which \\(\\zeta=0.0\\) are the global parameter estimates.",
    "crumbs": [
      "Bootstrap and profiling",
      "Confidence intervals from profiled objective"
    ]
  },
  {
    "objectID": "profiling.html#profile-zeta-plots",
    "href": "profiling.html#profile-zeta-plots",
    "title": "Confidence intervals from profiled objective",
    "section": "",
    "text": "Figure 1 shows, for each of the fixed effects parameters, \\(\\zeta\\) versus the parameter value.\n\n\nCode\nzetaplot!(Figure(; resolution=(1200, 350)), pr01; ptyp='β')\n\n\n\n\n\n\n\n\nFigure 1: ζ versus the value of the coefficient for the fixed-effects parameters in a model of response speed for the kb07 data.\n\n\n\n\n\nThe lines on these panels are read like normal probability plots, i.e. QQ plots against a standard normal distribution. Those on the \\(\\beta_2\\) and \\(\\beta_3\\) panels are, to the resolution of the plot, straight lines which indicates that the estimators of those parameters are normally distributed over the region of interest.\nThe points in the \\(\\beta_1\\) and \\(\\beta_4\\) panels are slightly over-dispersed relative to the straight line, which means that the estimators of these parameters are distributed like a T-distribution with a moderate number of degrees of freedom.\nThe profile-\\(\\zeta\\) function can be used to generate confidence intervals on the parameters\n\nconfint(pr01)\n\nDictTable with 3 columns and 8 rows:\n par   estimate   lower       upper\n ────┬─────────────────────────────────\n β1  │ 0.531523   0.497103    0.565942\n β2  │ 0.0212959  0.0148295   0.0277621\n β3  │ 0.011218   0.00475174  0.0176844\n β4  │ 0.0698293  0.0523046   0.0873562\n σ   │ 0.139458   0.13486     0.144322\n σ1  │ 0.0781364  0.0612443   0.103257\n σ2  │ 0.0452502  0.0338521   0.0618819\n σ3  │ 0.0736113  0.0600844   0.0916852\n\n\nas shown in Figure 2, which shows the absolute value of \\(\\zeta\\), which is simply the square root of the difference in the objective, versus the parameter being profiled.\n\n\nCode\nzetaplot!(Figure(; resolution=(1200, 330)), pr01; ptyp='β', absv=true)\n\n\n\n\n\n\n\n\nFigure 2: Absolute value of ζ versus value of the coefficient for the fixed-effects parameters in a model of response speed for the kb07 data. The horizontal lines are confidence intervals with nominal 50%, 80%, 90%, 95% and 99% confidence.\n\n\n\n\n\nThe 95% confidence intervals are the second horizontal lines from the top in each panel, at 1.96 on the vertical scale.\n\n\nCode\nzetaplot!(Figure(; resolution=(1200, 330)), pr01; ptyp='σ', absv=true)\n\n\n\n\n\n\n\n\nFigure 3: Absolute value of ζ versus value of the coefficient for the variance component parameters in a model of response speed for the kb07 data. The horizontal lines are confidence intervals with nominal 50%, 80%, 90%, 95% and 99% confidence.\n\n\n\n\n\nFigure 3 shows similar confidence intervals on the parameters representing standard deviations as does Figure 4 for the \\(\\theta\\) parameters.\n\n\nCode\nzetaplot!(Figure(; resolution=(1200, 330)), pr01; ptyp='θ', absv=true)\n\n\n\n\n\n\n\n\nFigure 4: Absolute value of ζ versus parameter value for the θ parameters in a model of response speed for the kb07 data. The horizontal lines are confidence intervals with nominal 50%, 80%, 90%, 95% and 99% confidence.",
    "crumbs": [
      "Bootstrap and profiling",
      "Confidence intervals from profiled objective"
    ]
  },
  {
    "objectID": "contrasts_kwdyz11.html",
    "href": "contrasts_kwdyz11.html",
    "title": "Contrast Coding of Visual Attention Effects",
    "section": "",
    "text": "Code\nusing Chain\nusing DataFrames\nusing MixedModels\nusing SMLP2023: dataset\nusing StatsBase\nusing StatsModels\n\nimport ProgressMeter\nProgressMeter.ijulia_behavior(:clear);",
    "crumbs": [
      "Contrast coding",
      "Contrast Coding of Visual Attention Effects"
    ]
  },
  {
    "objectID": "contrasts_kwdyz11.html#seqdiffcoding",
    "href": "contrasts_kwdyz11.html#seqdiffcoding",
    "title": "Contrast Coding of Visual Attention Effects",
    "section": "4.1 SeqDiffCoding",
    "text": "4.1 SeqDiffCoding\nThe SeqDiffCoding contrast corresponds to MASS::contr.sdif() in R. The assignment of random factors such as Subj to Grouping() is necessary when the sample size is very large. We recommend to include it always, but in this tutorial we do so only in the first example.\n\nm1 = let levels = [\"val\", \"sod\", \"dos\", \"dod\"]\n  contrasts = Dict(\n    :CTR =&gt; SeqDiffCoding(; levels),\n    :Subj =&gt; Grouping()\n  )\n  fit(MixedModel, form, dat1; contrasts)\nend\n\nMinimizing 197    Time: 0:00:00 ( 1.70 ms/it)\n  objective:  325809.5493080511\n\n\n\n\n\n\nEst.\nSE\nz\np\nσ_Subj\n\n\n\n\n(Intercept)\n389.7336\n7.0909\n54.96\n&lt;1e-99\n55.1962\n\n\nCTR: sod\n33.7817\n3.2873\n10.28\n&lt;1e-24\n23.2478\n\n\nCTR: dos\n13.9852\n2.3055\n6.07\n&lt;1e-08\n10.7515\n\n\nCTR: dod\n-2.7470\n2.2138\n-1.24\n0.2147\n9.5042\n\n\nResidual\n69.8349",
    "crumbs": [
      "Contrast coding",
      "Contrast Coding of Visual Attention Effects"
    ]
  },
  {
    "objectID": "contrasts_kwdyz11.html#hypothesiscoding",
    "href": "contrasts_kwdyz11.html#hypothesiscoding",
    "title": "Contrast Coding of Visual Attention Effects",
    "section": "4.2 HypothesisCoding",
    "text": "4.2 HypothesisCoding\nHypothesisCoding is the most general option available. We can implement all “canned” contrasts ourselves. The next example reproduces the test statistcs from SeqDiffCoding - with a minor modification illustrating the flexibility of going beyond the default version.\n\nm1b = let levels = [\"val\", \"sod\", \"dos\", \"dod\"]\n  contrasts = Dict(\n    :CTR =&gt; HypothesisCoding(\n      [\n        -1  1 0  0\n         0 -1 1  0\n         0  0 1 -1\n      ];\n      levels,\n      labels=[\"spt\", \"obj\", \"grv\"],\n    ),\n  )\n  fit(MixedModel, form, dat1; contrasts)\nend\n\n\n\n\n\nEst.\nSE\nz\np\nσ_Subj\n\n\n\n\n(Intercept)\n389.7336\n7.0913\n54.96\n&lt;1e-99\n55.1997\n\n\nCTR: spt\n33.7817\n3.2874\n10.28\n&lt;1e-24\n23.2481\n\n\nCTR: obj\n13.9852\n2.3057\n6.07\n&lt;1e-08\n10.7533\n\n\nCTR: grv\n2.7469\n2.2143\n1.24\n0.2148\n9.5105\n\n\nResidual\n69.8348\n\n\n\n\n\n\n\n\n\nThe difference to the preprogrammed SeqDiffCoding is that for the third contrast we changed the direction of the contrast such that the sign of the effect is positive when the result is in agreement with theoretical expectation, that is we subtract the fourth level from the third, not the third level from the fourth.",
    "crumbs": [
      "Contrast coding",
      "Contrast Coding of Visual Attention Effects"
    ]
  },
  {
    "objectID": "contrasts_kwdyz11.html#dummycoding",
    "href": "contrasts_kwdyz11.html#dummycoding",
    "title": "Contrast Coding of Visual Attention Effects",
    "section": "4.3 DummyCoding",
    "text": "4.3 DummyCoding\nThis contrast corresponds to contr.treatment() in R\n\nm2 = let\n  contrasts = Dict(:CTR =&gt; DummyCoding(; base=\"val\"))\n  fit(MixedModel, form, dat1; contrasts)\nend\n\n\n\n\n\nEst.\nSE\nz\np\nσ_Subj\n\n\n\n\n(Intercept)\n358.0914\n6.1545\n58.18\n&lt;1e-99\n47.9139\n\n\nCTR: dod\n45.0200\n4.3636\n10.32\n&lt;1e-24\n32.2912\n\n\nCTR: dos\n47.7669\n3.5566\n13.43\n&lt;1e-40\n25.5367\n\n\nCTR: sod\n33.7817\n3.2874\n10.28\n&lt;1e-24\n23.2481\n\n\nResidual\n69.8348\n\n\n\n\n\n\n\n\n\nThe DummyCoding contrast has the disadvantage that the intercept returns the mean of the level specified as base, default is the first level, not the GM.",
    "crumbs": [
      "Contrast coding",
      "Contrast Coding of Visual Attention Effects"
    ]
  },
  {
    "objectID": "contrasts_kwdyz11.html#ychycaeitcoding",
    "href": "contrasts_kwdyz11.html#ychycaeitcoding",
    "title": "Contrast Coding of Visual Attention Effects",
    "section": "4.4 YchycaeitCoding",
    "text": "4.4 YchycaeitCoding\nThe contrasts returned by DummyCoding may be exactly what we want. Can’t we have them, but also have the intercept estimate the GM, rather than the mean of the base level? Yes, we can! We call this “You can have your cake and it eat, too”-Coding (YchycaeitCoding). And we use HypothesisCoding to achieve this outcome.\n\nm2b = let levels = [\"val\", \"sod\", \"dos\", \"dod\"]\n  contrasts = Dict(\n    :CTR =&gt; HypothesisCoding(\n      [\n        -1 1 0 0\n        -1 0 1 0\n        -1 0 0 1\n      ];\n      levels,\n      labels=levels[2:end],\n    )\n  )\n  fit(MixedModel, form, dat1; contrasts)\nend\n\n\n\n\n\nEst.\nSE\nz\np\nσ_Subj\n\n\n\n\n(Intercept)\n389.7336\n7.0918\n54.96\n&lt;1e-99\n55.2039\n\n\nCTR: sod\n33.7817\n3.2877\n10.28\n&lt;1e-24\n23.2512\n\n\nCTR: dos\n47.7669\n3.5567\n13.43\n&lt;1e-40\n25.5374\n\n\nCTR: dod\n45.0200\n4.3637\n10.32\n&lt;1e-24\n32.2919\n\n\nResidual\n69.8349\n\n\n\n\n\n\n\n\n\nWe can simply relevel the factor or move the column with -1s for a different base.\n\nm2c = let levels = [\"val\", \"sod\", \"dos\", \"dod\"]\n  contrasts = Dict(\n    :CTR =&gt; HypothesisCoding(\n      [\n        -1/2 1/2   0   0\n        -1/2   0 1/2   0\n        -1/2   0   0  1/2\n      ];\n      levels,\n      labels=levels[2:end],\n    )\n  )\n  fit(MixedModel, form, dat1; contrasts)\nend\n\n\n\n\n\nEst.\nSE\nz\np\nσ_Subj\n\n\n\n\n(Intercept)\n389.7336\n7.0909\n54.96\n&lt;1e-99\n55.1963\n\n\nCTR: sod\n16.8909\n1.6436\n10.28\n&lt;1e-24\n11.6237\n\n\nCTR: dos\n23.8835\n1.7782\n13.43\n&lt;1e-40\n12.7676\n\n\nCTR: dod\n22.5100\n2.1817\n10.32\n&lt;1e-24\n16.1452\n\n\nResidual\n69.8348\n\n\n\n\n\n\n\n\n\nWe can simply relevel the factor or move the column with -1s for a different base.",
    "crumbs": [
      "Contrast coding",
      "Contrast Coding of Visual Attention Effects"
    ]
  },
  {
    "objectID": "contrasts_kwdyz11.html#effectscoding",
    "href": "contrasts_kwdyz11.html#effectscoding",
    "title": "Contrast Coding of Visual Attention Effects",
    "section": "4.5 EffectsCoding",
    "text": "4.5 EffectsCoding\nThis contrast corresponds almost to contr.sum() in R.\n\nm3 = let\n  contrasts = Dict(:CTR =&gt; EffectsCoding(; base=\"dod\"))\n  fit(MixedModel, form, dat1; contrasts)\nend\n\n\n\n\n\nEst.\nSE\nz\np\nσ_Subj\n\n\n\n\n(Intercept)\n389.7336\n7.1012\n54.88\n&lt;1e-99\n55.2772\n\n\nCTR: dos\n16.1247\n1.4410\n11.19\n&lt;1e-28\n7.3387\n\n\nCTR: sod\n2.1396\n1.3352\n1.60\n0.1091\n6.0268\n\n\nCTR: val\n-31.6422\n2.6439\n-11.97\n&lt;1e-32\n19.9638\n\n\nResidual\n69.8343\n\n\n\n\n\n\n\n\n\nThe “almost” qualification refers to the fact that contr.sum() uses the last factor levels as default base level; EffectsCoding uses the first level.\n\nm3b = let levels = [ \"dod\", \"val\", \"sod\", \"dos\"]\n  contrasts = Dict(\n    :CTR =&gt; HypothesisCoding(\n      [\n         -1/4   3/4 -1/4  -1/4\n         -1/4  -1/4  3/4  -1/4\n         -1/4  -1/4 -1/4   3/4\n      ];\n      levels,\n      labels=levels[2:end],\n    )\n  )\n  fit(MixedModel, form, dat1; contrasts)\nend\n\n\n\n\n\nEst.\nSE\nz\np\nσ_Subj\n\n\n\n\n(Intercept)\n389.7336\n7.0905\n54.97\n&lt;1e-99\n55.1933\n\n\nCTR: val\n-31.6422\n2.6424\n-11.97\n&lt;1e-32\n19.9515\n\n\nCTR: sod\n2.1396\n1.3339\n1.60\n0.1087\n6.0089\n\n\nCTR: dos\n16.1248\n1.4402\n11.20\n&lt;1e-28\n7.3291\n\n\nResidual\n69.8349\n\n\n\n\n\n\n\n\n\n\nm3c = let levels = [ \"dod\", \"val\", \"sod\", \"dos\"]\n  contrasts = Dict(\n    :CTR =&gt; HypothesisCoding(\n      [\n         -1/2   3/2 -1/2  -1/2\n         -1/2  -1/2  3/2  -1/2\n         -1/2  -1/2 -1/2   3/2\n      ];\n      levels,\n      labels=levels[2:end],\n    )\n  )\n  fit(MixedModel, form, dat1; contrasts)\nend\n\n\n\n\n\nEst.\nSE\nz\np\nσ_Subj\n\n\n\n\n(Intercept)\n389.7336\n7.0900\n54.97\n&lt;1e-99\n55.1893\n\n\nCTR: val\n-63.2843\n5.2844\n-11.98\n&lt;1e-32\n39.8997\n\n\nCTR: sod\n4.2792\n2.6679\n1.60\n0.1087\n12.0195\n\n\nCTR: dos\n32.2495\n2.8809\n11.19\n&lt;1e-28\n14.6648\n\n\nResidual\n69.8348",
    "crumbs": [
      "Contrast coding",
      "Contrast Coding of Visual Attention Effects"
    ]
  },
  {
    "objectID": "contrasts_kwdyz11.html#helmertcoding",
    "href": "contrasts_kwdyz11.html#helmertcoding",
    "title": "Contrast Coding of Visual Attention Effects",
    "section": "4.6 HelmertCoding",
    "text": "4.6 HelmertCoding\nHelmertCoding codes each level as the difference from the average of the lower levels. With the default order of CTR levels we get the following test statistics. These contrasts are othogonal.\n\nm4 = let\n  contrasts = Dict(:CTR =&gt; HelmertCoding())\n  fit(MixedModel, form, dat1; contrasts)\nend\n\n\n\n\n\nEst.\nSE\nz\np\nσ_Subj\n\n\n\n\n(Intercept)\n389.7336\n7.0913\n54.96\n&lt;1e-99\n55.1998\n\n\nCTR: dos\n1.3735\n1.1070\n1.24\n0.2147\n4.7539\n\n\nCTR: sod\n-4.2039\n0.6843\n-6.14\n&lt;1e-09\n3.3491\n\n\nCTR: val\n-10.5474\n0.8808\n-11.98\n&lt;1e-32\n6.6502\n\n\nResidual\n69.8348\n\n\n\n\n\n\n\n\n\n+ HeC1: (2 - 1)/2           # (391 - 358)/2\n+ HeC2: (3 - (2+1)/2)/3     # (405 - (391 + 358)/2)/3\n+ HeC3: (4 - (3+2+1)/3)/4   # (402 - (405 + 391 + 358)/3)/4",
    "crumbs": [
      "Contrast coding",
      "Contrast Coding of Visual Attention Effects"
    ]
  },
  {
    "objectID": "contrasts_kwdyz11.html#reverse-helmertcoding",
    "href": "contrasts_kwdyz11.html#reverse-helmertcoding",
    "title": "Contrast Coding of Visual Attention Effects",
    "section": "4.7 Reverse HelmertCoding",
    "text": "4.7 Reverse HelmertCoding\nReverse HelmertCoding codes each level as the difference from the average of the higher levels. To estimate these effects we simply reverse the order of factor levels. Of course, the contrasts are also orthogonal.\n\nm4b = let levels = reverse(StatsModels.levels(dat1.CTR))\n  contrasts = Dict(:CTR =&gt; HelmertCoding(; levels))\n  fit(MixedModel, form, dat1; contrasts)\nend\n\n\n\n\n\nEst.\nSE\nz\np\nσ_Subj\n\n\n\n\n(Intercept)\n389.7336\n7.0913\n54.96\n&lt;1e-99\n55.1998\n\n\nCTR: dos\n1.3735\n1.1070\n1.24\n0.2147\n4.7539\n\n\nCTR: sod\n-4.2039\n0.6843\n-6.14\n&lt;1e-09\n3.3491\n\n\nCTR: val\n-10.5474\n0.8808\n-11.98\n&lt;1e-32\n6.6502\n\n\nResidual\n69.8348\n\n\n\n\n\n\n\n\n\n+ HeC1:(3 - 4)/2            # (405 - 402)/2\n+ HeC2:(2 - (3+4)/2)/3      # (391 - (405 + 402)/2)/3\n+ HeC3:(1 - (2+3+4)/3/4     # (356  -(391 + 405 + 402)/3)/4",
    "crumbs": [
      "Contrast coding",
      "Contrast Coding of Visual Attention Effects"
    ]
  },
  {
    "objectID": "contrasts_kwdyz11.html#anova-coding",
    "href": "contrasts_kwdyz11.html#anova-coding",
    "title": "Contrast Coding of Visual Attention Effects",
    "section": "4.8 Anova Coding",
    "text": "4.8 Anova Coding\nFactorial designs (i.e., lab experiments) are traditionally analyzed with analysis of variance. The test statistics of main effects and interactions are based on an orthogonal set of contrasts. We specify them with HypothesisCoding.\n\n4.8.1 A(2) x B(2)\nAn A(2) x B(2) design can be recast as an F(4) design with the levels (A1-B1, A1-B2, A2-B1, A2-B2). The following contrast specification returns estimates for the main effect of A, the main effect of B, and the interaction of A and B. In a figure With A on the x-axis and the levels of B shown as two lines, the interaction tests the null hypothesis that the two lines are parallel. A positive coefficient implies overadditivity (diverging lines toward the right) and a negative coefficient underadditivity (converging lines).\n\nm5 = let levels = [\"val\", \"sod\", \"dos\", \"dod\"]\n  contrasts = Dict(\n    :CTR =&gt; HypothesisCoding(\n      [\n        -1 -1 +1 +1          # A\n        -1 +1 -1 +1          # B\n        +1 -1 -1 +1          # A x B\n      ];\n      levels,\n      labels=[\"A\", \"B\", \"AxB\"],\n    ),\n  )\n  fit(MixedModel, form, dat1; contrasts)\nend\n\n\n\n\n\nEst.\nSE\nz\np\nσ_Subj\n\n\n\n\n(Intercept)\n389.7336\n7.0917\n54.96\n&lt;1e-99\n55.2028\n\n\nCTR: A\n59.0052\n5.1827\n11.39\n&lt;1e-29\n36.2088\n\n\nCTR: B\n31.0348\n4.6753\n6.64\n&lt;1e-10\n31.7164\n\n\nCTR: AxB\n-36.5287\n3.0931\n-11.81\n&lt;1e-31\n16.0093\n\n\nResidual\n69.8348\n\n\n\n\n\n\n\n\n\nIt is also helpful to see the corresponding layout of the four means for the interaction of A and B (i.e., the third contrast)\n        B1     B2\n   A1   +1     -1\n   A2   -1     +1\nThus, interaction tests whether the difference between main diagonal and minor diagonal is different from zero.\n\n\n4.8.2 A(2) x B(2) x C(2)\nGoing beyond the four level factor; it is also helpful to see the corresponding layout of the eight means for the interaction of A and B and C.\n          C1              C2\n      B1     B2        B1     B2\n A1   +1     -1   A1   -1     +1\n A2   -1     +1   A2   +1     -1\n\n\n4.8.3 A(2) x B(2) x C(3)\nTO BE DONE",
    "crumbs": [
      "Contrast coding",
      "Contrast Coding of Visual Attention Effects"
    ]
  },
  {
    "objectID": "contrasts_kwdyz11.html#nested-coding",
    "href": "contrasts_kwdyz11.html#nested-coding",
    "title": "Contrast Coding of Visual Attention Effects",
    "section": "4.9 Nested coding",
    "text": "4.9 Nested coding\nNested contrasts are often specified as follow up as post-hoc tests for ANOVA interactions. They are orthogonal. We specify them with HypothesisCoding.\nAn A(2) x B(2) design can be recast as an F(4) design with the levels (A1-B1, A1-B2, A2-B1, A2-B2). The following contrast specification returns an estimate for the main effect of A and the effects of B nested in the two levels of A. In a figure With A on the x-axis and the levels of B shown as two lines, the second contrast tests whether A1-B1 is different from A1-B2 and the third contrast tests whether A2-B1 is different from A2-B2.\n\nm8 = let levels = [\"val\", \"sod\", \"dos\", \"dod\"]\n  contrasts = Dict(\n    :CTR =&gt; HypothesisCoding(\n      [\n        -1 -1 +1 +1\n        -1 +1  0  0\n         0  0 +1 -1\n      ];\n      levels,\n      labels=[\"do_so\", \"spt\", \"grv\"],\n    ),\n  )\n  fit(MixedModel, form, dat1; contrasts)\nend\n\n\n\n\n\nEst.\nSE\nz\np\nσ_Subj\n\n\n\n\n(Intercept)\n389.7336\n7.0914\n54.96\n&lt;1e-99\n55.2003\n\n\nCTR: do_so\n59.0052\n5.1825\n11.39\n&lt;1e-29\n36.2074\n\n\nCTR: spt\n33.7817\n3.2873\n10.28\n&lt;1e-24\n23.2474\n\n\nCTR: grv\n2.7470\n2.2142\n1.24\n0.2148\n9.5101\n\n\nResidual\n69.8348\n\n\n\n\n\n\n\n\n\nThe three contrasts for one main effect and two nested contrasts are orthogonal. There is no test of the interaction (parallelism).",
    "crumbs": [
      "Contrast coding",
      "Contrast Coding of Visual Attention Effects"
    ]
  },
  {
    "objectID": "contrasts_kwdyz11.html#standard-contrasts",
    "href": "contrasts_kwdyz11.html#standard-contrasts",
    "title": "Contrast Coding of Visual Attention Effects",
    "section": "6.1 Standard contrasts",
    "text": "6.1 Standard contrasts\nThe most commonly used contrasts are DummyCoding and EffectsCoding (which are similar to contr.treatment() and contr.sum() in R, respectively).",
    "crumbs": [
      "Contrast coding",
      "Contrast Coding of Visual Attention Effects"
    ]
  },
  {
    "objectID": "contrasts_kwdyz11.html#exotic-contrasts-rk-well",
    "href": "contrasts_kwdyz11.html#exotic-contrasts-rk-well",
    "title": "Contrast Coding of Visual Attention Effects",
    "section": "6.2 “Exotic” contrasts (rk: well …)",
    "text": "6.2 “Exotic” contrasts (rk: well …)\nWe also provide HelmertCoding and SeqDiffCoding (corresponding to base R’s contr.helmert() and MASS::contr.sdif()).",
    "crumbs": [
      "Contrast coding",
      "Contrast Coding of Visual Attention Effects"
    ]
  },
  {
    "objectID": "contrasts_kwdyz11.html#manual-contrasts",
    "href": "contrasts_kwdyz11.html#manual-contrasts",
    "title": "Contrast Coding of Visual Attention Effects",
    "section": "6.3 Manual contrasts",
    "text": "6.3 Manual contrasts\nContrastsCoding()\nThere are two ways to manually specify contrasts. First, you can specify them directly via ContrastsCoding. If you do, it’s good practice to specify the levels corresponding to the rows of the matrix, although they can be omitted in which case they’ll be inferred from the data.\nHypothesisCoding()\nA better way to specify manual contrasts is via HypothesisCoding, where each row of the matrix corresponds to the weights given to the cell means of the levels corresponding to each column (see Schad et al. (2020) for more information).",
    "crumbs": [
      "Contrast coding",
      "Contrast Coding of Visual Attention Effects"
    ]
  },
  {
    "objectID": "check_emotikon_transform.html",
    "href": "check_emotikon_transform.html",
    "title": "Transformed and original metrics in Emotikon",
    "section": "",
    "text": "In Fühner et al. (2021) the original metric of two tasks (Star, S20) is time, but they were transformed to speed scores in the publication prior to computing z-scores. The critical result is the absence of evidence for the age x Sex x Test interaction. Is this interaction significant if we analyse all tasks in their original metric?\nFitting the LMM of the publication takes time, roughly 1 hour. However, if you save the model parameters (and other relevant information), you can restore the fitted model object very quickly. The notebook also illustrates this procedure."
  },
  {
    "objectID": "check_emotikon_transform.html#getting-the-packages-and-data",
    "href": "check_emotikon_transform.html#getting-the-packages-and-data",
    "title": "Transformed and original metrics in Emotikon",
    "section": "1 Getting the packages and data",
    "text": "1 Getting the packages and data\n\n\nCode\nusing AlgebraOfGraphics\nusing Arrow\nusing CairoMakie\nusing DataFrames\nusing DataFrameMacros\nusing MixedModels\nusing MixedModelsMakie\nusing RCall\nusing Serialization\nusing StatsBase\n\nCairoMakie.activate!(; type=\"svg\")\ndatadir = joinpath(@__DIR__, \"data\");\n\n\n\n1.1 Data and figure in publication\n\ndat = DataFrame(Arrow.Table(joinpath(datadir, \"fggk21.arrow\")))\n@transform!(dat, :a1 = :age - 8.5);\nselect!(groupby(dat, :Test), :, :score =&gt; zscore =&gt; :zScore);\ndescribe(dat)\n\n9×7 DataFrame\n\n\n\nRow\nvariable\nmean\nmin\nmedian\nmax\nnmissing\neltype\n\n\n\nSymbol\nUnion…\nAny\nUnion…\nAny\nInt64\nDataType\n\n\n\n\n1\nTest\n\nBPT\n\nStar_r\n0\nString\n\n\n2\nCohort\n\n2011\n\n2019\n0\nString\n\n\n3\nSchool\n\nS100043\n\nS800200\n0\nString\n\n\n4\nChild\n\nC002352\n\nC117966\n0\nString\n\n\n5\nSex\n\nfemale\n\nmale\n0\nString\n\n\n6\nage\n8.56073\n7.99452\n8.55852\n9.10609\n0\nFloat64\n\n\n7\nscore\n226.141\n1.14152\n4.65116\n1530.0\n0\nFloat64\n\n\n8\na1\n0.0607297\n-0.505476\n0.0585216\n0.606092\n0\nFloat64\n\n\n9\nzScore\n-3.91914e-13\n-3.1542\n0.00031088\n3.55078\n0\nFloat64\n\n\n\n\n\n\n\n\n1.2 Data and figure with z-scores based on original metric\n\n# dat_om = rcopy(R\"readRDS('./data/fggk21_om.rds')\");  #Don't know what the _om is\n# @transform!(dat_om, :a1 = :age - 8.5);\n# select!(groupby(dat_om, :Test), :, :score =&gt; zscore =&gt; :zScore);\n# describe(dat_om)"
  },
  {
    "objectID": "check_emotikon_transform.html#lmms",
    "href": "check_emotikon_transform.html#lmms",
    "title": "Transformed and original metrics in Emotikon",
    "section": "2 LMMs",
    "text": "2 LMMs\n\n2.1 Contrasts\n\ncontrasts = Dict(\n  :Test =&gt; SeqDiffCoding(),\n  :Sex =&gt; HelmertCoding(),\n  :School =&gt; Grouping(),\n  :Child =&gt; Grouping(),\n  :Cohort =&gt; Grouping(),\n);\n\n\n\n2.2 Formula\n\nf1 = @formula zScore ~\n  1 +\n  Test * a1 * Sex +\n  (1 + Test + a1 + Sex | School) +\n  (1 + Test | Child) +\n  zerocorr(1 + Test | Cohort);\n\n\n\n2.3 Restore LMM m1 from publication\n\nCommand for fitting LMM m1 = fit(MixedModel, f1, dat, contrasts=contr)\nFit statistics for LMM m1: Minimizing 5179 Time: 0 Time: 1:00:38 ( 0.70 s/it)\n\n\nm1x = LinearMixedModel(f1, dat; contrasts)\nrestoreoptsum!(m1x, \"./fits/fggk21_m1_optsum.json\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEst.\nSE\nz\np\nσ_Child\nσ_School\nσ_Cohort\n\n\n\n\n(Intercept)\n-0.0383\n0.0108\n-3.56\n0.0004\n0.5939\n0.2024\n0.0157\n\n\nTest: Run\n-0.0228\n0.0274\n-0.83\n0.4052\n0.8384\n0.3588\n0.0651\n\n\nTest: S20_r\n-0.0147\n0.0405\n-0.36\n0.7171\n0.5825\n0.3596\n0.1107\n\n\nTest: SLJ\n0.0328\n0.0330\n0.99\n0.3198\n0.4127\n0.3027\n0.0896\n\n\nTest: Star_r\n0.0006\n0.0197\n0.03\n0.9763\n0.5574\n0.3620\n0.0313\n\n\na1\n0.2713\n0.0086\n31.63\n&lt;1e-99\n\n0.0966\n\n\n\nSex: male\n0.2064\n0.0024\n86.55\n&lt;1e-99\n\n0.0245\n\n\n\nTest: Run & a1\n-0.4464\n0.0131\n-34.05\n&lt;1e-99\n\n\n\n\n\nTest: S20_r & a1\n0.1473\n0.0114\n12.97\n&lt;1e-37\n\n\n\n\n\nTest: SLJ & a1\n-0.0068\n0.0103\n-0.66\n0.5116\n\n\n\n\n\nTest: Star_r & a1\n0.0761\n0.0111\n6.84\n&lt;1e-11\n\n\n\n\n\nTest: Run & Sex: male\n-0.0900\n0.0037\n-24.10\n&lt;1e-99\n\n\n\n\n\nTest: S20_r & Sex: male\n-0.0912\n0.0032\n-28.23\n&lt;1e-99\n\n\n\n\n\nTest: SLJ & Sex: male\n0.0330\n0.0029\n11.24\n&lt;1e-28\n\n\n\n\n\nTest: Star_r & Sex: male\n-0.0720\n0.0032\n-22.65\n&lt;1e-99\n\n\n\n\n\na1 & Sex: male\n0.0010\n0.0069\n0.14\n0.8876\n\n\n\n\n\nTest: Run & a1 & Sex: male\n-0.0154\n0.0126\n-1.22\n0.2233\n\n\n\n\n\nTest: S20_r & a1 & Sex: male\n0.0129\n0.0109\n1.18\n0.2380\n\n\n\n\n\nTest: SLJ & a1 & Sex: male\n-0.0098\n0.0100\n-0.98\n0.3256\n\n\n\n\n\nTest: Star_r & a1 & Sex: male\n0.0166\n0.0108\n1.54\n0.1241\n\n\n\n\n\nResidual\n0.5880\n\n\n\n\n\n\n\n\n\n\n\n\nVarCorr(m1x)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nColumn\nVariance\nStd.Dev\nCorr.\n\n\n\n\n\n\n\n\n\nChild\n(Intercept)\n0.3527294\n0.5939103\n\n\n\n\n\n\n\n\n\nTest: Run\n0.7029003\n0.8383915\n+0.11\n\n\n\n\n\n\n\n\nTest: S20_r\n0.3393356\n0.5825252\n+0.19\n-0.53\n\n\n\n\n\n\n\nTest: SLJ\n0.1702900\n0.4126621\n+0.05\n-0.14\n-0.29\n\n\n\n\n\n\nTest: Star_r\n0.3107227\n0.5574251\n-0.10\n+0.01\n-0.13\n-0.42\n\n\n\n\nSchool\n(Intercept)\n0.0409640\n0.2023957\n\n\n\n\n\n\n\n\n\nTest: Run\n0.1287690\n0.3588440\n+0.26\n\n\n\n\n\n\n\n\nTest: S20_r\n0.1293351\n0.3596319\n+0.01\n-0.57\n\n\n\n\n\n\n\nTest: SLJ\n0.0916522\n0.3027411\n-0.13\n+0.01\n-0.53\n\n\n\n\n\n\nTest: Star_r\n0.1310575\n0.3620187\n+0.26\n+0.09\n-0.06\n-0.28\n\n\n\n\n\na1\n0.0093412\n0.0966499\n+0.48\n+0.25\n-0.15\n-0.01\n+0.12\n\n\n\n\nSex: male\n0.0005999\n0.0244934\n+0.09\n+0.13\n-0.01\n+0.05\n-0.19\n+0.25\n\n\nCohort\n(Intercept)\n0.0002452\n0.0156587\n\n\n\n\n\n\n\n\n\nTest: Run\n0.0042389\n0.0651068\n.\n\n\n\n\n\n\n\n\nTest: S20_r\n0.0122535\n0.1106954\n.\n.\n\n\n\n\n\n\n\nTest: SLJ\n0.0080210\n0.0895599\n.\n.\n.\n\n\n\n\n\n\nTest: Star_r\n0.0009828\n0.0313498\n.\n.\n.\n.\n\n\n\n\nResidual\n\n0.3456872\n0.5879517\n\n\n\n\n\n\n\n\n\n\n\n\n\n2.4 Restore new LMM m1_om Star and S20 in original metric\n\nCommand for fitting LMM m1_om = fit(MixedModel, f1, dat_om, contrasts=contr)\nMinimizing 10502 Time: 0 Time: 2:09:40 ( 0.74 s/it)\nStore with: julia&gt; saveoptsum(“./fits/fggk21_m1_om_optsum.json”, m1_om)\nOnly for short-term and when desperate: julia&gt; serialize(“./fits/m1_om.jls”, m1_om);\n\n\n2.4.1 … restoreoptsum!()\n\nm1_om = LinearMixedModel(f1, dat; contrasts=contr);\nrestoreoptsum!(m1_om, \"./fits/fggk21_m1_om_optsum.json\");\n\n\n\n2.4.2 … deserialize()\n\nm1x_om = deserialize(\"./fits/m1_om.jls\")\n\n\nVarCorr(m1x_om)\n\n\n\n\n2.5 Residual diagnostics for LMM m1\nResidual plots for published LMM\n\n#scatter(fitted(m1x), residuals(m1x)\n\n\n#qqnorm(m1x)\n\n\n\n2.6 Residual diagnostics for LMM m1_om\nResidual plots for LMM with Star and Speed in original metric.\n\n#scatter(fitted(m1_om_v2), residuals(m1_om_v2)\n\n\n#qqnorm(m1_om_v2)\n\n\n\nFühner, T., Granacher, U., Golle, K., & Kliegl, R. (2021). Age and sex effects in physical fitness components of 108,295 third graders including 515 primary schools and 9 cohorts. Scientific Reports, 11(1). https://doi.org/10.1038/s41598-021-97000-4"
  },
  {
    "objectID": "AoGPlots.html",
    "href": "AoGPlots.html",
    "title": "Creating multi-panel plots",
    "section": "",
    "text": "This notebook shows creating a multi-panel plot similar to Figure 2 of Fühner et al. (2021).\nThe data are available from the SMLP2023 example datasets.\n\n\nCode\nusing Arrow\nusing AlgebraOfGraphics\nusing CairoMakie   # for displaying static plots\nusing DataFrames\nusing Statistics\nusing StatsBase\nusing SMLP2023: dataset\n\nCairoMakie.activate!(; type=\"svg\") # use SVG (other options include PNG)\n\n\n\ntbl = dataset(\"fggk21\")\n\nArrow.Table with 525126 rows, 7 columns, and schema:\n :Cohort  String\n :School  String\n :Child   String\n :Sex     String\n :age     Float64\n :Test    String\n :score   Float64\n\n\n\ntypeof(tbl)\n\nArrow.Table\n\n\n\ndf = DataFrame(tbl)\ntypeof(df)\n\nDataFrame\n\n\n\n1 Creating a summary data frame\nThe response to be plotted is the mean score by Test and Sex and age, rounded to the nearest 0.1 years.\nThe first task is to round the age to 1 digit after the decimal place, which can be done with select applied to a DataFrame. In some ways this is the most complicated expression in creating the plot so we will break it down. select is applied to DataFrame(dat), which is the conversion of the Arrow.Table, dat, to a DataFrame. This is necessary because an Arrow.Table is immutable but a DataFrame can be modified.\nThe arguments after the DataFrame describe how to modify the contents. The first : indicates that all the existing columns should be included. The other expression can be pairs (created with the =&gt; operator) of the form :col =&gt; function or of the form :col =&gt; function =&gt; :newname. (See the documentation of the DataFrames package for details.)\nIn this case the function is an anonymous function of the form round.(x, digits=1) where “dot-broadcasting” is used to apply to the entire column (see this documentation for details).\n\ntransform!(df, :age, :age =&gt; (x -&gt; x .- 8.5) =&gt; :a1) # centered age (linear)\nselect!(groupby(df, :Test), :, :score =&gt; zscore =&gt; :zScore) # z-score\ntlabels = [     # establish order and labels of tbl.Test\n  \"Run\" =&gt; \"Endurance\",\n  \"Star_r\" =&gt; \"Coordination\",\n  \"S20_r\" =&gt; \"Speed\",\n  \"SLJ\" =&gt; \"PowerLOW\",\n  \"BPT\" =&gt; \"PowerUP\",\n];\n\nThe next stage is a group-apply-combine operation to group the rows by Sex, Test and rnd_age then apply mean to the zScore and also apply length to zScore to record the number in each group.\n\ndf2 = combine(\n  groupby(\n    select(df, :, :age =&gt; ByRow(x -&gt; round(x; digits=1)) =&gt; :age),\n    [:Sex, :Test, :age],\n  ),\n  :zScore =&gt; mean =&gt; :zScore,\n  :zScore =&gt; length =&gt; :n,\n)\n\n120×5 DataFrame95 rows omitted\n\n\n\nRow\nSex\nTest\nage\nzScore\nn\n\n\n\nString\nString\nFloat64\nFloat64\nInt64\n\n\n\n\n1\nmale\nS20_r\n8.0\n-0.0265138\n1223\n\n\n2\nmale\nBPT\n8.0\n0.026973\n1227\n\n\n3\nmale\nSLJ\n8.0\n0.121609\n1227\n\n\n4\nmale\nStar_r\n8.0\n-0.0571726\n1186\n\n\n5\nmale\nRun\n8.0\n0.292695\n1210\n\n\n6\nfemale\nS20_r\n8.0\n-0.35164\n1411\n\n\n7\nfemale\nBPT\n8.0\n-0.610355\n1417\n\n\n8\nfemale\nSLJ\n8.0\n-0.279872\n1418\n\n\n9\nfemale\nStar_r\n8.0\n-0.268221\n1381\n\n\n10\nfemale\nRun\n8.0\n-0.245573\n1387\n\n\n11\nmale\nS20_r\n8.1\n0.0608397\n3042\n\n\n12\nmale\nBPT\n8.1\n0.0955413\n3069\n\n\n13\nmale\nSLJ\n8.1\n0.123099\n3069\n\n\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n\n\n109\nmale\nStar_r\n9.0\n0.254973\n4049\n\n\n110\nmale\nRun\n9.0\n0.258082\n4034\n\n\n111\nfemale\nS20_r\n9.1\n-0.0286172\n1154\n\n\n112\nfemale\nBPT\n9.1\n-0.0752301\n1186\n\n\n113\nfemale\nSLJ\n9.1\n-0.094587\n1174\n\n\n114\nfemale\nStar_r\n9.1\n0.00276252\n1162\n\n\n115\nfemale\nRun\n9.1\n-0.235591\n1150\n\n\n116\nmale\nS20_r\n9.1\n0.325745\n1303\n\n\n117\nmale\nBPT\n9.1\n0.616416\n1320\n\n\n118\nmale\nSLJ\n9.1\n0.267577\n1310\n\n\n119\nmale\nStar_r\n9.1\n0.254342\n1297\n\n\n120\nmale\nRun\n9.1\n0.251045\n1294\n\n\n\n\n\n\n\n\n2 Creating the plot\nThe AlgebraOfGraphics package applies operators to the results of functions such as data (specify the data table to be used), mapping (designate the roles of columns), and visual (type of visual presentation).\n\nlet\n  design = mapping(:age, :zScore; color=:Sex, col=:Test)\n  lines = design * linear()\n  means = design * visual(Scatter; markersize=5)\n  draw(data(df2) * means + data(df) * lines)\nend\n\n\n\n\n\n\n\n\n\nTBD: Relabel factor levels (Boys, Girls; fitness components for Test)\nTBD: Relevel factors; why not levels from Tables?\nTBD: Set range (7.8 to 9.2 and tick marks (8, 8.5, 9) of axes.\nTBD: Move legend in plot?\n\n\n\nFühner, T., Granacher, U., Golle, K., & Kliegl, R. (2021). Age and sex effects in physical fitness components of 108,295 third graders including 515 primary schools and 9 cohorts. Scientific Reports, 11(1). https://doi.org/10.1038/s41598-021-97000-4\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Visualizations and diagnostics",
      "Creating multi-panel plots"
    ]
  },
  {
    "objectID": "pkg.html",
    "href": "pkg.html",
    "title": "Package management and reproducible environments",
    "section": "",
    "text": "Julius Krumbiegel also has a great blog post with more details on Julia environments.\nJulia packages can be configured (in a file called Project.toml) on a per-project basis. The packaged sources and compiled versions are stored in a central location, e.g. ~/.julia/packages and ~/.julia/compiled on Linux systems, but the configuration of packages to be used can be local to a project. The Pkg package is used to modify the local project’s configuration. (An alternative is “package mode” in the read-eval-print-loop or REPL, which we will show at the summer school.) Start julia in the directory of the cloned SMLP2023 repository\n\nusing Pkg        # there's a package called 'Pkg' to manipulate package configs\nPkg.activate(\".\")# activate the current directory as the project\n\nIf you’ve received an environment from someone/somewhere else – such as this course repository – then you’ll need to first “instantiate” it (i.e., install all the dependencies).\n\nPkg.instantiate()# only needed the first time you work in a project\nPkg.update()     # get the latest package versions compatible with the project\n\n\nPkg.status()\n\nOccasionally the Pkg.status function call will give info about new versions being available but blocked by requirements of other packages. This is to be expected - the package system is large and the web of dependencies are complex. Generally the Julia package system is very good at resolving dependencies.\n\n\n\n Back to top",
    "crumbs": [
      "Getting started with Julia",
      "Package management and reproducible environments"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Seventh Summer School on Statistical Methods for Linguistics and Psychology",
    "section": "",
    "text": "This site provides materials for the Advanced frequentist methods stream of the Summer School on Statistical Methods to be held at the University of Potsdam, 11-15 September, 2023."
  },
  {
    "objectID": "index.html#git",
    "href": "index.html#git",
    "title": "Seventh Summer School on Statistical Methods for Linguistics and Psychology",
    "section": "1.1 git",
    "text": "1.1 git\nWe will assume that you have git installed and are able to clone a repository from github. If not, Happy Git with R is a good place to learn about git for data science.\nThis website is built using quarto, described below, from the repository. Clone this repository with, e.g.\ngit clone https://github.com/RePsychLing/SMLP2023"
  },
  {
    "objectID": "index.html#julia-programming-language",
    "href": "index.html#julia-programming-language",
    "title": "Seventh Summer School on Statistical Methods for Linguistics and Psychology",
    "section": "1.2 Julia Programming Language",
    "text": "1.2 Julia Programming Language\nWe will use Julia v1.9 in the summer school. We recommend using Juliaup to install and manage Julia versions. Juliaup makes it trivial to upgrade to new Julia releases or even use old ones. Alternatively, you can download the version appropriate for your setup from here: Julia Programming Language"
  },
  {
    "objectID": "index.html#visual-studio-code-vs-code",
    "href": "index.html#visual-studio-code-vs-code",
    "title": "Seventh Summer School on Statistical Methods for Linguistics and Psychology",
    "section": "1.3 Visual Studio Code (VS Code)",
    "text": "1.3 Visual Studio Code (VS Code)\nWe will use VS Code IDE, that is Julia : VS Code ~ R : RStudio. You can download the version appropriate for your setup from here: VS Code"
  },
  {
    "objectID": "index.html#quarto",
    "href": "index.html#quarto",
    "title": "Seventh Summer School on Statistical Methods for Linguistics and Psychology",
    "section": "1.4 Quarto",
    "text": "1.4 Quarto\nThe web site and other documents for this course are rendered using a knitr-like system called Quarto. You can download the version appropriate for your setup from here: quarto"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n\n\n Back to top"
  },
  {
    "objectID": "useful_packages.html",
    "href": "useful_packages.html",
    "title": "Useful packages",
    "section": "",
    "text": "Unlike R, Julia does not immediately expose a huge number of functions, but instead requires loading packages (whether from the standard library or from the broader package ecosystem) for a lot of relevant functionality for statistical analysis. There are technical reasons for this, such as the ease of using the Julia package system. One further motivation is that Julia is aimed at a broader “technical computing” audience (like MATLAB or perhaps Python) and less at a “statistical analysis” audience.\nThis has two important implications:\nThis notebook is not intended to be an exhaustive list of packages, but rather to highlight a few packages that I suspect will be particularly useful. Before getting onto the packages, I have one final hint: take advantage of how easy and first-class package management in Julia is. Having good package management makes reproducible analyses much easier and avoids breaking old analyses when you start a new one. The package-manager REPL mode (activated by typing ] at the julia&gt; prompt) is very useful.",
    "crumbs": [
      "Getting started with Julia",
      "Useful packages"
    ]
  },
  {
    "objectID": "useful_packages.html#reading-data",
    "href": "useful_packages.html#reading-data",
    "title": "Useful packages",
    "section": "1.1 Reading data",
    "text": "1.1 Reading data\n\nArrow.jl a high performance format for data storage, accessible in R via the arrow package and in Python via pyarrow. (Confusingly, the function for reading and writing Arrow format files in R is called read_feather and write_feather, but the modern Arrow format is distinct from the older Feather format provided by the feather package.) This is the format that we store the example and test datasets in for MixedModels.jl.\nCSV.jl useful for reading comma-separated values, tab-separated values and basically everything handled by the read.csv and read.table family of functions in R.\n\nNote that by default both Arrow.jl and CSV.jl do not return a DataFrame, but rather “column tables” – named tuples of column vectors.",
    "crumbs": [
      "Getting started with Julia",
      "Useful packages"
    ]
  },
  {
    "objectID": "useful_packages.html#dataframes",
    "href": "useful_packages.html#dataframes",
    "title": "Useful packages",
    "section": "1.2 DataFrames",
    "text": "1.2 DataFrames\nUnlike in R, DataFrames are not part of the base language, nor the standard library.\nDataFrames.jl provides the basic infrastructure around DataFrames, as well as its own mini language for doing the split-apply-combine approach that underlies R’s dplyr and much of the tidyverse. The DataFrames.jl documentation is the place for looking at how to e.g. read in a CSV or Arrow file as a DataFrame. Note that DataFrames.jl by default depends on CategoricalArrays.jl to handle the equivalent of factor in the R world, but there is an alternative package for factor-like array type in Julia, PooledArrays.jl. PooledArrays are simpler, but more limited than CategoricalArrays and we (Phillip and Doug) sometimes use them in our examples and simulations. The tables produced by reading an Arrow file have their own representation of factor-like data as DictEncoded arrays.\nDataFrame.jl’s mini language can be a bit daunting, if you’re used to manipulations in the style of base R or the tidyverse. For that, there are several options; recently, we’e had particularly nice experiences with DataFrameMacros.jl and Chain.jl for a convenient syntax to connect or “pipe” together successive operations. It’s your choice whether and which of these add-ons you want to use! Phillip tends to write his code using raw DataFrames.jl, but Doug really enjoys DataFrameMacros.jl.\nThe recently added Tidier collection of Julia packages is popular with those coming from the tidyverse.",
    "crumbs": [
      "Getting started with Julia",
      "Useful packages"
    ]
  },
  {
    "objectID": "useful_packages.html#regression",
    "href": "useful_packages.html#regression",
    "title": "Useful packages",
    "section": "1.3 Regression",
    "text": "1.3 Regression\nUnlike in R, neither formula processing nor basic regression are part of the base language or the standard library.\nThe formula syntax and basic contrast-coding schemes in Julia is provided by StatsModels.jl. By default, MixedModels.jl re-exports the @formula macro and most commonly used contrast schemes from StatsModels.jl, so you often don’t have to worry about loading StatsModels.jl directly. The same is true for GLM.jl, which provides basic linear and generalized linear models, such as ordinary least squares (OLS) regression and logistic regression, i.e. the classical, non mixed regression models.\nThe basic functionality looks quite similar to R, e.g.\njulia &gt; lm(@formula(y ~ 1 + x), data)\njulia &gt; glm(@formula(y ~ 1 + x), data, Binomial(), LogitLink())\nbut the more general modelling API (also used by MixedModels.jl) is also supported:\njulia &gt; fit(LinearModel, @formula(y ~ 1 + x), mydata)\njulia &gt; fit(\n  GeneralizedLinearModel,\n  @formula(y ~ 1 + x),\n  data,\n  Binomial(),\n  LogitLink(),\n)\n(You can also specify your model matrices directly and skip the formula interface, but we don’t recommend this as it’s easy to mess up in really subtle but very problematic ways.)",
    "crumbs": [
      "Getting started with Julia",
      "Useful packages"
    ]
  },
  {
    "objectID": "useful_packages.html#formula-macros-and-domain-specific-languages",
    "href": "useful_packages.html#formula-macros-and-domain-specific-languages",
    "title": "Useful packages",
    "section": "1.4 @formula, macros and domain-specific languages",
    "text": "1.4 @formula, macros and domain-specific languages\nAs a sidebar: why is @formula a macro and not a normal function? Well, that’s because formulas are essentially their own domain-specific language (a variant of Wilkinson-Rogers notation) and macros are used for manipulating the language itself – or in this case, handling an entirely new, embedded language! This is also why macros are used by packages like Turing.jl and Soss.jl that define a language for Bayesian probabilistic programming like PyMC3 or Stan.",
    "crumbs": [
      "Getting started with Julia",
      "Useful packages"
    ]
  },
  {
    "objectID": "useful_packages.html#extensions-to-the-formula-syntax",
    "href": "useful_packages.html#extensions-to-the-formula-syntax",
    "title": "Useful packages",
    "section": "1.5 Extensions to the formula syntax",
    "text": "1.5 Extensions to the formula syntax\nThere are several ongoing efforts to extend the formula syntax to include some of the “extras” available in R, e.g. RegressionFormulae.jl to use the caret (^) notation to limit interactions to a certain order ((a+b+c)^2 generates a + b + c + a&b + a&c + b&c, but not a&b&c). Note also that Julia uses & to express interactions, not : like in R.",
    "crumbs": [
      "Getting started with Julia",
      "Useful packages"
    ]
  },
  {
    "objectID": "useful_packages.html#standardizing-predictors",
    "href": "useful_packages.html#standardizing-predictors",
    "title": "Useful packages",
    "section": "1.6 Standardizing Predictors",
    "text": "1.6 Standardizing Predictors\nAlthough function calls such as log can be used within Julia formulae, they must act on a rowwise basis, i.e. on observations. Transformations such as z-scoring or centering (often done with scale in R) require knowledge of the entire column. StandardizedPredictors.jl provides functions for centering, scaling, and z-scoring within the formula. These are treated as pseudo-contrasts and computed on demand, meaning that predict and effects (see next) computations will handle these transformations on new data (e.g. centering new data around the mean computed during fitting the original data) correctly and automatically.",
    "crumbs": [
      "Getting started with Julia",
      "Useful packages"
    ]
  },
  {
    "objectID": "useful_packages.html#effects",
    "href": "useful_packages.html#effects",
    "title": "Useful packages",
    "section": "1.7 Effects",
    "text": "1.7 Effects\nJohn Fox’s effects package in R (and the related ggeffects package for plotting these using ggplot2) provides a nice way to visualize a model’s overall view of the data. This functionality is provided by Effects.jl and works out-of-the-box with most regression model packages in Julia (including MixedModels.jl). Support for formulae with embedded functions (such as log) is not yet complete, but we’re working on it!",
    "crumbs": [
      "Getting started with Julia",
      "Useful packages"
    ]
  },
  {
    "objectID": "useful_packages.html#estimated-marginal-least-square-means",
    "href": "useful_packages.html#estimated-marginal-least-square-means",
    "title": "Useful packages",
    "section": "1.8 Estimated Marginal / Least Square Means",
    "text": "1.8 Estimated Marginal / Least Square Means\nEffects.jl provides a subset of the functionality (basic estimated-marginal means and exhaustive pairwise comparisons) of the R package emmeans package. However, it is often better to use sensible, hypothesis-driven contrast coding than to compute all pairwise comparisons after the fact. 😃",
    "crumbs": [
      "Getting started with Julia",
      "Useful packages"
    ]
  },
  {
    "objectID": "useful_packages.html#makie",
    "href": "useful_packages.html#makie",
    "title": "Useful packages",
    "section": "3.1 Makie",
    "text": "3.1 Makie\nThe Makie ecosystem is a relatively new take on graphics that aims to be both powerful and easy to use. Makie.jl itself only provides abstract definitions for many components (and is used in e.g. MixedModelsMakie.jl to define plot types for MixedModels.jl). The actual plotting and rendering is handled by a backend package such as CairoMakie.jl (good for Quarto notebooks or rending static 2D images) and GLMakie.jl (good for dynamic, interactive visuals and 3D images). AlgebraOfGraphics.jl builds a grammar of graphics upon the Makie framework. It’s a great way to get good plots very quickly, but extensive customization is still best achieved by using Makie directly.",
    "crumbs": [
      "Getting started with Julia",
      "Useful packages"
    ]
  },
  {
    "objectID": "useful_packages.html#plots.jl",
    "href": "useful_packages.html#plots.jl",
    "title": "Useful packages",
    "section": "3.2 Plots.jl",
    "text": "3.2 Plots.jl\nPlots.jl is the original plotting package in Julia, but we often find it difficult to work with compared to some of the other alternatives. StatsPlots.jl builds on this, adding common statistical plots, while UnicodePlots.jl renders plots as Unicode characters directly in the REPL.\nPGFPlotsX.jl is a very new package that writes directly to PGF (the format used by LaTeX’s tikz framework) and can stand alone or be used as a rendering backend for the Plots.jl ecosystem.",
    "crumbs": [
      "Getting started with Julia",
      "Useful packages"
    ]
  },
  {
    "objectID": "useful_packages.html#gadfly",
    "href": "useful_packages.html#gadfly",
    "title": "Useful packages",
    "section": "3.3 Gadfly",
    "text": "3.3 Gadfly\nGadfly.jl was the original attempt to create a plotting system in Julia based on the grammar of graphics (the “gg” in ggplot2). Development has largely stalled, but some functionality still exceeds AlgebraOfGraphics.jl, which has taken up the grammar of graphics mantle. Notably, the MixedModels.jl documentation still uses Gadfly as of this writing (early September 2021).",
    "crumbs": [
      "Getting started with Julia",
      "Useful packages"
    ]
  },
  {
    "objectID": "useful_packages.html#others",
    "href": "useful_packages.html#others",
    "title": "Useful packages",
    "section": "3.4 Others",
    "text": "3.4 Others\nThere are many other graphics packages available in Julia, often wrapping well-established frameworks such as VegaLite.",
    "crumbs": [
      "Getting started with Julia",
      "Useful packages"
    ]
  },
  {
    "objectID": "selection.html",
    "href": "selection.html",
    "title": "Raw score density",
    "section": "",
    "text": "Code\nusing Arrow\nusing CairoMakie\nusing DataFrames\nCairoMakie.activate!(; type=\"svg\") # use SVG (other options include PNG)\n\n\n\ntbl = Arrow.Table(\"./data/fggk21.arrow\")\n\nArrow.Table with 525126 rows, 7 columns, and schema:\n :Cohort  String\n :School  String\n :Child   String\n :Sex     String\n :age     Float64\n :Test    String\n :score   Float64\n\n\n\ndf = DataFrame(tbl)\ndescribe(df)\n\n7×7 DataFrame\n\n\n\nRow\nvariable\nmean\nmin\nmedian\nmax\nnmissing\neltype\n\n\n\nSymbol\nUnion…\nAny\nUnion…\nAny\nInt64\nDataType\n\n\n\n\n1\nCohort\n\n2011\n\n2019\n0\nString\n\n\n2\nSchool\n\nS100043\n\nS800200\n0\nString\n\n\n3\nChild\n\nC002352\n\nC117966\n0\nString\n\n\n4\nSex\n\nfemale\n\nmale\n0\nString\n\n\n5\nage\n8.56073\n7.99452\n8.55852\n9.10609\n0\nFloat64\n\n\n6\nTest\n\nBPT\n\nStar_r\n0\nString\n\n\n7\nscore\n226.141\n1.14152\n4.65116\n1530.0\n0\nFloat64\n\n\n\n\n\n\n\nlet\n  fdensity = Figure(; resolution=(1000, 500))\n  axs = Axis(fdensity[1, 1])\n  tdf = filter(:Test =&gt; ==(test), df)\n  colors = Makie.cgrad(:PuOr_4, 2; categorical=true, alpha=0.6)\n  if by_sex\n    density!(\n      axs,\n      filter(:Sex =&gt; ==(\"female\"), tdf).score;\n      color=colors[1],\n      label=\"Girls\",\n    )\n    density!(\n      axs,\n      filter(:Sex =&gt; ==(\"male\"), tdf).score;\n      color=colors[2],\n      label=\"Boys\",\n    )\n    axislegend(axs; position=:lt)\n  else\n    density!(axs, tdf.score)\n  end\n  fdensity\nend\n\n\n\n\n Back to top"
  },
  {
    "objectID": "singularity.html",
    "href": "singularity.html",
    "title": "Convergence, singularity and all that",
    "section": "",
    "text": "Add the packages to be used\n\n\nCode\nusing CairoMakie\nusing DataFrames\nusing LinearAlgebra\nusing MixedModels\nusing MixedModelsMakie\nusing ProgressMeter\n\nProgressMeter.ijulia_behavior(:clear)\nCairoMakie.activate!(; type=\"svg\")\n\n\nFit a model for reaction time in the sleepstudy example, preserving information on the estimation progress (the thin=1 optional argument)\n\nm01 = let f = @formula reaction ~ 1 + days + (1 + days|subj)\n  fit(MixedModel, f, MixedModels.dataset(:sleepstudy); thin=1)\nend\nprint(m01)\n\nLinear mixed model fit by maximum likelihood\n reaction ~ 1 + days + (1 + days | subj)\n   logLik   -2 logLik     AIC       AICc        BIC    \n  -875.9697  1751.9393  1763.9393  1764.4249  1783.0971\n\nVariance components:\n            Column    Variance Std.Dev.   Corr.\nsubj     (Intercept)  565.51067 23.78047\n         days          32.68212  5.71683 +0.08\nResidual              654.94145 25.59182\n Number of obs: 180; levels of grouping factors: 18\n\n  Fixed-effects parameters:\n──────────────────────────────────────────────────\n                Coef.  Std. Error      z  Pr(&gt;|z|)\n──────────────────────────────────────────────────\n(Intercept)  251.405      6.63226  37.91    &lt;1e-99\ndays          10.4673     1.50224   6.97    &lt;1e-11\n──────────────────────────────────────────────────\n\n\nThe covariance matrix for each subject’s random effects is evaluated from its “matrix square root”, called the Cholesky factor.\n\nλ = only(m01.λ)\n\n2×2 LowerTriangular{Float64, Matrix{Float64}}:\n 0.929221    ⋅ \n 0.0181684  0.222645\n\n\nThe transpose of \\(\\lambda\\), written \\(\\lambda'\\), is an upper triangular matrix generated by “flipping” \\(\\lambda\\) about the main diagonal.\n\nλ'\n\n2×2 UpperTriangular{Float64, Adjoint{Float64, Matrix{Float64}}}:\n 0.929221  0.0181684\n  ⋅        0.222645\n\n\nThe product \\(\\lambda * \\lambda'\\) will be symmetric. The covariance matrix of the random effects, \\(\\Sigma\\), is this symmetric matrix scaled by \\(\\sigma^2\\)\n\nΣ = m01.σ^2 * λ * λ'\n\n2×2 Matrix{Float64}:\n 565.511  11.057\n  11.057  32.6821\n\n\nThe estimated variances of the random effects, which are the diagonal elements of \\(\\Sigma\\), correspond to the values shown in the table. To evaluate the covariance, isolate the correlation\n\n# m01.σρs extracts the `σρs` property of the model.\n# This property is a NamedTuple where the names\n# correspond to grouping factors - in this case, `subj`.\n# So `m01.σρs.subj.ρ` is the estimated correlation(s) for\n# this grouping factor.  Because there is only one such correlation\n# we can extract it with `only()`, which also verifies that\n# there is exactly one.\nρ = only(m01.σρs.subj.ρ)\n\n0.08133219589180817\n\n\nand multiply by the standard deviations\n\nρ * sqrt(first(Σ) * last(Σ))\n\n11.057017778812778",
    "crumbs": [
      "Visualizations and diagnostics",
      "Convergence, singularity and all that"
    ]
  },
  {
    "objectID": "singularity.html#what-does-it-mean-for-a-converged-model-to-be-singular",
    "href": "singularity.html#what-does-it-mean-for-a-converged-model-to-be-singular",
    "title": "Convergence, singularity and all that",
    "section": "",
    "text": "Add the packages to be used\n\n\nCode\nusing CairoMakie\nusing DataFrames\nusing LinearAlgebra\nusing MixedModels\nusing MixedModelsMakie\nusing ProgressMeter\n\nProgressMeter.ijulia_behavior(:clear)\nCairoMakie.activate!(; type=\"svg\")\n\n\nFit a model for reaction time in the sleepstudy example, preserving information on the estimation progress (the thin=1 optional argument)\n\nm01 = let f = @formula reaction ~ 1 + days + (1 + days|subj)\n  fit(MixedModel, f, MixedModels.dataset(:sleepstudy); thin=1)\nend\nprint(m01)\n\nLinear mixed model fit by maximum likelihood\n reaction ~ 1 + days + (1 + days | subj)\n   logLik   -2 logLik     AIC       AICc        BIC    \n  -875.9697  1751.9393  1763.9393  1764.4249  1783.0971\n\nVariance components:\n            Column    Variance Std.Dev.   Corr.\nsubj     (Intercept)  565.51067 23.78047\n         days          32.68212  5.71683 +0.08\nResidual              654.94145 25.59182\n Number of obs: 180; levels of grouping factors: 18\n\n  Fixed-effects parameters:\n──────────────────────────────────────────────────\n                Coef.  Std. Error      z  Pr(&gt;|z|)\n──────────────────────────────────────────────────\n(Intercept)  251.405      6.63226  37.91    &lt;1e-99\ndays          10.4673     1.50224   6.97    &lt;1e-11\n──────────────────────────────────────────────────\n\n\nThe covariance matrix for each subject’s random effects is evaluated from its “matrix square root”, called the Cholesky factor.\n\nλ = only(m01.λ)\n\n2×2 LowerTriangular{Float64, Matrix{Float64}}:\n 0.929221    ⋅ \n 0.0181684  0.222645\n\n\nThe transpose of \\(\\lambda\\), written \\(\\lambda'\\), is an upper triangular matrix generated by “flipping” \\(\\lambda\\) about the main diagonal.\n\nλ'\n\n2×2 UpperTriangular{Float64, Adjoint{Float64, Matrix{Float64}}}:\n 0.929221  0.0181684\n  ⋅        0.222645\n\n\nThe product \\(\\lambda * \\lambda'\\) will be symmetric. The covariance matrix of the random effects, \\(\\Sigma\\), is this symmetric matrix scaled by \\(\\sigma^2\\)\n\nΣ = m01.σ^2 * λ * λ'\n\n2×2 Matrix{Float64}:\n 565.511  11.057\n  11.057  32.6821\n\n\nThe estimated variances of the random effects, which are the diagonal elements of \\(\\Sigma\\), correspond to the values shown in the table. To evaluate the covariance, isolate the correlation\n\n# m01.σρs extracts the `σρs` property of the model.\n# This property is a NamedTuple where the names\n# correspond to grouping factors - in this case, `subj`.\n# So `m01.σρs.subj.ρ` is the estimated correlation(s) for\n# this grouping factor.  Because there is only one such correlation\n# we can extract it with `only()`, which also verifies that\n# there is exactly one.\nρ = only(m01.σρs.subj.ρ)\n\n0.08133219589180817\n\n\nand multiply by the standard deviations\n\nρ * sqrt(first(Σ) * last(Σ))\n\n11.057017778812778",
    "crumbs": [
      "Visualizations and diagnostics",
      "Convergence, singularity and all that"
    ]
  },
  {
    "objectID": "singularity.html#the-factor-is-generated-from-a-parameter-vector",
    "href": "singularity.html#the-factor-is-generated-from-a-parameter-vector",
    "title": "Convergence, singularity and all that",
    "section": "0.2 The factor is generated from a parameter vector",
    "text": "0.2 The factor is generated from a parameter vector\nIn practice we optimize the log-likelihood with respect to a parameter vector called \\(\\theta\\) that generates \\(\\lambda\\).\n\nm01.θ\n\n3-element Vector{Float64}:\n 0.9292213124664976\n 0.01816838673115054\n 0.22264486480463275\n\n\nThe elements of this parameter vector are subject to constraints. In particular, two of the three elements have a lower bound of zero.\n\nm01.lowerbd\n\n3-element Vector{Float64}:\n   0.0\n -Inf\n   0.0\n\n\nThat is, the first and third elements of \\(\\theta\\), corresponding to diagonal elements of \\(\\lambda\\), must be non-negative, whereas the second component is unconstrained (has a lower bound of \\(-\\infty\\)).",
    "crumbs": [
      "Visualizations and diagnostics",
      "Convergence, singularity and all that"
    ]
  },
  {
    "objectID": "singularity.html#progress-of-the-iterations",
    "href": "singularity.html#progress-of-the-iterations",
    "title": "Convergence, singularity and all that",
    "section": "0.3 Progress of the iterations",
    "text": "0.3 Progress of the iterations\nThe optsum.fitlog property of the model is a vector of tuples where each tuple contains the value of the \\(\\theta\\) vector and the value of the objective at that \\(\\theta\\). The fitlog always contains the first and the last evaluation. When the thin named argument is set, this property has a row for every thin’th evaluation.\n\nm01.optsum.fitlog\n\n57-element Vector{Tuple{Vector{Float64}, Float64}}:\n ([1.0, 0.0, 1.0], 1784.642296192456)\n ([1.75, 0.0, 1.0], 1790.1256369894486)\n ([1.0, 1.0, 1.0], 1798.9996244965819)\n ([1.0, 0.0, 1.75], 1803.8532002844051)\n ([0.25, 0.0, 1.0], 1800.6139807455452)\n ([1.0, -1.0, 1.0], 1798.6046308389245)\n ([1.0, 0.0, 0.25], 1752.2607369909213)\n ([1.1832612965368465, -0.008661887958066326, 0.0], 1797.5876920198457)\n ([1.075, 0.0, 0.32499999999999996], 1754.9541095798602)\n ([0.8166315695343028, 0.01116725445704456, 0.28823768689703677], 1753.695681656805)\n ([1.0, -0.07071067811865475, 0.19696699141100893], 1754.8169985163813)\n ([0.9436827046395643, 0.06383542916407543, 0.2626963029646229], 1753.1067335474727)\n ([0.9801419885633994, -0.026656844944244907, 0.27474275609535526], 1752.939376719002)\n ⋮\n ([0.9289855999666455, 0.01823660250666579, 0.22248440076567516], 1751.93935485824)\n ([0.9286970797848914, 0.018293694810667324, 0.2231753585201247], 1751.939489774018)\n ([0.9282426145734711, 0.01826952230111312, 0.22258371399259966], 1751.9393630914042)\n ([0.9291127489262951, 0.018179125424596244, 0.22262388984119313], 1751.9393448296826)\n ([0.9291905988237738, 0.01816575408997575, 0.22264320543969665], 1751.9393444889993)\n ([0.9292543047658205, 0.018209270421704643, 0.2226208143044214], 1751.9393450338166)\n ([0.929189229695291, 0.01812979185998945, 0.22257323648679392], 1751.9393475283666)\n ([0.9292535809765234, 0.018167625515921226, 0.22264990504608415], 1751.939344490362)\n ([0.9292145006688363, 0.01817173915920612, 0.22264674299586876], 1751.93934447441)\n ([0.9292083468967692, 0.018171477280421077, 0.22264619293728768], 1751.939344475018)\n ([0.9292092940780088, 0.018172966348127092, 0.22265206223369866], 1751.939344506563)\n ([0.9292213124664976, 0.01816838673115054, 0.22264486480463275], 1751.9393444646873)\n\n\nThere were 57 evaluations of the objective before convergence was declared, according to rather stringent convergence criteria. We can see that the last 10 evaluations only produced changes in the fourth decimal place of the objective or even smaller. That is, effective convergence occurred after about 40 or 45 evaluations.",
    "crumbs": [
      "Visualizations and diagnostics",
      "Convergence, singularity and all that"
    ]
  },
  {
    "objectID": "singularity.html#evaluating-the-random-effects-correlation-from-theta",
    "href": "singularity.html#evaluating-the-random-effects-correlation-from-theta",
    "title": "Convergence, singularity and all that",
    "section": "2.1 Evaluating the random effects correlation from \\(\\theta\\)",
    "text": "2.1 Evaluating the random effects correlation from \\(\\theta\\)\nThere is a short-cut for evaluating the correlation which is to “normalize” the second row of \\(\\lambda\\), in the sense that the row is scaled so that it has unit length.\n\nnormed = normalize!(λ[2, :])\n\n2-element Vector{Float64}:\n 0.08133219589180815\n 0.9966870491339879\n\n\nproviding the correlation as\n\nfirst(normed)\n\n0.08133219589180815",
    "crumbs": [
      "Visualizations and diagnostics",
      "Convergence, singularity and all that"
    ]
  },
  {
    "objectID": "singularity.html#optimizing-with-a-fixed-correlation",
    "href": "singularity.html#optimizing-with-a-fixed-correlation",
    "title": "Convergence, singularity and all that",
    "section": "2.2 Optimizing with a fixed correlation",
    "text": "2.2 Optimizing with a fixed correlation\nTo profile the correlation we need optimize the objective while fixing a value of the correlation. The way we will do this is to determine \\(\\theta_2\\) as a function of the fixed \\(\\rho\\) and \\(\\theta_3\\).\nWe need to solve \\[\n\\rho = \\frac{\\theta_2}{\\sqrt{\\theta_2^2 + \\theta_3^2}}\n\\]\nfor \\(\\theta_2\\) as a function of \\(\\rho\\) and \\(\\theta_3\\).\nNotice that \\(\\theta_2\\) and \\(\\rho\\) have the same sign. Thus it is sufficient to determine the absolute value of \\(\\theta_2\\) then transfer the sign from \\(\\rho\\). \\[\n\\theta_2^2=\\rho^2(\\theta_2^2 + \\theta_3^2)\n\\] which implies \\[\n\\theta_2^2 = \\frac{\\rho^2}{1-\\rho^2}\\theta_3^2, \\quad \\theta_3\\ge 0\n\\] and thus \\[\n\\theta_2=\\frac{\\rho}{\\sqrt{1-\\rho^2}}\\theta_3\n\\]",
    "crumbs": [
      "Visualizations and diagnostics",
      "Convergence, singularity and all that"
    ]
  },
  {
    "objectID": "sleepstudy_speed.html",
    "href": "sleepstudy_speed.html",
    "title": "The sleepstudy: Speed - for a change …",
    "section": "",
    "text": "Belenky et al. (2003) reported effects of sleep deprivation across a 14-day study of 30-to-40-year old men and women holding commercial vehicle driving licenses. Their analyses are based on a subset of tasks and ratings from very large and comprehensive test and questionnaire battery (Balkin et al., 2000).\nInitially 66 subjects were assigned to one of four time-in-bed (TIB) groups with 9 hours (22:00-07:00) of sleep augmentation or 7 hours (24:00-07:00), 5 hours (02:00-07:00), and 3 hours (04:00-0:00) of sleep restrictions per night, respectively. The final sample comprised 56 subjects. The Psychomotor Vigilance Test (PVT) measures simple reaction time to a visual stimulus, presented approximately 10 times ⁄ minute (interstimulus interval varied from 2 to 10 s in 2-s increments) for 10 min and implemented in a thumb-operated, hand-held device (Dinges & Powell, 1985).\n\n\nThe study comprised 2 training days (T1, T2), one day with baseline measures (B), seven days with sleep deprivation (E1 to E7), and four recovery days (R1 to R4). T1 and T2 were devoted to training on the performance tests and familiarization with study procedures. PVT baseline testing commenced on the morning of the third day (B) and testing continued for the duration of the study (E1–E7, R1–R3; no measures were taken on R4). Bed times during T, B, and R days were 8 hours (23:00-07:00).\n\n\n\nThe PVT (along with the Stanford Sleepiness Scale) was administered as a battery four times per day (09:00, 12:00, 15:00, and 21:00 h); the battery included other tests not reported here (see Balkin et al., 2000). The sleep latency test was administered at 09:40 and 15:30 h for all groups. Subjects in the 3- and 5-h TIB groups performed an additional battery at 00:00 h and 02:00 h to occupy their additional time awake. The PVT and SSS were administered in this battery; however, as data from the 00:00 and 02:00 h sessions were not common to all TIB groups, these data were not included in the statistical analyses reported in the paper.\n\n\n\nThe authors analyzed response speed, that is (1/RT)*1000 – completely warranted according to a Box-Cox check of the current data – with mixed-model ANOVAs using group as between- and day as within-subject factors. The ANOVA was followed up with simple tests of the design effects implemented over days for each of the four groups.\n\n\n\nThe current data distributed with the RData collection is attributed to the 3-hour TIB group, but the means do not agree at all with those reported for this group in (Belenky et al., 2003, fig. 3) where the 3-hour TIB group is also based on only 13 (not 18) subjects. Specifically, the current data show a much smaller slow-down of response speed across E1 to E7 and do not reflect the recovery during R1 to R3. The current data also cover only 10 not 11 days, but it looks like only R3 is missing. The closest match of the current means was with the average of the 3-hour and 7-hour TIB groups; if only males were included, this would amount to 18 subjects. (This conjecture is based only on visual inspection of graphs.)",
    "crumbs": [
      "Worked examples",
      "The sleepstudy: Speed - for a change ..."
    ]
  },
  {
    "objectID": "sleepstudy_speed.html#design",
    "href": "sleepstudy_speed.html#design",
    "title": "The sleepstudy: Speed - for a change …",
    "section": "",
    "text": "The study comprised 2 training days (T1, T2), one day with baseline measures (B), seven days with sleep deprivation (E1 to E7), and four recovery days (R1 to R4). T1 and T2 were devoted to training on the performance tests and familiarization with study procedures. PVT baseline testing commenced on the morning of the third day (B) and testing continued for the duration of the study (E1–E7, R1–R3; no measures were taken on R4). Bed times during T, B, and R days were 8 hours (23:00-07:00).",
    "crumbs": [
      "Worked examples",
      "The sleepstudy: Speed - for a change ..."
    ]
  },
  {
    "objectID": "sleepstudy_speed.html#test-schedule-within-days",
    "href": "sleepstudy_speed.html#test-schedule-within-days",
    "title": "The sleepstudy: Speed - for a change …",
    "section": "",
    "text": "The PVT (along with the Stanford Sleepiness Scale) was administered as a battery four times per day (09:00, 12:00, 15:00, and 21:00 h); the battery included other tests not reported here (see Balkin et al., 2000). The sleep latency test was administered at 09:40 and 15:30 h for all groups. Subjects in the 3- and 5-h TIB groups performed an additional battery at 00:00 h and 02:00 h to occupy their additional time awake. The PVT and SSS were administered in this battery; however, as data from the 00:00 and 02:00 h sessions were not common to all TIB groups, these data were not included in the statistical analyses reported in the paper.",
    "crumbs": [
      "Worked examples",
      "The sleepstudy: Speed - for a change ..."
    ]
  },
  {
    "objectID": "sleepstudy_speed.html#statistical-analyses",
    "href": "sleepstudy_speed.html#statistical-analyses",
    "title": "The sleepstudy: Speed - for a change …",
    "section": "",
    "text": "The authors analyzed response speed, that is (1/RT)*1000 – completely warranted according to a Box-Cox check of the current data – with mixed-model ANOVAs using group as between- and day as within-subject factors. The ANOVA was followed up with simple tests of the design effects implemented over days for each of the four groups.",
    "crumbs": [
      "Worked examples",
      "The sleepstudy: Speed - for a change ..."
    ]
  },
  {
    "objectID": "sleepstudy_speed.html#current-data",
    "href": "sleepstudy_speed.html#current-data",
    "title": "The sleepstudy: Speed - for a change …",
    "section": "",
    "text": "The current data distributed with the RData collection is attributed to the 3-hour TIB group, but the means do not agree at all with those reported for this group in (Belenky et al., 2003, fig. 3) where the 3-hour TIB group is also based on only 13 (not 18) subjects. Specifically, the current data show a much smaller slow-down of response speed across E1 to E7 and do not reflect the recovery during R1 to R3. The current data also cover only 10 not 11 days, but it looks like only R3 is missing. The closest match of the current means was with the average of the 3-hour and 7-hour TIB groups; if only males were included, this would amount to 18 subjects. (This conjecture is based only on visual inspection of graphs.)",
    "crumbs": [
      "Worked examples",
      "The sleepstudy: Speed - for a change ..."
    ]
  },
  {
    "objectID": "sleepstudy_speed.html#within-subject-simple-regressions",
    "href": "sleepstudy_speed.html#within-subject-simple-regressions",
    "title": "The sleepstudy: Speed - for a change …",
    "section": "5.1 Within-subject simple regressions",
    "text": "5.1 Within-subject simple regressions\nApplying combine to a grouped data frame like gdf produces a DataFrame with a row for each group. The permutation ord provides an ordering for the groups by increasing intercept (predicted response at day 0).\n\nwithin = combine(gdf, [:day, :speed] =&gt; simplelinreg =&gt; :coef)\n\n18×2 DataFrame\n\n\n\nRow\nSubj\ncoef\n\n\n\nString\nTuple…\n\n\n\n\n1\nS308\n(3.94806, -0.194812)\n\n\n2\nS309\n(4.87022, -0.0475185)\n\n\n3\nS310\n(4.90606, -0.120054)\n\n\n4\nS330\n(3.4449, -0.0291309)\n\n\n5\nS331\n(3.47647, -0.0498047)\n\n\n6\nS332\n(3.84436, -0.105511)\n\n\n7\nS333\n(3.60159, -0.0917378)\n\n\n8\nS334\n(4.04528, -0.133527)\n\n\n9\nS335\n(3.80451, 0.0455771)\n\n\n10\nS337\n(3.34374, -0.137744)\n\n\n11\nS349\n(4.46855, -0.170885)\n\n\n12\nS350\n(4.21414, -0.20151)\n\n\n13\nS351\n(3.80469, -0.0728582)\n\n\n14\nS352\n(3.68634, -0.144957)\n\n\n15\nS369\n(3.85384, -0.120531)\n\n\n16\nS370\n(4.52679, -0.215965)\n\n\n17\nS371\n(3.853, -0.0936243)\n\n\n18\nS372\n(3.69208, -0.113292)\n\n\n\n\n\n\nFigure 1 shows the reaction speed versus days of sleep deprivation by subject. The panels are arranged by increasing initial reaction speed starting at the lower left and proceeding across rows.\n\n\nCode\nlet\n  ord = sortperm(first.(within.coef))\n  labs = values(only.(keys(gdf)))[ord]       # labels for panels\n  f = clevelandaxes!(Figure(; resolution=(1000, 750)), labs, (2, 9))\n  for (axs, sdf) in zip(f.content, gdf[ord]) # iterate over the panels and groups\n    scatter!(axs, sdf.day, sdf.speed)      # add the points\n    coef = simplelinreg(sdf.day, sdf.speed)\n    abline!(axs, first(coef), last(coef))  # add the regression line\n  end\n  f\nend\n\n\n┌ Warning: abline! is deprecated and will be removed in the future. Use ablines / ablines! instead.\n│   caller = top-level scope at In[7]:8\n└ @ Core ./In[7]:8\n\n\n\n\n\n\n\n\nFigure 1: Reaction speed (s⁻¹) versus days of sleep deprivation by subject",
    "crumbs": [
      "Worked examples",
      "The sleepstudy: Speed - for a change ..."
    ]
  },
  {
    "objectID": "kb07.html",
    "href": "kb07.html",
    "title": "Bootstrapping a fitted model",
    "section": "",
    "text": "Begin by loading the packages to be used.\n\n\nCode\nusing AlgebraOfGraphics\nusing CairoMakie\nusing DataFrames\nusing MixedModels\nusing ProgressMeter\nusing Random\nusing SMLP2023: dataset\n\nCairoMakie.activate!(; type=\"svg\")\n\nProgressMeter.ijulia_behavior(:clear)\n\n\n\n1 Data set and model\nThe kb07 data (Kronmüller & Barr, 2007) are one of the datasets provided by the MixedModels package.\n\nkb07 = dataset(:kb07)\n\nArrow.Table with 1789 rows, 7 columns, and schema:\n :subj      String\n :item      String\n :spkr      String\n :prec      String\n :load      String\n :rt_trunc  Int16\n :rt_raw    Int16\n\n\nConvert the table to a DataFrame for summary.\n\ndescribe(DataFrame(kb07))\n\n7×7 DataFrame\n\n\n\nRow\nvariable\nmean\nmin\nmedian\nmax\nnmissing\neltype\n\n\n\nSymbol\nUnion…\nAny\nUnion…\nAny\nInt64\nDataType\n\n\n\n\n1\nsubj\n\nS030\n\nS103\n0\nString\n\n\n2\nitem\n\nI01\n\nI32\n0\nString\n\n\n3\nspkr\n\nnew\n\nold\n0\nString\n\n\n4\nprec\n\nbreak\n\nmaintain\n0\nString\n\n\n5\nload\n\nno\n\nyes\n0\nString\n\n\n6\nrt_trunc\n2182.2\n579\n1940.0\n5171\n0\nInt16\n\n\n7\nrt_raw\n2226.24\n579\n1940.0\n15923\n0\nInt16\n\n\n\n\n\n\nThe experimental factors; spkr, prec, and load, are two-level factors. The EffectsCoding contrast is used with these to create a \\(\\pm1\\) encoding. Furthermore, Grouping contrasts are assigned to the subj and item factors. This is not a contrast per-se but an indication that these factors will be used as grouping factors for random effects and, therefore, there is no need to create a contrast matrix. For large numbers of levels in a grouping factor, an attempt to create a contrast matrix may cause memory overflow.\nIt is not important in these cases but a good practice in any case.\n\ncontrasts = merge(\n  Dict(nm =&gt; EffectsCoding() for nm in (:spkr, :prec, :load)),\n  Dict(nm =&gt; Grouping() for nm in (:subj, :item)),\n)\n\nThe display of an initial model fit\n\nkbm01 = let\n  form = @formula(\n    rt_trunc ~\n      1 +\n      spkr * prec * load +\n      (1 + spkr + prec + load | subj) +\n      (1 + spkr + prec + load | item)\n  )\n  fit(MixedModel, form, kb07; contrasts)\nend\n\nMinimizing 874    Time: 0:00:01 ( 1.38 ms/it)\n  objective:  28637.971010073215\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEst.\nSE\nz\np\nσ_subj\nσ_item\n\n\n\n\n(Intercept)\n2181.6424\n77.3515\n28.20\n&lt;1e-99\n301.8721\n362.4695\n\n\nspkr: old\n67.7496\n17.9607\n3.77\n0.0002\n33.0588\n41.1159\n\n\nprec: maintain\n-333.9200\n47.1563\n-7.08\n&lt;1e-11\n58.8512\n247.3299\n\n\nload: yes\n78.8007\n19.7269\n3.99\n&lt;1e-04\n66.9526\n43.3991\n\n\nspkr: old & prec: maintain\n-21.9960\n15.8191\n-1.39\n0.1644\n\n\n\n\nspkr: old & load: yes\n18.3832\n15.8191\n1.16\n0.2452\n\n\n\n\nprec: maintain & load: yes\n4.5327\n15.8191\n0.29\n0.7745\n\n\n\n\nspkr: old & prec: maintain & load: yes\n23.6377\n15.8191\n1.49\n0.1351\n\n\n\n\nResidual\n669.0515\n\n\n\n\n\n\n\n\n\n\ndoes not include the estimated correlations of the random effects.\nThe VarCorr extractor displays these.\n\nVarCorr(kbm01)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nColumn\nVariance\nStd.Dev\nCorr.\n\n\n\n\n\n\nsubj\n(Intercept)\n91126.7545\n301.8721\n\n\n\n\n\n\nspkr: old\n1092.8862\n33.0588\n+1.00\n\n\n\n\n\nprec: maintain\n3463.4603\n58.8512\n-0.62\n-0.62\n\n\n\n\nload: yes\n4482.6493\n66.9526\n+0.36\n+0.36\n+0.51\n\n\nitem\n(Intercept)\n131384.1084\n362.4695\n\n\n\n\n\n\nspkr: old\n1690.5210\n41.1159\n+0.42\n\n\n\n\n\nprec: maintain\n61172.0781\n247.3299\n-0.69\n+0.37\n\n\n\n\nload: yes\n1883.4785\n43.3991\n+0.29\n+0.14\n-0.13\n\n\nResidual\n\n447629.9270\n669.0515\n\n\n\n\n\n\n\n\nNone of the two-factor or three-factor interaction terms in the fixed-effects are significant. In the random-effects terms only the scalar random effects and the prec random effect for item appear to be warranted, leading to the reduced formula\n\nkbm02 = let\n  form = @formula(\n    rt_trunc ~\n      1 + spkr + prec + load + (1 | subj) + (1 + prec | item)\n  )\n  fit(MixedModel, form, kb07; contrasts)\nend\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEst.\nSE\nz\np\nσ_item\nσ_subj\n\n\n\n\n(Intercept)\n2181.8526\n77.4681\n28.16\n&lt;1e-99\n364.7125\n298.0259\n\n\nspkr: old\n67.8790\n16.0785\n4.22\n&lt;1e-04\n\n\n\n\nprec: maintain\n-333.7906\n47.4472\n-7.03\n&lt;1e-11\n252.5212\n\n\n\nload: yes\n78.5904\n16.0785\n4.89\n&lt;1e-05\n\n\n\n\nResidual\n680.0319\n\n\n\n\n\n\n\n\n\n\n\nVarCorr(kbm02)\n\n\n\n\n\nColumn\nVariance\nStd.Dev\nCorr.\n\n\n\n\nitem\n(Intercept)\n133015.244\n364.713\n\n\n\n\nprec: maintain\n63766.937\n252.521\n-0.70\n\n\nsubj\n(Intercept)\n88819.437\n298.026\n\n\n\nResidual\n\n462443.388\n680.032\n\n\n\n\n\n\nThese two models are nested and can be compared with a likelihood-ratio test.\n\nMixedModels.likelihoodratiotest(kbm02, kbm01)\n\n\n\n\n\n\n\n\n\n\n\n\n\nmodel-dof\ndeviance\nχ²\nχ²-dof\nP(&gt;χ²)\n\n\n\n\nrt_trunc ~ 1 + spkr + prec + load + (1 | subj) + (1 + prec | item)\n9\n28664\n\n\n\n\n\nrt_trunc ~ 1 + spkr + prec + load + spkr & prec + spkr & load + prec & load + spkr & prec & load + (1 + spkr + prec + load | subj) + (1 + spkr + prec + load | item)\n29\n28638\n26\n20\n0.1698\n\n\n\n\n\nThe p-value of approximately 17% leads us to prefer the simpler model, kbm02, to the more complex, kbm01.\n\n\n2 A bootstrap sample\nCreate a bootstrap sample of a few thousand parameter estimates from the reduced model. The pseudo-random number generator is initialized to a fixed value for reproducibility.\n\nRandom.seed!(1234321)\nkbm02samp = parametricbootstrap(2000, kbm02)\nkbm02tbl = kbm02samp.tbl\n\nTable with 14 columns and 2000 rows:\n      obj      β1       β2       β3        β4       σ        σ1       σ2       ⋯\n    ┌───────────────────────────────────────────────────────────────────────────\n 1  │ 28769.9  2152.69  60.3921  -359.134  54.1231  702.318  421.681  212.916  ⋯\n 2  │ 28775.3  2221.81  61.7318  -349.417  71.1992  697.028  442.812  278.931  ⋯\n 3  │ 28573.6  2325.31  53.9668  -404.225  81.3407  657.746  364.354  274.392  ⋯\n 4  │ 28682.9  2274.65  52.9438  -384.66   69.6788  682.801  398.901  319.725  ⋯\n 5  │ 28616.1  2191.34  54.7785  -357.919  111.829  673.878  253.244  235.912  ⋯\n 6  │ 28634.1  2176.73  95.9793  -378.383  82.1559  671.685  510.996  309.876  ⋯\n 7  │ 28620.4  2128.07  57.6928  -267.747  60.4621  666.986  396.878  295.009  ⋯\n 8  │ 28665.9  2152.59  55.8208  -274.382  76.5128  684.746  369.091  274.446  ⋯\n 9  │ 28651.2  2088.3   76.4636  -296.14   95.6124  680.282  265.684  186.041  ⋯\n 10 │ 28679.9  2375.23  72.2422  -423.922  94.0833  680.729  425.72   287.686  ⋯\n 11 │ 28686.8  2034.11  59.9149  -224.859  59.5448  682.433  308.677  272.183  ⋯\n 12 │ 28702.2  2130.98  83.3953  -271.61   79.974   689.679  401.98   264.258  ⋯\n 13 │ 28625.0  2055.35  55.3526  -363.937  67.7128  676.817  309.149  265.788  ⋯\n 14 │ 28570.3  2160.73  81.5575  -321.913  108.979  661.525  426.862  259.819  ⋯\n 15 │ 28613.6  2237.73  36.9996  -362.443  82.1812  673.873  312.859  241.592  ⋯\n 16 │ 28681.4  2306.64  43.8801  -429.983  103.082  686.437  360.536  204.514  ⋯\n 17 │ 28577.2  2143.78  66.2959  -373.598  93.157   668.15   321.469  183.119  ⋯\n 18 │ 28616.1  2197.03  55.9111  -356.06   74.1476  675.709  302.678  212.917  ⋯\n 19 │ 28771.6  2067.71  56.5032  -324.984  64.804   699.136  344.776  225.602  ⋯\n 20 │ 28674.3  2181.99  40.6566  -251.315  63.1301  683.81   340.569  295.054  ⋯\n 21 │ 28677.6  2129.89  72.7198  -245.006  84.7693  681.415  359.093  287.654  ⋯\n 22 │ 28582.1  2179.8   74.8364  -405.798  68.2978  664.168  357.696  231.402  ⋯\n 23 │ 28714.9  2326.34  74.7091  -400.441  67.79    692.381  354.032  259.052  ⋯\n ⋮  │    ⋮        ⋮        ⋮        ⋮         ⋮        ⋮        ⋮        ⋮     ⋱\n\n\nOne of the uses of such a sample is to form “confidence intervals” on the parameters by obtaining the shortest interval that covers a given proportion (95%, by default) of the sample.\n\nconfint(kbm02samp)\n\nDictTable with 2 columns and 9 rows:\n par   lower      upper\n ────┬─────────────────────\n β1  │ 2028.01    2337.92\n β2  │ 38.431     99.5944\n β3  │ -439.321   -245.864\n β4  │ 46.0262    107.511\n ρ1  │ -0.897984  -0.445605\n σ   │ 655.25     701.497\n σ1  │ 261.196    448.51\n σ2  │ 175.489    312.045\n σ3  │ 228.099    357.789\n\n\nA sample like this can be used for more than just creating an interval because it approximates the distribution of the estimator. For the fixed-effects parameters the estimators are close to being normally distributed, Figure 1.\n\n\nCode\ndraw(\n  data(kbm02samp.β) * mapping(:β; color=:coefname) * AlgebraOfGraphics.density();\n  figure=(; resolution=(800, 450)),\n)\n\n\n\n\n\n\n\n\nFigure 1: Comparative densities of the fixed-effects coefficients in kbm02samp\n\n\n\n\n\n\n\nCode\nlet pars = [\"σ1\", \"σ2\", \"σ3\"]\n  draw(\n    data(kbm02tbl) *\n    mapping(pars .=&gt; \"σ\"; color=dims(1) =&gt; renamer(pars)) *\n    AlgebraOfGraphics.density();\n    figure=(; resolution=(800, 450)),\n  )\nend\n\n\n\n\n\n\n\n\nFigure 2: Density plot of bootstrap samples standard deviation of random effects\n\n\n\n\n\n\n\nCode\ndraw(\n  data(kbm02tbl) *\n  mapping(:ρ1 =&gt; \"Correlation\") *\n  AlgebraOfGraphics.density();\n  figure=(; resolution=(800, 450)),\n)\n\n\n\n\n\n\n\n\nFigure 3: Density plot of correlation parameters in bootstrap sample from model kbm02\n\n\n\n\n\n\n\n3 References\n\n\nKronmüller, E., & Barr, D. J. (2007). Perspective-free pragmatics: Broken precedents and the recovery-from-preemption hypothesis. Journal of Memory and Language, 56(3), 436–455. https://doi.org/10.1016/j.jml.2006.05.002\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Bootstrap and profiling",
      "Bootstrapping a fitted model"
    ]
  },
  {
    "objectID": "arrow.html",
    "href": "arrow.html",
    "title": "Saving data with Arrow",
    "section": "",
    "text": "1 The Arrow storage format\nThe Arrow storage format provides a language-agnostic storage and memory specification for columnar data tables, which just means “something that looks like a data frame in R”. That is, an arrow table is an ordered, named collection of columns, all of the same length.\nThe columns can be of different types including numeric values, character strings, and factor-like representations - called DictEncoded.\nAn Arrow file can be read or written from R, Python, Julia and many other languages. Somewhat confusingly in R and Python the name feather, which refers to an earlier version of the storage format, is used in some function names like read_feather.\nInternally, the SMLP2023 package uses Arrow to store all of its datasets.\n\n\n2 The Emotikon data\nThe SMLP2021 repository contains a version of the data from Fühner et al. (2021) in notebooks/data/fggk21.arrow. After that file was created there were changes in the master RDS file on the osf.io site for the project. We will recreate the Arrow file here then split it into two separate tables, one with a row for each child in the study and one with a row for each test result.\nThe Arrow package for Julia does not export any function names, which means that the function to read an Arrow file must be called as Arrow.Table. It returns a column table, as described in the Tables package. This is like a read-only data frame, which can be easily converted to a full-fledged DataFrame if desired.\nThis arrangement allows for the Arrow package not to depend on the DataFrames package, which is a heavy-weight dependency, but still easily produce a DataFrame if warranted.\nLoad the packages to be used.\n\n\nCode\nusing AlgebraOfGraphics\nusing Arrow\nusing CairoMakie\nusing Chain\nusing DataFrameMacros\nusing DataFrames\nusing Downloads\nusing KernelDensity\nusing RCall   # access R from within Julia\nusing StatsBase\n\nCairoMakie.activate!(; type=\"svg\")\nusing AlgebraOfGraphics: density\n\n\n\n\n3 Downloading and importing the RDS file\nThis is similar to some of the code shown by Julius Krumbiegel on Monday. In the data directory of the emotikon project on osf.io under Data, the url for the rds data file is found to be [https://osf.io/xawdb/]. Note that we want version 2 of this file.\n\nfn = Downloads.download(\"https://osf.io/xawdb/download?version=2\");\n\n\ndfrm = rcopy(R\"readRDS($fn)\")\n\n525126×7 DataFrame525101 rows omitted\n\n\n\nRow\nCohort\nSchool\nChild\nSex\nage\nTest\nscore\n\n\n\nCat…\nCat…\nCat…\nCat…\nFloat64\nCat…\nFloat64\n\n\n\n\n1\n2013\nS100067\nC002352\nmale\n7.99452\nS20_r\n5.26316\n\n\n2\n2013\nS100067\nC002352\nmale\n7.99452\nBPT\n3.7\n\n\n3\n2013\nS100067\nC002352\nmale\n7.99452\nSLJ\n125.0\n\n\n4\n2013\nS100067\nC002352\nmale\n7.99452\nStar_r\n2.47146\n\n\n5\n2013\nS100067\nC002352\nmale\n7.99452\nRun\n1053.0\n\n\n6\n2013\nS100067\nC002353\nmale\n7.99452\nS20_r\n5.0\n\n\n7\n2013\nS100067\nC002353\nmale\n7.99452\nBPT\n4.1\n\n\n8\n2013\nS100067\nC002353\nmale\n7.99452\nSLJ\n116.0\n\n\n9\n2013\nS100067\nC002353\nmale\n7.99452\nStar_r\n1.76778\n\n\n10\n2013\nS100067\nC002353\nmale\n7.99452\nRun\n1089.0\n\n\n11\n2013\nS100067\nC002354\nmale\n7.99452\nS20_r\n4.54545\n\n\n12\n2013\nS100067\nC002354\nmale\n7.99452\nBPT\n3.9\n\n\n13\n2013\nS100067\nC002354\nmale\n7.99452\nSLJ\n111.0\n\n\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n\n\n525115\n2018\nS401470\nC117964\nmale\n9.10609\nStar_r\n1.63704\n\n\n525116\n2018\nS401470\nC117964\nmale\n9.10609\nRun\n864.0\n\n\n525117\n2018\nS401470\nC117965\nfemale\n9.10609\nS20_r\n4.65116\n\n\n525118\n2018\nS401470\nC117965\nfemale\n9.10609\nBPT\n3.8\n\n\n525119\n2018\nS401470\nC117965\nfemale\n9.10609\nSLJ\n123.0\n\n\n525120\n2018\nS401470\nC117965\nfemale\n9.10609\nStar_r\n1.52889\n\n\n525121\n2018\nS401470\nC117965\nfemale\n9.10609\nRun\n1080.0\n\n\n525122\n2018\nS800200\nC117966\nmale\n9.10609\nS20_r\n4.54545\n\n\n525123\n2018\nS800200\nC117966\nmale\n9.10609\nBPT\n3.8\n\n\n525124\n2018\nS800200\nC117966\nmale\n9.10609\nSLJ\n100.0\n\n\n525125\n2018\nS800200\nC117966\nmale\n9.10609\nStar_r\n2.18506\n\n\n525126\n2018\nS800200\nC117966\nmale\n9.10609\nRun\n990.0\n\n\n\n\n\n\nNow write this file as a Arrow file and read it back in.\n\narrowfn = joinpath(\"data\", \"fggk21.arrow\")\nArrow.write(arrowfn, dfrm; compress=:lz4)\ntbl = Arrow.Table(arrowfn)\n\nArrow.Table with 525126 rows, 7 columns, and schema:\n :Cohort  String\n :School  String\n :Child   String\n :Sex     String\n :age     Float64\n :Test    String\n :score   Float64\n\n\n\nfilesize(arrowfn)\n\n3077850\n\n\n\ndf = DataFrame(tbl)\n\n525126×7 DataFrame525101 rows omitted\n\n\n\nRow\nCohort\nSchool\nChild\nSex\nage\nTest\nscore\n\n\n\nString\nString\nString\nString\nFloat64\nString\nFloat64\n\n\n\n\n1\n2013\nS100067\nC002352\nmale\n7.99452\nS20_r\n5.26316\n\n\n2\n2013\nS100067\nC002352\nmale\n7.99452\nBPT\n3.7\n\n\n3\n2013\nS100067\nC002352\nmale\n7.99452\nSLJ\n125.0\n\n\n4\n2013\nS100067\nC002352\nmale\n7.99452\nStar_r\n2.47146\n\n\n5\n2013\nS100067\nC002352\nmale\n7.99452\nRun\n1053.0\n\n\n6\n2013\nS100067\nC002353\nmale\n7.99452\nS20_r\n5.0\n\n\n7\n2013\nS100067\nC002353\nmale\n7.99452\nBPT\n4.1\n\n\n8\n2013\nS100067\nC002353\nmale\n7.99452\nSLJ\n116.0\n\n\n9\n2013\nS100067\nC002353\nmale\n7.99452\nStar_r\n1.76778\n\n\n10\n2013\nS100067\nC002353\nmale\n7.99452\nRun\n1089.0\n\n\n11\n2013\nS100067\nC002354\nmale\n7.99452\nS20_r\n4.54545\n\n\n12\n2013\nS100067\nC002354\nmale\n7.99452\nBPT\n3.9\n\n\n13\n2013\nS100067\nC002354\nmale\n7.99452\nSLJ\n111.0\n\n\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n\n\n525115\n2018\nS401470\nC117964\nmale\n9.10609\nStar_r\n1.63704\n\n\n525116\n2018\nS401470\nC117964\nmale\n9.10609\nRun\n864.0\n\n\n525117\n2018\nS401470\nC117965\nfemale\n9.10609\nS20_r\n4.65116\n\n\n525118\n2018\nS401470\nC117965\nfemale\n9.10609\nBPT\n3.8\n\n\n525119\n2018\nS401470\nC117965\nfemale\n9.10609\nSLJ\n123.0\n\n\n525120\n2018\nS401470\nC117965\nfemale\n9.10609\nStar_r\n1.52889\n\n\n525121\n2018\nS401470\nC117965\nfemale\n9.10609\nRun\n1080.0\n\n\n525122\n2018\nS800200\nC117966\nmale\n9.10609\nS20_r\n4.54545\n\n\n525123\n2018\nS800200\nC117966\nmale\n9.10609\nBPT\n3.8\n\n\n525124\n2018\nS800200\nC117966\nmale\n9.10609\nSLJ\n100.0\n\n\n525125\n2018\nS800200\nC117966\nmale\n9.10609\nStar_r\n2.18506\n\n\n525126\n2018\nS800200\nC117966\nmale\n9.10609\nRun\n990.0\n\n\n\n\n\n\n\n\n4 Avoiding needless repetition\nOne of the principles of relational database design is that information should not be repeated needlessly. Each row of df is determined by a combination of Child and Test, together producing a score, which can be converted to a zScore.\nThe other columns in the table, Cohort, School, age, and Sex, are properties of the Child.\nStoring these values redundantly in the full table takes up space but, more importantly, allows for inconsistency. As it stands, a given Child could be recorded as being in one Cohort for the Run test and in another Cohort for the S20_r test and nothing about the table would detect this as being an error.\nThe approach used in relational databases is to store the information for score in one table that contains only Child, Test and score, store the information for the Child in another table including Cohort, School, age and Sex. These tables can then be combined to create the table to be used for analysis by joining the different tables together.\nThe maintainers of the DataFrames package have put in a lot of work over the past few years to make joins quite efficient in Julia. Thus the processing penalty of reassembling the big table from three smaller tables is minimal.\nIt is important to note that the main advantage of using smaller tables that are joined together to produce the analysis table is the fact that the information in the analysis table is consistent by design.\n\n\n5 Creating the smaller table\n\nChild = unique(select(df, :Child, :School, :Cohort, :Sex, :age))\n\n108295×5 DataFrame108270 rows omitted\n\n\n\nRow\nChild\nSchool\nCohort\nSex\nage\n\n\n\nString\nString\nString\nString\nFloat64\n\n\n\n\n1\nC002352\nS100067\n2013\nmale\n7.99452\n\n\n2\nC002353\nS100067\n2013\nmale\n7.99452\n\n\n3\nC002354\nS100067\n2013\nmale\n7.99452\n\n\n4\nC002355\nS100122\n2013\nfemale\n7.99452\n\n\n5\nC002356\nS100146\n2013\nmale\n7.99452\n\n\n6\nC002357\nS100146\n2013\nmale\n7.99452\n\n\n7\nC002358\nS100146\n2013\nmale\n7.99452\n\n\n8\nC002359\nS100183\n2013\nfemale\n7.99452\n\n\n9\nC002360\nS100195\n2013\nfemale\n7.99452\n\n\n10\nC002361\nS100213\n2013\nmale\n7.99452\n\n\n11\nC002362\nS100237\n2013\nfemale\n7.99452\n\n\n12\nC002363\nS100237\n2013\nfemale\n7.99452\n\n\n13\nC002364\nS100250\n2013\nfemale\n7.99452\n\n\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n\n\n108284\nC117943\nS130539\n2018\nfemale\n9.10609\n\n\n108285\nC117944\nS130539\n2018\nmale\n9.10609\n\n\n108286\nC117945\nS130539\n2018\nmale\n9.10609\n\n\n108287\nC117946\nS130539\n2018\nmale\n9.10609\n\n\n108288\nC117956\nS400580\n2018\nmale\n9.10609\n\n\n108289\nC117957\nS400919\n2018\nmale\n9.10609\n\n\n108290\nC117958\nS400919\n2018\nmale\n9.10609\n\n\n108291\nC117959\nS400919\n2018\nmale\n9.10609\n\n\n108292\nC117962\nS401250\n2018\nfemale\n9.10609\n\n\n108293\nC117964\nS401470\n2018\nmale\n9.10609\n\n\n108294\nC117965\nS401470\n2018\nfemale\n9.10609\n\n\n108295\nC117966\nS800200\n2018\nmale\n9.10609\n\n\n\n\n\n\n\nlength(unique(Child.Child))  # should be 108295\n\n108295\n\n\n\nfilesize(\n  Arrow.write(\"./data/fggk21_Child.arrow\", Child; compress=:lz4)\n)\n\n1774946\n\n\n\nfilesize(\n  Arrow.write(\n    \"./data/fggk21_Score.arrow\",\n    select(df, :Child, :Test, :score);\n    compress=:lz4,\n  ),\n)\n\n2794058\n\n\n\n\n\n\n\n\nNote\n\n\n\nA careful examination of the file sizes versus that of ./data/fggk21.arrow will show that the separate tables combined take up more space than the original because of the compression. Compression algorithms are often more successful when applied to larger files.\n\n\nNow read the Arrow tables in and reassemble the original table.\n\nScore = DataFrame(Arrow.Table(\"./data/fggk21_Score.arrow\"))\n\n525126×3 DataFrame525101 rows omitted\n\n\n\nRow\nChild\nTest\nscore\n\n\n\nString\nString\nFloat64\n\n\n\n\n1\nC002352\nS20_r\n5.26316\n\n\n2\nC002352\nBPT\n3.7\n\n\n3\nC002352\nSLJ\n125.0\n\n\n4\nC002352\nStar_r\n2.47146\n\n\n5\nC002352\nRun\n1053.0\n\n\n6\nC002353\nS20_r\n5.0\n\n\n7\nC002353\nBPT\n4.1\n\n\n8\nC002353\nSLJ\n116.0\n\n\n9\nC002353\nStar_r\n1.76778\n\n\n10\nC002353\nRun\n1089.0\n\n\n11\nC002354\nS20_r\n4.54545\n\n\n12\nC002354\nBPT\n3.9\n\n\n13\nC002354\nSLJ\n111.0\n\n\n⋮\n⋮\n⋮\n⋮\n\n\n525115\nC117964\nStar_r\n1.63704\n\n\n525116\nC117964\nRun\n864.0\n\n\n525117\nC117965\nS20_r\n4.65116\n\n\n525118\nC117965\nBPT\n3.8\n\n\n525119\nC117965\nSLJ\n123.0\n\n\n525120\nC117965\nStar_r\n1.52889\n\n\n525121\nC117965\nRun\n1080.0\n\n\n525122\nC117966\nS20_r\n4.54545\n\n\n525123\nC117966\nBPT\n3.8\n\n\n525124\nC117966\nSLJ\n100.0\n\n\n525125\nC117966\nStar_r\n2.18506\n\n\n525126\nC117966\nRun\n990.0\n\n\n\n\n\n\nAt this point we can create the z-score column by standardizing the scores for each Test. The code to do this follows Julius’s presentation on Monday.\n\n@transform!(groupby(Score, :Test), :zScore = @bycol zscore(:score))\n\n525126×4 DataFrame525101 rows omitted\n\n\n\nRow\nChild\nTest\nscore\nzScore\n\n\n\nString\nString\nFloat64\nFloat64\n\n\n\n\n1\nC002352\nS20_r\n5.26316\n1.7913\n\n\n2\nC002352\nBPT\n3.7\n-0.0622317\n\n\n3\nC002352\nSLJ\n125.0\n-0.0336567\n\n\n4\nC002352\nStar_r\n2.47146\n1.46874\n\n\n5\nC002352\nRun\n1053.0\n0.331058\n\n\n6\nC002353\nS20_r\n5.0\n1.15471\n\n\n7\nC002353\nBPT\n4.1\n0.498354\n\n\n8\nC002353\nSLJ\n116.0\n-0.498822\n\n\n9\nC002353\nStar_r\n1.76778\n-0.9773\n\n\n10\nC002353\nRun\n1089.0\n0.574056\n\n\n11\nC002354\nS20_r\n4.54545\n0.0551481\n\n\n12\nC002354\nBPT\n3.9\n0.218061\n\n\n13\nC002354\nSLJ\n111.0\n-0.757248\n\n\n⋮\n⋮\n⋮\n⋮\n⋮\n\n\n525115\nC117964\nStar_r\n1.63704\n-1.43175\n\n\n525116\nC117964\nRun\n864.0\n-0.944681\n\n\n525117\nC117965\nS20_r\n4.65116\n0.31086\n\n\n525118\nC117965\nBPT\n3.8\n0.0779146\n\n\n525119\nC117965\nSLJ\n123.0\n-0.137027\n\n\n525120\nC117965\nStar_r\n1.52889\n-1.8077\n\n\n525121\nC117965\nRun\n1080.0\n0.513306\n\n\n525122\nC117966\nS20_r\n4.54545\n0.0551481\n\n\n525123\nC117966\nBPT\n3.8\n0.0779146\n\n\n525124\nC117966\nSLJ\n100.0\n-1.32578\n\n\n525125\nC117966\nStar_r\n2.18506\n0.473217\n\n\n525126\nC117966\nRun\n990.0\n-0.0941883\n\n\n\n\n\n\n\nChild = DataFrame(Arrow.Table(\"./data/fggk21_Child.arrow\"))\n\n108295×5 DataFrame108270 rows omitted\n\n\n\nRow\nChild\nSchool\nCohort\nSex\nage\n\n\n\nString\nString\nString\nString\nFloat64\n\n\n\n\n1\nC002352\nS100067\n2013\nmale\n7.99452\n\n\n2\nC002353\nS100067\n2013\nmale\n7.99452\n\n\n3\nC002354\nS100067\n2013\nmale\n7.99452\n\n\n4\nC002355\nS100122\n2013\nfemale\n7.99452\n\n\n5\nC002356\nS100146\n2013\nmale\n7.99452\n\n\n6\nC002357\nS100146\n2013\nmale\n7.99452\n\n\n7\nC002358\nS100146\n2013\nmale\n7.99452\n\n\n8\nC002359\nS100183\n2013\nfemale\n7.99452\n\n\n9\nC002360\nS100195\n2013\nfemale\n7.99452\n\n\n10\nC002361\nS100213\n2013\nmale\n7.99452\n\n\n11\nC002362\nS100237\n2013\nfemale\n7.99452\n\n\n12\nC002363\nS100237\n2013\nfemale\n7.99452\n\n\n13\nC002364\nS100250\n2013\nfemale\n7.99452\n\n\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n\n\n108284\nC117943\nS130539\n2018\nfemale\n9.10609\n\n\n108285\nC117944\nS130539\n2018\nmale\n9.10609\n\n\n108286\nC117945\nS130539\n2018\nmale\n9.10609\n\n\n108287\nC117946\nS130539\n2018\nmale\n9.10609\n\n\n108288\nC117956\nS400580\n2018\nmale\n9.10609\n\n\n108289\nC117957\nS400919\n2018\nmale\n9.10609\n\n\n108290\nC117958\nS400919\n2018\nmale\n9.10609\n\n\n108291\nC117959\nS400919\n2018\nmale\n9.10609\n\n\n108292\nC117962\nS401250\n2018\nfemale\n9.10609\n\n\n108293\nC117964\nS401470\n2018\nmale\n9.10609\n\n\n108294\nC117965\nS401470\n2018\nfemale\n9.10609\n\n\n108295\nC117966\nS800200\n2018\nmale\n9.10609\n\n\n\n\n\n\n\ndf1 = disallowmissing!(leftjoin(Score, Child; on=:Child))\n\n525126×8 DataFrame525101 rows omitted\n\n\n\nRow\nChild\nTest\nscore\nzScore\nSchool\nCohort\nSex\nage\n\n\n\nString\nString\nFloat64\nFloat64\nString\nString\nString\nFloat64\n\n\n\n\n1\nC002352\nS20_r\n5.26316\n1.7913\nS100067\n2013\nmale\n7.99452\n\n\n2\nC002352\nBPT\n3.7\n-0.0622317\nS100067\n2013\nmale\n7.99452\n\n\n3\nC002352\nSLJ\n125.0\n-0.0336567\nS100067\n2013\nmale\n7.99452\n\n\n4\nC002352\nStar_r\n2.47146\n1.46874\nS100067\n2013\nmale\n7.99452\n\n\n5\nC002352\nRun\n1053.0\n0.331058\nS100067\n2013\nmale\n7.99452\n\n\n6\nC002353\nS20_r\n5.0\n1.15471\nS100067\n2013\nmale\n7.99452\n\n\n7\nC002353\nBPT\n4.1\n0.498354\nS100067\n2013\nmale\n7.99452\n\n\n8\nC002353\nSLJ\n116.0\n-0.498822\nS100067\n2013\nmale\n7.99452\n\n\n9\nC002353\nStar_r\n1.76778\n-0.9773\nS100067\n2013\nmale\n7.99452\n\n\n10\nC002353\nRun\n1089.0\n0.574056\nS100067\n2013\nmale\n7.99452\n\n\n11\nC002354\nS20_r\n4.54545\n0.0551481\nS100067\n2013\nmale\n7.99452\n\n\n12\nC002354\nBPT\n3.9\n0.218061\nS100067\n2013\nmale\n7.99452\n\n\n13\nC002354\nSLJ\n111.0\n-0.757248\nS100067\n2013\nmale\n7.99452\n\n\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n\n\n525115\nC117964\nStar_r\n1.63704\n-1.43175\nS401470\n2018\nmale\n9.10609\n\n\n525116\nC117964\nRun\n864.0\n-0.944681\nS401470\n2018\nmale\n9.10609\n\n\n525117\nC117965\nS20_r\n4.65116\n0.31086\nS401470\n2018\nfemale\n9.10609\n\n\n525118\nC117965\nBPT\n3.8\n0.0779146\nS401470\n2018\nfemale\n9.10609\n\n\n525119\nC117965\nSLJ\n123.0\n-0.137027\nS401470\n2018\nfemale\n9.10609\n\n\n525120\nC117965\nStar_r\n1.52889\n-1.8077\nS401470\n2018\nfemale\n9.10609\n\n\n525121\nC117965\nRun\n1080.0\n0.513306\nS401470\n2018\nfemale\n9.10609\n\n\n525122\nC117966\nS20_r\n4.54545\n0.0551481\nS800200\n2018\nmale\n9.10609\n\n\n525123\nC117966\nBPT\n3.8\n0.0779146\nS800200\n2018\nmale\n9.10609\n\n\n525124\nC117966\nSLJ\n100.0\n-1.32578\nS800200\n2018\nmale\n9.10609\n\n\n525125\nC117966\nStar_r\n2.18506\n0.473217\nS800200\n2018\nmale\n9.10609\n\n\n525126\nC117966\nRun\n990.0\n-0.0941883\nS800200\n2018\nmale\n9.10609\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe call to disallowmissing! is because the join will create columns that allow for missing values but we know that we should not get missing values in the result. This call will fail if, for some reason, missing values were created.\n\n\n\n\n6 Discovering patterns in the data\nOne of the motivations for creating the Child table was to be able to bin the ages according to the age of each child, not the age of each Child-Test combination. Not all children have all 5 test results. We can check the number of results by grouping on :Child and evaluate the number of rows in each group.\n\nnobsChild = combine(groupby(Score, :Child), nrow =&gt; :ntest)\n\n108295×2 DataFrame108270 rows omitted\n\n\n\nRow\nChild\nntest\n\n\n\nString\nInt64\n\n\n\n\n1\nC002352\n5\n\n\n2\nC002353\n5\n\n\n3\nC002354\n5\n\n\n4\nC002355\n5\n\n\n5\nC002356\n5\n\n\n6\nC002357\n5\n\n\n7\nC002358\n5\n\n\n8\nC002359\n4\n\n\n9\nC002360\n5\n\n\n10\nC002361\n4\n\n\n11\nC002362\n5\n\n\n12\nC002363\n5\n\n\n13\nC002364\n5\n\n\n⋮\n⋮\n⋮\n\n\n108284\nC117943\n5\n\n\n108285\nC117944\n5\n\n\n108286\nC117945\n5\n\n\n108287\nC117946\n5\n\n\n108288\nC117956\n5\n\n\n108289\nC117957\n4\n\n\n108290\nC117958\n5\n\n\n108291\nC117959\n5\n\n\n108292\nC117962\n5\n\n\n108293\nC117964\n5\n\n\n108294\nC117965\n5\n\n\n108295\nC117966\n5\n\n\n\n\n\n\nNow create a table of the number of children with 1, 2, …, 5 test scores.\n\ncombine(groupby(nobsChild, :ntest), nrow)\n\n5×2 DataFrame\n\n\n\nRow\nntest\nnrow\n\n\n\nInt64\nInt64\n\n\n\n\n1\n1\n462\n\n\n2\n2\n729\n\n\n3\n3\n1739\n\n\n4\n4\n8836\n\n\n5\n5\n96529\n\n\n\n\n\n\nA natural question at this point is whether there is something about those students who have few observations. For example, are they from only a few schools?\nOne approach to examining properties like is to add the number of observations for each child to the :Child table. Later we can group the table according to this :ntest to look at properties of :Child by :ntest.\n\ngdf = groupby(\n  disallowmissing!(leftjoin(Child, nobsChild; on=:Child)), :ntest\n)\n\nGroupedDataFrame with 5 groups based on key: ntestFirst Group (462 rows): ntest = 1437 rows omitted\n\n\n\nRow\nChild\nSchool\nCohort\nSex\nage\nntest\n\n\n\nString\nString\nString\nString\nFloat64\nInt64\n\n\n\n\n1\nC002452\nS101175\n2013\nmale\n7.99452\n1\n\n\n2\nC002625\nS103329\n2013\nmale\n7.99452\n1\n\n\n3\nC002754\nS104814\n2013\nfemale\n7.99452\n1\n\n\n4\nC003269\nS102258\n2012\nfemale\n7.99726\n1\n\n\n5\nC003599\nS105843\n2012\nfemale\n7.99726\n1\n\n\n6\nC003807\nS100754\n2011\nmale\n8.0\n1\n\n\n7\nC003985\nS102945\n2011\nmale\n8.0\n1\n\n\n8\nC004086\nS104255\n2011\nmale\n8.0\n1\n\n\n9\nC004657\nS101400\n2014\nmale\n8.03833\n1\n\n\n10\nC005036\nS105909\n2014\nmale\n8.03833\n1\n\n\n11\nC005440\nS101023\n2019\nmale\n8.05202\n1\n\n\n12\nC005523\nS101825\n2019\nfemale\n8.05202\n1\n\n\n13\nC005697\nS103615\n2019\nmale\n8.05202\n1\n\n\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n\n\n451\nC112638\nS103718\n2015\nfemale\n9.0486\n1\n\n\n452\nC114749\nS112938\n2017\nmale\n9.06502\n1\n\n\n453\nC115460\nS101953\n2015\nmale\n9.08145\n1\n\n\n454\nC115569\nS100572\n2017\nmale\n9.08419\n1\n\n\n455\nC115587\nS100754\n2017\nfemale\n9.08419\n1\n\n\n456\nC117108\nS103196\n2018\nfemale\n9.10609\n1\n\n\n457\nC117229\nS103615\n2018\nmale\n9.10609\n1\n\n\n458\nC117230\nS103615\n2018\nmale\n9.10609\n1\n\n\n459\nC117419\nS104954\n2018\nfemale\n9.10609\n1\n\n\n460\nC117437\nS105004\n2018\nmale\n9.10609\n1\n\n\n461\nC117539\nS105636\n2018\nmale\n9.10609\n1\n\n\n462\nC117659\nS106483\n2018\nfemale\n9.10609\n1\n\n\n\n⋮Last Group (96529 rows): ntest = 596504 rows omitted\n\n\n\nRow\nChild\nSchool\nCohort\nSex\nage\nntest\n\n\n\nString\nString\nString\nString\nFloat64\nInt64\n\n\n\n\n1\nC002352\nS100067\n2013\nmale\n7.99452\n5\n\n\n2\nC002353\nS100067\n2013\nmale\n7.99452\n5\n\n\n3\nC002354\nS100067\n2013\nmale\n7.99452\n5\n\n\n4\nC002355\nS100122\n2013\nfemale\n7.99452\n5\n\n\n5\nC002356\nS100146\n2013\nmale\n7.99452\n5\n\n\n6\nC002357\nS100146\n2013\nmale\n7.99452\n5\n\n\n7\nC002358\nS100146\n2013\nmale\n7.99452\n5\n\n\n8\nC002360\nS100195\n2013\nfemale\n7.99452\n5\n\n\n9\nC002362\nS100237\n2013\nfemale\n7.99452\n5\n\n\n10\nC002363\nS100237\n2013\nfemale\n7.99452\n5\n\n\n11\nC002364\nS100250\n2013\nfemale\n7.99452\n5\n\n\n12\nC002365\nS100304\n2013\nmale\n7.99452\n5\n\n\n13\nC002366\nS100304\n2013\nmale\n7.99452\n5\n\n\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n\n\n96518\nC117942\nS130539\n2018\nmale\n9.10609\n5\n\n\n96519\nC117943\nS130539\n2018\nfemale\n9.10609\n5\n\n\n96520\nC117944\nS130539\n2018\nmale\n9.10609\n5\n\n\n96521\nC117945\nS130539\n2018\nmale\n9.10609\n5\n\n\n96522\nC117946\nS130539\n2018\nmale\n9.10609\n5\n\n\n96523\nC117956\nS400580\n2018\nmale\n9.10609\n5\n\n\n96524\nC117958\nS400919\n2018\nmale\n9.10609\n5\n\n\n96525\nC117959\nS400919\n2018\nmale\n9.10609\n5\n\n\n96526\nC117962\nS401250\n2018\nfemale\n9.10609\n5\n\n\n96527\nC117964\nS401470\n2018\nmale\n9.10609\n5\n\n\n96528\nC117965\nS401470\n2018\nfemale\n9.10609\n5\n\n\n96529\nC117966\nS800200\n2018\nmale\n9.10609\n5\n\n\n\n\n\n\nAre the sexes represented more-or-less equally?\n\ncombine(groupby(first(gdf), :Sex), nrow =&gt; :nchild)\n\n2×2 DataFrame\n\n\n\nRow\nSex\nnchild\n\n\n\nString\nInt64\n\n\n\n\n1\nmale\n230\n\n\n2\nfemale\n232\n\n\n\n\n\n\n\ncombine(groupby(last(gdf), :Sex), nrow =&gt; :nchild)\n\n2×2 DataFrame\n\n\n\nRow\nSex\nnchild\n\n\n\nString\nInt64\n\n\n\n\n1\nmale\n47552\n\n\n2\nfemale\n48977\n\n\n\n\n\n\n\n\n7 Reading Arrow files in other languages\nThere are Arrow implementations for R (the arrow package) and for Python (pyarrow).\n#| eval: false\nimport pyarrow.feather: read_table\nread_table(\"./data/fggk21.arrow\")\n#| eval: false\nlibrary(\"arrow\")\nfggk21 &lt;- read_feather(\"./data/fggk21.arrow\")\nnrow(fggk21)\n\n\n8 References\n\n\nFühner, T., Granacher, U., Golle, K., & Kliegl, R. (2021). Age and sex effects in physical fitness components of 108,295 third graders including 515 primary schools and 9 cohorts. Scientific Reports, 11(1). https://doi.org/10.1038/s41598-021-97000-4\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Getting started with Julia",
      "Saving data with Arrow"
    ]
  },
  {
    "objectID": "partial_within.html",
    "href": "partial_within.html",
    "title": "Partially-within subjects designs",
    "section": "",
    "text": "Begin by loading the packages to be used.\n\n\nCode\nusing AlgebraOfGraphics\nusing CairoMakie\nusing DataFrames\nusing MixedModels\nusing MixedModelsMakie\nusing MixedModelsSim\nusing ProgressMeter\nusing Random\n\nCairoMakie.activate!(; type=\"svg\")\n\nProgressMeter.ijulia_behavior(:clear)\n\n\n\n\nCode\nn_subj = 40\nn_item = 3\n# things are expressed as \"between\", so \"within subjects\" is \"between items\"\nitem_btwn = Dict(:frequency =&gt; [\"high\", \"medium\", \"low\"])\ndesign = simdat_crossed(MersenneTwister(42), n_subj, n_item;\n                        item_btwn = item_btwn)\ndesign = DataFrame(design)\n\n\n120×4 DataFrame95 rows omitted\n\n\n\nRow\nsubj\nitem\nfrequency\ndv\n\n\n\nString\nString\nString\nFloat64\n\n\n\n\n1\nS01\nI1\nhigh\n-0.556027\n\n\n2\nS02\nI1\nhigh\n-0.444383\n\n\n3\nS03\nI1\nhigh\n0.0271553\n\n\n4\nS04\nI1\nhigh\n-0.299484\n\n\n5\nS05\nI1\nhigh\n1.77786\n\n\n6\nS06\nI1\nhigh\n-1.1449\n\n\n7\nS07\nI1\nhigh\n-0.468606\n\n\n8\nS08\nI1\nhigh\n0.156143\n\n\n9\nS09\nI1\nhigh\n-2.64199\n\n\n10\nS10\nI1\nhigh\n1.00331\n\n\n11\nS11\nI1\nhigh\n1.08238\n\n\n12\nS12\nI1\nhigh\n0.187028\n\n\n13\nS13\nI1\nhigh\n0.518149\n\n\n⋮\n⋮\n⋮\n⋮\n⋮\n\n\n109\nS29\nI3\nlow\n-0.338763\n\n\n110\nS30\nI3\nlow\n-0.0953675\n\n\n111\nS31\nI3\nlow\n0.768972\n\n\n112\nS32\nI3\nlow\n1.44244\n\n\n113\nS33\nI3\nlow\n-0.275032\n\n\n114\nS34\nI3\nlow\n-0.379637\n\n\n115\nS35\nI3\nlow\n-0.722696\n\n\n116\nS36\nI3\nlow\n0.139661\n\n\n117\nS37\nI3\nlow\n-1.40934\n\n\n118\nS38\nI3\nlow\n1.05546\n\n\n119\nS39\nI3\nlow\n-2.23782\n\n\n120\nS40\nI3\nlow\n1.15915\n\n\n\n\n\n\n\n\nCode\nunique!(select(design, :item, :frequency))\n\n\n3×2 DataFrame\n\n\n\nRow\nitem\nfrequency\n\n\n\nString\nString\n\n\n\n\n1\nI1\nhigh\n\n\n2\nI2\nmedium\n\n\n3\nI3\nlow\n\n\n\n\n\n\n\n\nCode\nm0 = let contrasts, form\n    contrasts = Dict(:frequency =&gt; HelmertCoding(base=\"high\"))\n    form = @formula(dv ~ 1 + frequency +\n                    (1 + frequency | subj))\n    fit(MixedModel, form, design; contrasts)\nend\n\n\nMinimizing 73    Time: 0:00:00 ( 4.52 ms/it)\n\n\n\n\n\n\nEst.\nSE\nz\np\nσ_subj\n\n\n\n\n(Intercept)\n-0.0830\n0.0983\n-0.84\n0.3983\n0.5440\n\n\nfrequency: low\n0.0411\n0.1258\n0.33\n0.7438\n0.7051\n\n\nfrequency: medium\n-0.0060\n0.0572\n-0.11\n0.9160\n0.2928\n\n\nResidual\n0.5204\n\n\n\n\n\n\n\n\n\n\n\nCode\ncorrmat = [ 1    0.1 -0.2\n            0.1  1    0.1\n           -0.2  0.1  1 ]\nre_subj = create_re(1.2, 1.5, 1.5; corrmat)\n\n\n3×3 LinearAlgebra.LowerTriangular{Float64, Matrix{Float64}}:\n  1.2    ⋅         ⋅ \n  0.15  1.49248    ⋅ \n -0.3   0.180907  1.45852\n\n\n\n\nCode\nθ = createθ(m0; subj=re_subj)\n\n\n6-element Vector{Float64}:\n  1.2\n  0.15000000000000002\n -0.30000000000000004\n  1.49248115565993\n  0.18090680674665818\n  1.4585173044131932\n\n\n\n\nCode\nσ = 1;\nβ = [1.0, -3, -2];\n\n\n\n\nCode\nfit!(simulate!(m0; θ, β, σ))\n\n\n\n\n\n\nEst.\nSE\nz\np\nσ_subj\n\n\n\n\n(Intercept)\n1.2070\n0.1527\n7.90\n&lt;1e-14\n0.6362\n\n\nfrequency: low\n-3.2655\n0.3056\n-10.68\n&lt;1e-25\n1.7158\n\n\nfrequency: medium\n-2.0013\n0.2044\n-9.79\n&lt;1e-21\n1.1859\n\n\nResidual\n1.2587\n\n\n\n\n\n\n\n\n\n\n\nCode\nshrinkageplot(m0)\n\n\n\n\n\n\n\n\n\n\n\nCode\ncaterpillar(m0; orderby=nothing, vline_at_zero=true)\n\n\n\n\n\n\n\n\n\n\n\nCode\ndesign[!, :dv] .= response(m0)\n\n\n120-element Vector{Float64}:\n  7.34755654258727\n 10.229604583695\n  3.6043175855673883\n  5.3566728415298925\n  4.474249009070753\n  7.719899856563901\n  4.025122582186671\n  5.209742237453524\n  5.451105316896041\n  5.39384999765566\n  4.332314108764023\n  8.376518943225815\n  4.405193973744266\n  ⋮\n  0.07309586813780955\n  1.4581187646069615\n  0.7913143551731827\n  1.2178125667082424\n  1.5417032495112117\n -3.1374916197365295\n  1.230062037484938\n  0.35610481040040387\n  2.3340449614726744\n  1.5733946081398242\n -0.01802135858697218\n -3.7247679264549767\n\n\n\n\nCode\ndesign_partial = filter(design) do row\n    subj = parse(Int, row.subj[2:end])\n    item = parse(Int, row.item[2:end])\n    # for even-numbered subjects, we keep all conditions\n    # for odd-numbered subjects, we keep only the two \"odd\" items,\n    # i.e. the first and last conditions\n    return iseven(subj) || isodd(item)\nend\nsort!(unique!(select(design_partial, :subj, :frequency)), :subj)\n\n\n100×2 DataFrame75 rows omitted\n\n\n\nRow\nsubj\nfrequency\n\n\n\nString\nString\n\n\n\n\n1\nS01\nhigh\n\n\n2\nS01\nlow\n\n\n3\nS02\nhigh\n\n\n4\nS02\nmedium\n\n\n5\nS02\nlow\n\n\n6\nS03\nhigh\n\n\n7\nS03\nlow\n\n\n8\nS04\nhigh\n\n\n9\nS04\nmedium\n\n\n10\nS04\nlow\n\n\n11\nS05\nhigh\n\n\n12\nS05\nlow\n\n\n13\nS06\nhigh\n\n\n⋮\n⋮\n⋮\n\n\n89\nS36\nmedium\n\n\n90\nS36\nlow\n\n\n91\nS37\nhigh\n\n\n92\nS37\nlow\n\n\n93\nS38\nhigh\n\n\n94\nS38\nmedium\n\n\n95\nS38\nlow\n\n\n96\nS39\nhigh\n\n\n97\nS39\nlow\n\n\n98\nS40\nhigh\n\n\n99\nS40\nmedium\n\n\n100\nS40\nlow\n\n\n\n\n\n\n\n\nCode\nm1 = let contrasts, form\n    contrasts = Dict(:frequency =&gt; HelmertCoding(base=\"high\"))\n    form = @formula(dv ~ 1 + frequency +\n                    (1 + frequency | subj))\n    fit(MixedModel, form, design_partial; contrasts)\nend\n\n\n\n\n\n\nEst.\nSE\nz\np\nσ_subj\n\n\n\n\n(Intercept)\n1.1885\n0.1832\n6.49\n&lt;1e-10\n0.5590\n\n\nfrequency: low\n-3.2655\n0.3056\n-10.68\n&lt;1e-25\n1.7496\n\n\nfrequency: medium\n-2.0198\n0.2726\n-7.41\n&lt;1e-12\n1.4723\n\n\nResidual\n1.1621\n\n\n\n\n\n\n\n\n\n\n\nCode\nshrinkageplot(m1)\n\n\n\n\n\n\n\n\n\n\n\nCode\ncaterpillar(m1; orderby=nothing, vline_at_zero=true)\n\n\n\n\n\n\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Worked examples",
      "Partially-within subjects designs"
    ]
  },
  {
    "objectID": "mrk17.html",
    "href": "mrk17.html",
    "title": "RePsychLing Masson, Rabe, & Kliegl, 2017) with Julia: Specification and selection",
    "section": "",
    "text": "Packages we (might) use.\n\nusing CategoricalArrays\nusing DataFrames\nusing MixedModels\nusing MixedModelsMakie\nusing SMLP2023: dataset\nusing Statistics: mean, std\n\n\ndat = DataFrame(dataset(:mrk17_exp1))\ndescribe(dat)\n\n9×7 DataFrame\n\n\n\nRow\nvariable\nmean\nmin\nmedian\nmax\nnmissing\neltype\n\n\n\nSymbol\nUnion…\nAny\nUnion…\nAny\nInt64\nDataType\n\n\n\n\n1\nsubj\n\nS01\n\nS73\n0\nString\n\n\n2\nitem\n\nACHE\n\nYEAR\n0\nString\n\n\n3\ntrial\n239.958\n2\n239.0\n480\n0\nInt16\n\n\n4\nF\n\nHF\n\nLF\n0\nString\n\n\n5\nP\n\nrel\n\nunr\n0\nString\n\n\n6\nQ\n\nclr\n\ndeg\n0\nString\n\n\n7\nlQ\n\nclr\n\ndeg\n0\nString\n\n\n8\nlT\n\nNW\n\nWD\n0\nString\n\n\n9\nrt\n647.173\n301\n601.0\n2994\n0\nInt16",
    "crumbs": [
      "Worked examples",
      "RePsychLing Masson, Rabe, & Kliegl, 2017) with Julia: Specification and selection"
    ]
  },
  {
    "objectID": "mrk17.html#response-covariates-and-factors",
    "href": "mrk17.html#response-covariates-and-factors",
    "title": "RePsychLing Masson, Rabe, & Kliegl, 2017) with Julia: Specification and selection",
    "section": "2.1 Response, covariates, and factors",
    "text": "2.1 Response, covariates, and factors\nLinear mixed models (LMMs), like many other types of statistical models, describe a relationship between a response variable and covariates that have been measured or observed along with the response. The statistical model assumes that the residuals of the fitted response (i.e., not the responses) are normally – also identically and independently – distributed. This is the first assumption of normality in the LMM. It is standard practice that model residuals are inspected and, if serious skew is indicated, that the response is Box-Cox transformed (unless not justified for theoretical reasons) to fulfill this model assumption.\nIn the following we distinguish between categorical covariates and numerical covariates. Categorical covariates are factors. The important characteristic of a factor is that, for each observed value of the response, the factor takes on the value of one of a set of discrete levels. The levels can be unordered (nominal) or ordered (ordinal). We use the term covariate when we refer to numerical covariates, that is to continuous measures with some distribution. In principle, statistical models are not constrained by the distribution of observations across levels of factors and covariates, but the distribution may lead to problems of model identification and it does implications for statistical power.\nStatistical power, especially for the detection of interactions, is best when observations are uniformly distributed across levels of factors or uniform across the values of covariates. In experimental designs, uniform distributions may be achieved by balanced assignment of subjects (or other carriers of responses) to the levels of factors or combinations of factor levels. In observational contexts, we achieve uniform distributions by stratification (e..g., on age, gender, or IQ scores). Statistical power is worse for skewed than normal distributions (I think …). Therefore, although it is not required to meet an assumption of the statistical model, it may be useful to consider Box-Cox transformations of covariates.",
    "crumbs": [
      "Worked examples",
      "RePsychLing Masson, Rabe, & Kliegl, 2017) with Julia: Specification and selection"
    ]
  },
  {
    "objectID": "mrk17.html#nested-and-crossed-random-grouping-factors",
    "href": "mrk17.html#nested-and-crossed-random-grouping-factors",
    "title": "RePsychLing Masson, Rabe, & Kliegl, 2017) with Julia: Specification and selection",
    "section": "2.2 Nested and crossed random (grouping) factors",
    "text": "2.2 Nested and crossed random (grouping) factors\nIn LMMs the levels of at least one of the factors represents units in the data set that are assumed to be sampled, ideally randomly, from a population that is normally distributed with respect to the response. This is the second assumption of normal distribution in LMMs. In psychology and linguistics the observational units are often the subjects or items (e..g., texts, sentences, words, pictures) in the study. We may use numbers, such as subject identifiers, to designate the particular levels that we observed; we recommend to prepend these numbers with “S” or “I” to avoid confusion with numeric variables.\nRandom sampling is the basis of generalization from the sample to the population. The core statistics we will estimate in this context are variances and correlations of grand means and (quasi-)experimental effects. These terms will be explained below. What we want to stress here is that the estimation of (co-)variances / correlations requires a larger number of units (levels) than the estimation of means. Therefore, from a practical perspective, it is important that random factors are represented with many units.\nWhen there is more than one random factor, we must be clear about their relation. The two prototypical cases are that the factors are nested or crossed. In multilevel models, a special case of mixed models, the levels of the random factors are strictly nested. For example, at a given time, every student attends a specific class in a specific school. Students, classes, and schools could be three random factors. As soon as we look at this scenario across several school years, the nesting quickly falls apart because students may move between classes and between schools.\nIn psychology and linguistics, random factors are often crossed, for example, when every subject reads every word in every sentence in a word-by-word self-paced reading experiment (or alternatively: when every word in every sentence elicits a response from every subject). However, in an eye-movement experiment (for example), the perfect crossing on a measure like fixation duration is not attainable because of blinks or skipping of words.\nIn summary, the typical situation in experimental and observational studies with more than one random factor is partial crossing or partial nesting of levels of the random factors. Linear mixed models handle these situations very well.",
    "crumbs": [
      "Worked examples",
      "RePsychLing Masson, Rabe, & Kliegl, 2017) with Julia: Specification and selection"
    ]
  },
  {
    "objectID": "mrk17.html#experimental-and-quasi-experimental-fixed-factors-covariates",
    "href": "mrk17.html#experimental-and-quasi-experimental-fixed-factors-covariates",
    "title": "RePsychLing Masson, Rabe, & Kliegl, 2017) with Julia: Specification and selection",
    "section": "2.3 Experimental and quasi-experimental fixed factors / covariates",
    "text": "2.3 Experimental and quasi-experimental fixed factors / covariates\nFixed experimental factor or covariate. In experiments the units (or levels) of the random factor(s) are assigned to manipulations implemented in their design. The researcher controls the assignment of units of the random factor(s) (e.g., subjects, items) to experimental manipulations. These manipulations are represented as factors with a fixed and discrete set of levels (e.g., training vs. control group) or as covariates associated with continuous numeric values (e.g., presentation times).\nFixed quasi-experimental factor or covariate. In observational studies (which can also be experiments) the units (or levels) of random factors may “bring along” characteristics that represent the levels of quasi-experimental factors or covariates beyond the control of the researcher. Whether a a subject is female, male, or diverse or whether a word is a noun, a verb, or an adjective are examples of quasi-experimental factors of gender or word type, respectively. Subject-related covariates are body height, body mass, and IQ scores; word-related covariates are their lengths, frequency, and cloze predictability.",
    "crumbs": [
      "Worked examples",
      "RePsychLing Masson, Rabe, & Kliegl, 2017) with Julia: Specification and selection"
    ]
  },
  {
    "objectID": "mrk17.html#between-unit-and-within-unit-factors-covariates",
    "href": "mrk17.html#between-unit-and-within-unit-factors-covariates",
    "title": "RePsychLing Masson, Rabe, & Kliegl, 2017) with Julia: Specification and selection",
    "section": "2.4 Between-unit and within-unit factors / covariates",
    "text": "2.4 Between-unit and within-unit factors / covariates\nThe distinction between between-unit and within-unit factors is always relative to a random (grouping) factor of an experimental design. A between-unit factor / covariate is a factor for which every unit of the random factor is assigned to or characterized by only one level of the factor. A within-unit factor is a factor for which units of the random factor appear at every level of the factor.\nFor the typical random factor, say Subject, there is little ambiguity because we are used to the between-within distinction from ANOVAs, more specifically the F1-ANOVA. In psycholinguistics, there is the tradition to test effects also for the second random factor Item in an F2-ANOVA. Importantly, for a given fixed factor all four combinations are possible. For example, Gender is a fixed quasi-experimental between-subject / within-item factor; word frequency is a fixed quasi-experimental within-subject / between-item covariate; Prime-target relation is a fixed experimental within-subject / within-item factor (assuming that targets are presented both in a primed and in an unprimed situation); and when a training manipulation is defined by the items used in the training, then in a training-control group design, the fixed factor Group is a fixed experimental between-subject / between-item factor.\nThese distinctions are critical for setting up LMMs because variance components for (quasi-)experimental effects can only be specified for within-unit effects. Note also that loss of data (within limits), counterbalancing or blocking of items are irrelevant for these definitions.",
    "crumbs": [
      "Worked examples",
      "RePsychLing Masson, Rabe, & Kliegl, 2017) with Julia: Specification and selection"
    ]
  },
  {
    "objectID": "mrk17.html#factor-based-contrasts-and-covariate-based-trends",
    "href": "mrk17.html#factor-based-contrasts-and-covariate-based-trends",
    "title": "RePsychLing Masson, Rabe, & Kliegl, 2017) with Julia: Specification and selection",
    "section": "2.5 Factor-based contrasts and covariate-based trends",
    "text": "2.5 Factor-based contrasts and covariate-based trends\nThe simplest fixed factor has two levels and the model estimates the difference between them. When we move to factors with k levels, we must decide on how we spend the k-1 degrees of freedom, that is we must specify a set of contrasts. (If we don’t do it, the program chooses DummyCoding contrasts for us.)\nThe simplest specification of a covariate is to include its linear trend, that is its slope. The slope (like a contrast) represents a difference score, that is the change in response to a one-unit change on the covariate. For covariates we must decide on the order of the trend we want to model.",
    "crumbs": [
      "Worked examples",
      "RePsychLing Masson, Rabe, & Kliegl, 2017) with Julia: Specification and selection"
    ]
  },
  {
    "objectID": "mrk17.html#contrast--and-trend-based-fixed-effect-model-parameters",
    "href": "mrk17.html#contrast--and-trend-based-fixed-effect-model-parameters",
    "title": "RePsychLing Masson, Rabe, & Kliegl, 2017) with Julia: Specification and selection",
    "section": "2.6 Contrast- and trend-based fixed-effect model parameters",
    "text": "2.6 Contrast- and trend-based fixed-effect model parameters\nFixed factors and covariates are expected to have effects on the response. Fixed-effect model parameters estimate the hypothesized main and interaction effects of the study. The estimates of factors are based on contrasts; the estimates of covariates are based on trends. Conceptually, they correspond to unstandardized regression coefficients in multiple regression.\nThe intercept is a special regression coefficient; it estimates the value of the dependent variable when all fixed effects associated with factors and trends associated with covariates are zero. In experimental designs with higher-order interactions there is an advantage of specifying the LMM in such a way that the intercept estimates the grand mean (GM; mean of the means of design cells). This happens if (a) contrasts for factors are chosen such that the intercept estimates the GM (positive: EffectsCoding, SeqDifferenceCoding, or HelmertCoding contrasts; negative: DummyCoding), (b) orthogonal polynomial trends are used (Helmert, anova-based), and (c) covariates are centered on their mean before inclusion in the model. As always, there may be good theoretical reasons to depart from the default recommendation.\nThe specification of contrasts / trends does not depend on the status of the fixed factor / covariate. It does not matter whether a factor varies between or within the units of a random factor or whether it is an experimental or quasi-experimental factor. Contrasts are not specified for random (grouping) factors.",
    "crumbs": [
      "Worked examples",
      "RePsychLing Masson, Rabe, & Kliegl, 2017) with Julia: Specification and selection"
    ]
  },
  {
    "objectID": "mrk17.html#variance-components-vcs-and-correlation-parameters-cps",
    "href": "mrk17.html#variance-components-vcs-and-correlation-parameters-cps",
    "title": "RePsychLing Masson, Rabe, & Kliegl, 2017) with Julia: Specification and selection",
    "section": "2.7 Variance components (VCs) and correlation parameters (CPs)",
    "text": "2.7 Variance components (VCs) and correlation parameters (CPs)\nVariance components (VCs) and correlation parameters (CPs) are within-group model parameters; they correspond to (some of the) within-unit (quasi-)experimental fixed-effect model parameters. Thus, we may be able to estimate a subject-related VC for word frequency. If we included a linear trend for word frequency, the VC estimates the subject-related variance in these slopes. We cannot estimate an item-related VC for the word-frequency slopes because there is only one frequency associated with words. Analogously, we may able to estimate an item-related VC for the effect of Group (training vs. control), but we cannot estimate a subject-related VC for this effect.\nThe within-between characteristics of fixed factors and covariates relative to the random factor(s) are features of the design of the experiment or observational study. They fundamentally constrain the specification of the LMM. That’s why it is of upmost importance to be absolutely clear about their status.",
    "crumbs": [
      "Worked examples",
      "RePsychLing Masson, Rabe, & Kliegl, 2017) with Julia: Specification and selection"
    ]
  },
  {
    "objectID": "mrk17.html#conditional-modes-of-random-effects",
    "href": "mrk17.html#conditional-modes-of-random-effects",
    "title": "RePsychLing Masson, Rabe, & Kliegl, 2017) with Julia: Specification and selection",
    "section": "2.8 Conditional modes of random effects",
    "text": "2.8 Conditional modes of random effects\nIn this outline of the dimensions underlying the specification of an LMM, we have said nothing so far about the conditional modes of random effects (i.e., the results shown in caterpillar and shrinkage plots). They are not needed for model specification or model selection.\nThe VC is the prior variance of the random effects, whereas var(ranef(model)) is the variance of the posterior means/modes of the random effects. See Kliegl et al. (2010, VisualCognition); Rizopoulos (2019, stackexchange.",
    "crumbs": [
      "Worked examples",
      "RePsychLing Masson, Rabe, & Kliegl, 2017) with Julia: Specification and selection"
    ]
  },
  {
    "objectID": "mrk17.html#abstract",
    "href": "mrk17.html#abstract",
    "title": "RePsychLing Masson, Rabe, & Kliegl, 2017) with Julia: Specification and selection",
    "section": "3.1 Abstract",
    "text": "3.1 Abstract\nThis semantic-priming experiment was reported in Masson, Rabe, & Kliegl (2017, Exp. 1, Memory & Cognition). It is a direct replication of an experiment reported in Masson & Kliegl (2013, Exp. 1, JEPLMC). Following a prime word a related or unrelated high- or low-frequency target word or a nonword was presented in clear or dim font. The subject’s task was to decide as quickly as possible whether the target was a word or a nonword, that is subjects performed a lexical decision task (LDT). The reaction time and the accuracy of the response were recorded. Only correct reaction times to words are included. After filtering there were 16,409 observations recorded from 73 subjects and 240 items.",
    "crumbs": [
      "Worked examples",
      "RePsychLing Masson, Rabe, & Kliegl, 2017) with Julia: Specification and selection"
    ]
  },
  {
    "objectID": "mrk17.html#codebook",
    "href": "mrk17.html#codebook",
    "title": "RePsychLing Masson, Rabe, & Kliegl, 2017) with Julia: Specification and selection",
    "section": "3.2 Codebook",
    "text": "3.2 Codebook\nThe data (variables and observations) used by Masson et al. (2017) are available in file MRK17_Exp1.RDS\n\n\n\nVariable\nDescription\n\n\n\n\nSubj\nSubject identifier\n\n\nItem\nTarget (non-)word\n\n\ntrial\nTrial number\n\n\nF\nTarget frequency is high or low\n\n\nP\nPrime is related or unrelated to target\n\n\nQ\nTarget quality is clear or degraded\n\n\nlQ\nLast-trial target quality is clear or degraded\n\n\nlT\nLast-trail target requires word or nonword response\n\n\nrt\nReaction time [ms]\n\n\n\nlagQlty and lagTrgt refer to experimental conditions in the last trial.\nCorresponding indicator variables (-1/+1):",
    "crumbs": [
      "Worked examples",
      "RePsychLing Masson, Rabe, & Kliegl, 2017) with Julia: Specification and selection"
    ]
  },
  {
    "objectID": "mrk17.html#model-fit",
    "href": "mrk17.html#model-fit",
    "title": "RePsychLing Masson, Rabe, & Kliegl, 2017) with Julia: Specification and selection",
    "section": "5.1 Model fit",
    "text": "5.1 Model fit\n\ncontrasts =\n    Dict( :F =&gt; EffectsCoding(; levels=[\"LF\", \"HF\"]) ,\n          :P =&gt; EffectsCoding(; levels=[\"unr\", \"rel\"]),\n          :Q =&gt; EffectsCoding(; levels=[\"deg\", \"clr\"]),\n          :lQ =&gt;EffectsCoding(; levels=[\"deg\", \"clr\"]),\n          :lT =&gt;EffectsCoding(; levels=[\"NW\", \"WD\"])\n          );\n\nm_cpx = let\n    form = @formula (1000/rt) ~ 1+F*P*Q*lQ*lT +\n                               (1+F+P+Q+lQ+lT | subj) +\n                               (1  +P+Q+lQ+lT | item);\n    fit(MixedModel, form, dat; contrasts)\nend\n\nVarCorr(m_cpx)\n\n┌ Warning: ProgressMeter by default refresh meters with additional information in IJulia via `IJulia.clear_output`, which clears all outputs in the cell. \n│  - To prevent this behaviour, do `ProgressMeter.ijulia_behavior(:append)`. \n│  - To disable this warning message, do `ProgressMeter.ijulia_behavior(:clear)`.\n└ @ ProgressMeter ~/.julia/packages/ProgressMeter/dMfiC/src/ProgressMeter.jl:594\nMinimizing 1227    Time: 0:00:06 ( 5.02 ms/it)\n  objective:  7147.992395971073\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nColumn\nVariance\nStd.Dev\nCorr.\n\n\n\n\n\n\n\n\nitem\n(Intercept)\n0.00320236\n0.05658940\n\n\n\n\n\n\n\n\nP: rel\n0.00012481\n0.01117165\n+0.05\n\n\n\n\n\n\n\nQ: clr\n0.00015754\n0.01255162\n+0.36\n+0.40\n\n\n\n\n\n\nlQ: clr\n0.00000864\n0.00293953\n+0.76\n+0.33\n-0.11\n\n\n\n\n\nlT: WD\n0.00015751\n0.01255021\n-0.11\n-0.89\n-0.00\n-0.60\n\n\n\nsubj\n(Intercept)\n0.03061785\n0.17497956\n\n\n\n\n\n\n\n\nF: HF\n0.00002887\n0.00537302\n+0.45\n\n\n\n\n\n\n\nP: rel\n0.00012875\n0.01134679\n+0.35\n+0.99\n\n\n\n\n\n\nQ: clr\n0.00078832\n0.02807711\n+0.41\n+0.72\n+0.70\n\n\n\n\n\nlQ: clr\n0.00011482\n0.01071537\n+0.06\n+0.16\n+0.17\n+0.58\n\n\n\n\nlT: WD\n0.00104494\n0.03232551\n+0.26\n-0.02\n-0.05\n+0.37\n+0.50\n\n\nResidual\n\n0.08574920\n0.29282964\n\n\n\n\n\n\n\n\n\n\n\nissingular(m_cpx)\n\ntrue\n\n\n\nMixedModels.PCA(m_cpx)\n\n(item = \nPrincipal components based on correlation matrix\n (Intercept)   1.0     .      .      .     .\n P: rel        0.05   1.0     .      .     .\n Q: clr        0.36   0.4    1.0     .     .\n lQ: clr       0.76   0.33  -0.11   1.0    .\n lT: WD       -0.11  -0.89  -0.0   -0.6   1.0\n\nNormalized cumulative variances:\n[0.4953, 0.7628, 1.0, 1.0, 1.0]\n\nComponent loadings\n                PC1    PC2    PC3    PC4    PC5\n (Intercept)  -0.37   0.69  -0.14   0.6    0.11\n P: rel       -0.51  -0.49  -0.18   0.08   0.68\n Q: clr       -0.19   0.05  -0.87  -0.32  -0.3\n lQ: clr      -0.51   0.37   0.36  -0.68   0.06\n lT: WD        0.55   0.38  -0.23  -0.27   0.65, subj = \nPrincipal components based on correlation matrix\n (Intercept)   1.0     .      .      .      .     .\n F: HF         0.45   1.0     .      .      .     .\n P: rel        0.35   0.99   1.0     .      .     .\n Q: clr        0.41   0.72   0.7    1.0     .     .\n lQ: clr       0.06   0.16   0.17   0.58   1.0    .\n lT: WD        0.26  -0.02  -0.05   0.37   0.5   1.0\n\nNormalized cumulative variances:\n[0.5111, 0.7668, 0.9113, 0.9728, 1.0, 1.0]\n\nComponent loadings\n                PC1    PC2    PC3    PC4    PC5    PC6\n (Intercept)  -0.33   0.02   0.83   0.43  -0.08  -0.08\n F: HF        -0.51   0.34  -0.08  -0.17  -0.27   0.72\n P: rel       -0.5    0.35  -0.18  -0.23  -0.27  -0.69\n Q: clr       -0.52  -0.14  -0.15   0.05   0.83  -0.0\n lQ: clr      -0.28  -0.56  -0.41   0.54  -0.38  -0.0\n lT: WD       -0.18  -0.66   0.28  -0.66  -0.14   0.0)\n\n\nVariance-covariance matrix of random-effect structure suggests overparameterization for both subject-related and item-related components.\nWe don’t look at fixed effects before model selection.",
    "crumbs": [
      "Worked examples",
      "RePsychLing Masson, Rabe, & Kliegl, 2017) with Julia: Specification and selection"
    ]
  },
  {
    "objectID": "mrk17.html#vcs-and-cps",
    "href": "mrk17.html#vcs-and-cps",
    "title": "RePsychLing Masson, Rabe, & Kliegl, 2017) with Julia: Specification and selection",
    "section": "5.2 VCs and CPs",
    "text": "5.2 VCs and CPs\nWe can also look separately at item- and subj-related VCs and CPs for subjects and items.\n\nfirst(m_cpx.λ)\n\n5×5 LinearAlgebra.LowerTriangular{Float64, Matrix{Float64}}:\n  0.19325       ⋅            ⋅            ⋅            ⋅ \n  0.00194837   0.0381009     ⋅            ⋅            ⋅ \n  0.0156423    0.0163521    0.036403      ⋅            ⋅ \n  0.0076146    0.00290876  -0.00585688   0.000150091   ⋅ \n -0.00489674  -0.0380148    0.0191766   -5.7594e-5    1.93105e-6\n\n\nVP is zero for last diagonal entry; not supported by data.\n\nlast(m_cpx.λ)\n\n6×6 LinearAlgebra.LowerTriangular{Float64, Matrix{Float64}}:\n 0.597547      ⋅          ⋅           ⋅          ⋅            ⋅ \n 0.00817716   0.0164258   ⋅           ⋅          ⋅            ⋅ \n 0.0134442    0.0363417  0.0          ⋅          ⋅            ⋅ \n 0.0392617    0.0572024  0.00654036  0.0658558   ⋅            ⋅ \n 0.00205337   0.005683   0.0285271   0.0221063  0.000113326   ⋅ \n 0.0283612   -0.0168128  0.0297579   0.0543555  0.0819662    0.0232474\n\n\nVP is zero for fourth diagonal entry; not supported by data.",
    "crumbs": [
      "Worked examples",
      "RePsychLing Masson, Rabe, & Kliegl, 2017) with Julia: Specification and selection"
    ]
  },
  {
    "objectID": "mrk17.html#model-fit-1",
    "href": "mrk17.html#model-fit-1",
    "title": "RePsychLing Masson, Rabe, & Kliegl, 2017) with Julia: Specification and selection",
    "section": "6.1 Model fit",
    "text": "6.1 Model fit\nWe take out correlation parameters.\n\nm_zcp = let\n    form = @formula (1000/rt) ~ 1+F*P*Q*lQ*lT +\n                       zerocorr(1+F+P+Q+lQ+lT | subj) +\n                       zerocorr(1  +P+Q+lQ+lT | item);\n    fit(MixedModel, form, dat; contrasts)\nend\n\nVarCorr(m_zcp)\nissingular(m_zcp)\nMixedModels.PCA(m_zcp)\n\n┌ Warning: ProgressMeter by default refresh meters with additional information in IJulia via `IJulia.clear_output`, which clears all outputs in the cell. \n│  - To prevent this behaviour, do `ProgressMeter.ijulia_behavior(:append)`. \n│  - To disable this warning message, do `ProgressMeter.ijulia_behavior(:clear)`.\n└ @ ProgressMeter ~/.julia/packages/ProgressMeter/dMfiC/src/ProgressMeter.jl:594\nMinimizing 304    Time: 0:00:01 ( 3.85 ms/it)\n\n\n(item = \nPrincipal components based on correlation matrix\n (Intercept)  1.0  .    .    .    .\n P: rel       0.0  1.0  .    .    .\n Q: clr       0.0  0.0  1.0  .    .\n lQ: clr      0.0  0.0  0.0  1.0  .\n lT: WD       0.0  0.0  0.0  0.0  1.0\n\nNormalized cumulative variances:\n[0.2, 0.4, 0.6, 0.8, 1.0]\n\nComponent loadings\n               PC1   PC2   PC3   PC4   PC5\n (Intercept)  1.0   0.0   0.0   0.0   0.0\n P: rel       0.0   1.0   0.0   0.0   0.0\n Q: clr       0.0   0.0   1.0   0.0   0.0\n lQ: clr      0.0   0.0   0.0   1.0   0.0\n lT: WD       0.0   0.0   0.0   0.0   1.0, subj = \nPrincipal components based on correlation matrix\n (Intercept)  1.0  .    .    .    .    .\n F: HF        0.0  1.0  .    .    .    .\n P: rel       0.0  0.0  1.0  .    .    .\n Q: clr       0.0  0.0  0.0  1.0  .    .\n lQ: clr      0.0  0.0  0.0  0.0  1.0  .\n lT: WD       0.0  0.0  0.0  0.0  0.0  1.0\n\nNormalized cumulative variances:\n[0.1667, 0.3333, 0.5, 0.6667, 0.8333, 1.0]\n\nComponent loadings\n               PC1   PC2   PC3   PC4   PC5   PC6\n (Intercept)  1.0   0.0   0.0   0.0   0.0   0.0\n F: HF        0.0   1.0   0.0   0.0   0.0   0.0\n P: rel       0.0   0.0   1.0   0.0   0.0   0.0\n Q: clr       0.0   0.0   0.0   1.0   0.0   0.0\n lQ: clr      0.0   0.0   0.0   0.0   1.0   0.0\n lT: WD       0.0   0.0   0.0   0.0   0.0   1.0)\n\n\n\nMixedModels.likelihoodratiotest(m_zcp, m_cpx)\n\n\n\n\n\n\n\n\n\n\n\n\n\nmodel-dof\ndeviance\nχ²\nχ²-dof\nP(&gt;χ²)\n\n\n\n\n:(1000 / rt) ~ 1 + F + P + Q + lQ + lT + F & P + F & Q + P & Q + F & lQ + P & lQ + Q & lQ + F & lT + P & lT + Q & lT + lQ & lT + F & P & Q + F & P & lQ + F & Q & lQ + P & Q & lQ + F & P & lT + F & Q & lT + P & Q & lT + F & lQ & lT + P & lQ & lT + Q & lQ & lT + F & P & Q & lQ + F & P & Q & lT + F & P & lQ & lT + F & Q & lQ & lT + P & Q & lQ & lT + F & P & Q & lQ & lT + zerocorr(1 + F + P + Q + lQ + lT | subj) + zerocorr(1 + P + Q + lQ + lT | item)\n44\n7188\n\n\n\n\n\n:(1000 / rt) ~ 1 + F + P + Q + lQ + lT + F & P + F & Q + P & Q + F & lQ + P & lQ + Q & lQ + F & lT + P & lT + Q & lT + lQ & lT + F & P & Q + F & P & lQ + F & Q & lQ + P & Q & lQ + F & P & lT + F & Q & lT + P & Q & lT + F & lQ & lT + P & lQ & lT + Q & lQ & lT + F & P & Q & lQ + F & P & Q & lT + F & P & lQ & lT + F & Q & lQ & lT + P & Q & lQ & lT + F & P & Q & lQ & lT + (1 + F + P + Q + lQ + lT | subj) + (1 + P + Q + lQ + lT | item)\n69\n7148\n40\n25\n0.0259\n\n\n\n\n\nLooks ok. It might be a good idea to prune the LMM by removing small VCs.\n\nm_zcp2 = let\n    form = @formula (1000/rt) ~ 1+F*P*Q*lQ*lT +\n                       zerocorr(1  +P+Q+lQ+lT | subj) +\n                       zerocorr(1  +P+Q   +lT | item);\n    fit(MixedModel, form, dat; contrasts)\nend\nVarCorr(m_zcp2)\n\n┌ Warning: ProgressMeter by default refresh meters with additional information in IJulia via `IJulia.clear_output`, which clears all outputs in the cell. \n│  - To prevent this behaviour, do `ProgressMeter.ijulia_behavior(:append)`. \n│  - To disable this warning message, do `ProgressMeter.ijulia_behavior(:clear)`.\n└ @ ProgressMeter ~/.julia/packages/ProgressMeter/dMfiC/src/ProgressMeter.jl:594\nMinimizing 272    Time: 0:00:00 ( 2.61 ms/it)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nColumn\nVariance\nStd.Dev\nCorr.\n\n\n\n\n\n\n\nitem\n(Intercept)\n0.00320203\n0.05658644\n\n\n\n\n\n\n\nP: rel\n0.00007137\n0.00844831\n.\n\n\n\n\n\n\nQ: clr\n0.00014719\n0.01213221\n.\n.\n\n\n\n\n\nlT: WD\n0.00011708\n0.01082044\n.\n.\n.\n\n\n\nsubj\n(Intercept)\n0.03061081\n0.17495945\n\n\n\n\n\n\n\nP: rel\n0.00009921\n0.00996036\n.\n\n\n\n\n\n\nQ: clr\n0.00077304\n0.02780361\n.\n.\n\n\n\n\n\nlQ: clr\n0.00011890\n0.01090435\n.\n.\n.\n\n\n\n\nlT: WD\n0.00106148\n0.03258038\n.\n.\n.\n.\n\n\nResidual\n\n0.08591587\n0.29311409\n\n\n\n\n\n\n\n\n\n\nMixedModels.likelihoodratiotest(m_zcp2, m_zcp, m_cpx)\n\n\n\n\n\n\n\n\n\n\n\n\n\nmodel-dof\ndeviance\nχ²\nχ²-dof\nP(&gt;χ²)\n\n\n\n\n:(1000 / rt) ~ 1 + F + P + Q + lQ + lT + F & P + F & Q + P & Q + F & lQ + P & lQ + Q & lQ + F & lT + P & lT + Q & lT + lQ & lT + F & P & Q + F & P & lQ + F & Q & lQ + P & Q & lQ + F & P & lT + F & Q & lT + P & Q & lT + F & lQ & lT + P & lQ & lT + Q & lQ & lT + F & P & Q & lQ + F & P & Q & lT + F & P & lQ & lT + F & Q & lQ & lT + P & Q & lQ & lT + F & P & Q & lQ & lT + zerocorr(1 + P + Q + lQ + lT | subj) + zerocorr(1 + P + Q + lT | item)\n42\n7189\n\n\n\n\n\n:(1000 / rt) ~ 1 + F + P + Q + lQ + lT + F & P + F & Q + P & Q + F & lQ + P & lQ + Q & lQ + F & lT + P & lT + Q & lT + lQ & lT + F & P & Q + F & P & lQ + F & Q & lQ + P & Q & lQ + F & P & lT + F & Q & lT + P & Q & lT + F & lQ & lT + P & lQ & lT + Q & lQ & lT + F & P & Q & lQ + F & P & Q & lT + F & P & lQ & lT + F & Q & lQ & lT + P & Q & lQ & lT + F & P & Q & lQ & lT + zerocorr(1 + F + P + Q + lQ + lT | subj) + zerocorr(1 + P + Q + lQ + lT | item)\n44\n7188\n0\n2\n0.9634\n\n\n:(1000 / rt) ~ 1 + F + P + Q + lQ + lT + F & P + F & Q + P & Q + F & lQ + P & lQ + Q & lQ + F & lT + P & lT + Q & lT + lQ & lT + F & P & Q + F & P & lQ + F & Q & lQ + P & Q & lQ + F & P & lT + F & Q & lT + P & Q & lT + F & lQ & lT + P & lQ & lT + Q & lQ & lT + F & P & Q & lQ + F & P & Q & lT + F & P & lQ & lT + F & Q & lQ & lT + P & Q & lQ & lT + F & P & Q & lQ & lT + (1 + F + P + Q + lQ + lT | subj) + (1 + P + Q + lQ + lT | item)\n69\n7148\n40\n25\n0.0259\n\n\n\n\n\nWe can perhaps remove some more.\n\nm_zcp3 = let\n    form = @formula (1000/rt) ~ 1+F*P*Q*lQ*lT +\n                       zerocorr(1    +Q   +lT | subj) + (1 | item);\n    fit(MixedModel, form, dat; contrasts)\nend\nVarCorr(m_zcp3)\n\n\n\n\n\nColumn\nVariance\nStd.Dev\nCorr.\n\n\n\n\n\nitem\n(Intercept)\n0.0032049\n0.0566119\n\n\n\n\nsubj\n(Intercept)\n0.0306171\n0.1749774\n\n\n\n\n\nQ: clr\n0.0007629\n0.0276197\n.\n\n\n\n\nlT: WD\n0.0010645\n0.0326268\n.\n.\n\n\nResidual\n\n0.0864752\n0.2940667\n\n\n\n\n\n\n\n\nMixedModels.likelihoodratiotest(m_zcp3, m_zcp2, m_zcp, m_cpx)\n\n\n\n\n\n\n\n\n\n\n\n\n\nmodel-dof\ndeviance\nχ²\nχ²-dof\nP(&gt;χ²)\n\n\n\n\n:(1000 / rt) ~ 1 + F + P + Q + lQ + lT + F & P + F & Q + P & Q + F & lQ + P & lQ + Q & lQ + F & lT + P & lT + Q & lT + lQ & lT + F & P & Q + F & P & lQ + F & Q & lQ + P & Q & lQ + F & P & lT + F & Q & lT + P & Q & lT + F & lQ & lT + P & lQ & lT + Q & lQ & lT + F & P & Q & lQ + F & P & Q & lT + F & P & lQ & lT + F & Q & lQ & lT + P & Q & lQ & lT + F & P & Q & lQ & lT + zerocorr(1 + Q + lT | subj) + (1 | item)\n37\n7196\n\n\n\n\n\n:(1000 / rt) ~ 1 + F + P + Q + lQ + lT + F & P + F & Q + P & Q + F & lQ + P & lQ + Q & lQ + F & lT + P & lT + Q & lT + lQ & lT + F & P & Q + F & P & lQ + F & Q & lQ + P & Q & lQ + F & P & lT + F & Q & lT + P & Q & lT + F & lQ & lT + P & lQ & lT + Q & lQ & lT + F & P & Q & lQ + F & P & Q & lT + F & P & lQ & lT + F & Q & lQ & lT + P & Q & lQ & lT + F & P & Q & lQ & lT + zerocorr(1 + P + Q + lQ + lT | subj) + zerocorr(1 + P + Q + lT | item)\n42\n7189\n8\n5\n0.1781\n\n\n:(1000 / rt) ~ 1 + F + P + Q + lQ + lT + F & P + F & Q + P & Q + F & lQ + P & lQ + Q & lQ + F & lT + P & lT + Q & lT + lQ & lT + F & P & Q + F & P & lQ + F & Q & lQ + P & Q & lQ + F & P & lT + F & Q & lT + P & Q & lT + F & lQ & lT + P & lQ & lT + Q & lQ & lT + F & P & Q & lQ + F & P & Q & lT + F & P & lQ & lT + F & Q & lQ & lT + P & Q & lQ & lT + F & P & Q & lQ & lT + zerocorr(1 + F + P + Q + lQ + lT | subj) + zerocorr(1 + P + Q + lQ + lT | item)\n44\n7188\n0\n2\n0.9634\n\n\n:(1000 / rt) ~ 1 + F + P + Q + lQ + lT + F & P + F & Q + P & Q + F & lQ + P & lQ + Q & lQ + F & lT + P & lT + Q & lT + lQ & lT + F & P & Q + F & P & lQ + F & Q & lQ + P & Q & lQ + F & P & lT + F & Q & lT + P & Q & lT + F & lQ & lT + P & lQ & lT + Q & lQ & lT + F & P & Q & lQ + F & P & Q & lT + F & P & lQ & lT + F & Q & lQ & lT + P & Q & lQ & lT + F & P & Q & lQ & lT + (1 + F + P + Q + lQ + lT | subj) + (1 + P + Q + lQ + lT | item)\n69\n7148\n40\n25\n0.0259\n\n\n\n\n\nAnd another iteration.\n\nm_zcp4 = let\n    form = @formula (1000/rt) ~ 1+F*P*Q*lQ*lT +\n                       zerocorr(1         +lT | subj) + (1 | item);\n    fit(MixedModel, form, dat; contrasts)\nend\nVarCorr(m_zcp4)\n\n\n\n\n\nColumn\nVariance\nStd.Dev\nCorr.\n\n\n\n\nitem\n(Intercept)\n0.0032067\n0.0566280\n\n\n\nsubj\n(Intercept)\n0.0306601\n0.1751003\n\n\n\n\nlT: WD\n0.0010896\n0.0330098\n.\n\n\nResidual\n\n0.0872330\n0.2953523\n\n\n\n\n\n\n\nMixedModels.likelihoodratiotest(m_zcp4, m_zcp3, m_zcp2, m_zcp, m_cpx)\n\n\n\n\n\n\n\n\n\n\n\n\n\nmodel-dof\ndeviance\nχ²\nχ²-dof\nP(&gt;χ²)\n\n\n\n\n:(1000 / rt) ~ 1 + F + P + Q + lQ + lT + F & P + F & Q + P & Q + F & lQ + P & lQ + Q & lQ + F & lT + P & lT + Q & lT + lQ & lT + F & P & Q + F & P & lQ + F & Q & lQ + P & Q & lQ + F & P & lT + F & Q & lT + P & Q & lT + F & lQ & lT + P & lQ & lT + Q & lQ & lT + F & P & Q & lQ + F & P & Q & lT + F & P & lQ & lT + F & Q & lQ & lT + P & Q & lQ & lT + F & P & Q & lQ & lT + zerocorr(1 + lT | subj) + (1 | item)\n36\n7259\n\n\n\n\n\n:(1000 / rt) ~ 1 + F + P + Q + lQ + lT + F & P + F & Q + P & Q + F & lQ + P & lQ + Q & lQ + F & lT + P & lT + Q & lT + lQ & lT + F & P & Q + F & P & lQ + F & Q & lQ + P & Q & lQ + F & P & lT + F & Q & lT + P & Q & lT + F & lQ & lT + P & lQ & lT + Q & lQ & lT + F & P & Q & lQ + F & P & Q & lT + F & P & lQ & lT + F & Q & lQ & lT + P & Q & lQ & lT + F & P & Q & lQ & lT + zerocorr(1 + Q + lT | subj) + (1 | item)\n37\n7196\n63\n1\n&lt;1e-14\n\n\n:(1000 / rt) ~ 1 + F + P + Q + lQ + lT + F & P + F & Q + P & Q + F & lQ + P & lQ + Q & lQ + F & lT + P & lT + Q & lT + lQ & lT + F & P & Q + F & P & lQ + F & Q & lQ + P & Q & lQ + F & P & lT + F & Q & lT + P & Q & lT + F & lQ & lT + P & lQ & lT + Q & lQ & lT + F & P & Q & lQ + F & P & Q & lT + F & P & lQ & lT + F & Q & lQ & lT + P & Q & lQ & lT + F & P & Q & lQ & lT + zerocorr(1 + P + Q + lQ + lT | subj) + zerocorr(1 + P + Q + lT | item)\n42\n7189\n8\n5\n0.1781\n\n\n:(1000 / rt) ~ 1 + F + P + Q + lQ + lT + F & P + F & Q + P & Q + F & lQ + P & lQ + Q & lQ + F & lT + P & lT + Q & lT + lQ & lT + F & P & Q + F & P & lQ + F & Q & lQ + P & Q & lQ + F & P & lT + F & Q & lT + P & Q & lT + F & lQ & lT + P & lQ & lT + Q & lQ & lT + F & P & Q & lQ + F & P & Q & lT + F & P & lQ & lT + F & Q & lQ & lT + P & Q & lQ & lT + F & P & Q & lQ & lT + zerocorr(1 + F + P + Q + lQ + lT | subj) + zerocorr(1 + P + Q + lQ + lT | item)\n44\n7188\n0\n2\n0.9634\n\n\n:(1000 / rt) ~ 1 + F + P + Q + lQ + lT + F & P + F & Q + P & Q + F & lQ + P & lQ + Q & lQ + F & lT + P & lT + Q & lT + lQ & lT + F & P & Q + F & P & lQ + F & Q & lQ + P & Q & lQ + F & P & lT + F & Q & lT + P & Q & lT + F & lQ & lT + P & lQ & lT + Q & lQ & lT + F & P & Q & lQ + F & P & Q & lT + F & P & lQ & lT + F & Q & lQ & lT + P & Q & lQ & lT + F & P & Q & lQ & lT + (1 + F + P + Q + lQ + lT | subj) + (1 + P + Q + lQ + lT | item)\n69\n7148\n40\n25\n0.0259\n\n\n\n\n\nToo much removed. Stay with m_zcp3, but extend with CPs.\n\nm_prm = let\n    form = @formula (1000/rt) ~ 1+F*P*Q*lQ*lT +\n                               (1+    Q+lT | subj) + (1 | item);\n    fit(MixedModel, form, dat; contrasts)\nend\nVarCorr(m_prm)\n\n┌ Warning: ProgressMeter by default refresh meters with additional information in IJulia via `IJulia.clear_output`, which clears all outputs in the cell. \n│  - To prevent this behaviour, do `ProgressMeter.ijulia_behavior(:append)`. \n│  - To disable this warning message, do `ProgressMeter.ijulia_behavior(:clear)`.\n└ @ ProgressMeter ~/.julia/packages/ProgressMeter/dMfiC/src/ProgressMeter.jl:594\nMinimizing 250    Time: 0:00:00 ( 0.46 ms/it)\n\n\n\n\n\n\nColumn\nVariance\nStd.Dev\nCorr.\n\n\n\n\n\nitem\n(Intercept)\n0.0032010\n0.0565777\n\n\n\n\nsubj\n(Intercept)\n0.0306222\n0.1749919\n\n\n\n\n\nQ: clr\n0.0007626\n0.0276156\n+0.42\n\n\n\n\nlT: WD\n0.0010621\n0.0325896\n+0.26\n+0.38\n\n\nResidual\n\n0.0864769\n0.2940695\n\n\n\n\n\n\n\n\n6.1.1 post-hoc LMM\n\nm_prm = let\n    form = @formula (1000/rt) ~ 1+F*P*Q*lQ*lT +\n                               (1+    Q+lT | subj) + (1 | item);\n    fit(MixedModel, form, dat; contrasts)\nend\nVarCorr(m_prm)\n\n┌ Warning: ProgressMeter by default refresh meters with additional information in IJulia via `IJulia.clear_output`, which clears all outputs in the cell. \n│  - To prevent this behaviour, do `ProgressMeter.ijulia_behavior(:append)`. \n│  - To disable this warning message, do `ProgressMeter.ijulia_behavior(:clear)`.\n└ @ ProgressMeter ~/.julia/packages/ProgressMeter/dMfiC/src/ProgressMeter.jl:594\nMinimizing 250    Time: 0:00:00 ( 0.47 ms/it)\n\n\n\n\n\n\nColumn\nVariance\nStd.Dev\nCorr.\n\n\n\n\n\nitem\n(Intercept)\n0.0032010\n0.0565777\n\n\n\n\nsubj\n(Intercept)\n0.0306222\n0.1749919\n\n\n\n\n\nQ: clr\n0.0007626\n0.0276156\n+0.42\n\n\n\n\nlT: WD\n0.0010621\n0.0325896\n+0.26\n+0.38\n\n\nResidual\n\n0.0864769\n0.2940695",
    "crumbs": [
      "Worked examples",
      "RePsychLing Masson, Rabe, & Kliegl, 2017) with Julia: Specification and selection"
    ]
  },
  {
    "objectID": "mrk17.html#vcs-and-cps-1",
    "href": "mrk17.html#vcs-and-cps-1",
    "title": "RePsychLing Masson, Rabe, & Kliegl, 2017) with Julia: Specification and selection",
    "section": "6.2 VCs and CPs",
    "text": "6.2 VCs and CPs\n\nMixedModels.likelihoodratiotest(m_zcp3, m_prm, m_cpx)\n\n\n\n\n\n\n\n\n\n\n\n\n\nmodel-dof\ndeviance\nχ²\nχ²-dof\nP(&gt;χ²)\n\n\n\n\n:(1000 / rt) ~ 1 + F + P + Q + lQ + lT + F & P + F & Q + P & Q + F & lQ + P & lQ + Q & lQ + F & lT + P & lT + Q & lT + lQ & lT + F & P & Q + F & P & lQ + F & Q & lQ + P & Q & lQ + F & P & lT + F & Q & lT + P & Q & lT + F & lQ & lT + P & lQ & lT + Q & lQ & lT + F & P & Q & lQ + F & P & Q & lT + F & P & lQ & lT + F & Q & lQ & lT + P & Q & lQ & lT + F & P & Q & lQ & lT + zerocorr(1 + Q + lT | subj) + (1 | item)\n37\n7196\n\n\n\n\n\n:(1000 / rt) ~ 1 + F + P + Q + lQ + lT + F & P + F & Q + P & Q + F & lQ + P & lQ + Q & lQ + F & lT + P & lT + Q & lT + lQ & lT + F & P & Q + F & P & lQ + F & Q & lQ + P & Q & lQ + F & P & lT + F & Q & lT + P & Q & lT + F & lQ & lT + P & lQ & lT + Q & lQ & lT + F & P & Q & lQ + F & P & Q & lT + F & P & lQ & lT + F & Q & lQ & lT + P & Q & lQ & lT + F & P & Q & lQ & lT + (1 + Q + lT | subj) + (1 | item)\n40\n7180\n16\n3\n0.0011\n\n\n:(1000 / rt) ~ 1 + F + P + Q + lQ + lT + F & P + F & Q + P & Q + F & lQ + P & lQ + Q & lQ + F & lT + P & lT + Q & lT + lQ & lT + F & P & Q + F & P & lQ + F & Q & lQ + P & Q & lQ + F & P & lT + F & Q & lT + P & Q & lT + F & lQ & lT + P & lQ & lT + Q & lQ & lT + F & P & Q & lQ + F & P & Q & lT + F & P & lQ & lT + F & Q & lQ & lT + P & Q & lQ & lT + F & P & Q & lQ & lT + (1 + F + P + Q + lQ + lT | subj) + (1 + P + Q + lQ + lT | item)\n69\n7148\n32\n29\n0.3103\n\n\n\n\n\nThe LRT favors the complex LMM, but not that χ² &lt; 2*(χ²-dof); AIC and BIC suggest against selection.\n\ngof_summary = let\n  nms = [:m_zcp3, :m_prm, :m_cpx]\n  mods = eval.(nms)\n  lrt = MixedModels.likelihoodratiotest(m_zcp3, m_prm, m_cpx)\n  DataFrame(;\n    name = nms,\n    dof=dof.(mods),\n    deviance=round.(deviance.(mods), digits=0),\n    AIC=round.(aic.(mods),digits=0),\n    AICc=round.(aicc.(mods),digits=0),\n    BIC=round.(bic.(mods),digits=0),\n    χ²=vcat(:., round.(lrt.tests.deviancediff, digits=0)),\n    χ²_dof=vcat(:., round.(lrt.tests.dofdiff, digits=0)),\n    pvalue=vcat(:., round.(lrt.tests.pvalues, digits=3))\n  )\nend\n\n3×9 DataFrame\n\n\n\nRow\nname\ndof\ndeviance\nAIC\nAICc\nBIC\nχ²\nχ²_dof\npvalue\n\n\n\nSymbol\nInt64\nFloat64\nFloat64\nFloat64\nFloat64\nAny\nAny\nAny\n\n\n\n\n1\nm_zcp3\n37\n7196.0\n7270.0\n7270.0\n7555.0\n.\n.\n.\n\n\n2\nm_prm\n40\n7180.0\n7260.0\n7260.0\n7568.0\n16.0\n3.0\n0.001\n\n\n3\nm_cpx\n69\n7148.0\n7286.0\n7287.0\n7818.0\n32.0\n29.0\n0.31",
    "crumbs": [
      "Worked examples",
      "RePsychLing Masson, Rabe, & Kliegl, 2017) with Julia: Specification and selection"
    ]
  },
  {
    "objectID": "mrk17.html#crossed-fixed-effects",
    "href": "mrk17.html#crossed-fixed-effects",
    "title": "RePsychLing Masson, Rabe, & Kliegl, 2017) with Julia: Specification and selection",
    "section": "7.1 Crossed fixed effects",
    "text": "7.1 Crossed fixed effects\n\nm_mrk17_crossed =let\n   form = @formula (1000/rt) ~ 1 + F*P*Q*lQ*lT +\n        (1+Q | subj) + zerocorr(0+lT | subj) + zerocorr(1 + P | item) ;\n    fit(MixedModel, form, dat; contrasts)\nend\n\nVarCorr(m_prm)\n\n┌ Warning: ProgressMeter by default refresh meters with additional information in IJulia via `IJulia.clear_output`, which clears all outputs in the cell. \n│  - To prevent this behaviour, do `ProgressMeter.ijulia_behavior(:append)`. \n│  - To disable this warning message, do `ProgressMeter.ijulia_behavior(:clear)`.\n└ @ ProgressMeter ~/.julia/packages/ProgressMeter/dMfiC/src/ProgressMeter.jl:594\nMinimizing 189    Time: 0:00:00 ( 1.03 ms/it)\n\n\n\n\n\n\nColumn\nVariance\nStd.Dev\nCorr.\n\n\n\n\n\nitem\n(Intercept)\n0.0032010\n0.0565777\n\n\n\n\nsubj\n(Intercept)\n0.0306222\n0.1749919\n\n\n\n\n\nQ: clr\n0.0007626\n0.0276156\n+0.42\n\n\n\n\nlT: WD\n0.0010621\n0.0325896\n+0.26\n+0.38\n\n\nResidual\n\n0.0864769\n0.2940695\n\n\n\n\n\n\n\n\nshow(m_mrk17_crossed)\n\nLinear mixed model fit by maximum likelihood\n :(1000 / rt) ~ 1 + F + P + Q + lQ + lT + F & P + F & Q + P & Q + F & lQ + P & lQ + Q & lQ + F & lT + P & lT + Q & lT + lQ & lT + F & P & Q + F & P & lQ + F & Q & lQ + P & Q & lQ + F & P & lT + F & Q & lT + P & Q & lT + F & lQ & lT + P & lQ & lT + Q & lQ & lT + F & P & Q & lQ + F & P & Q & lT + F & P & lQ & lT + F & Q & lQ & lT + P & Q & lQ & lT + F & P & Q & lQ & lT + (1 + Q | subj) + zerocorr(0 + lT | subj) + zerocorr(1 + P | item)\n   logLik   -2 logLik     AIC       AICc        BIC    \n -3593.4086  7186.8171  7264.8171  7265.0077  7565.3349\n\nVariance components:\n            Column    Variance   Std.Dev.    Corr.\nitem     (Intercept)  0.00320568 0.05661871\n         P: rel       0.00006693 0.00818092   .  \nsubj     (Intercept)  0.03061748 0.17497850\n         Q: clr       0.00076248 0.02761305 +0.42\n         lT: WD       0.00106208 0.03258964   .     .  \nResidual              0.08640800 0.29395239\n Number of obs: 16409; levels of grouping factors: 240, 73\n\n  Fixed-effects parameters:\n─────────────────────────────────────────────────────────────────────────────────────\n                                                   Coef.  Std. Error      z  Pr(&gt;|z|)\n─────────────────────────────────────────────────────────────────────────────────────\n(Intercept)                                  1.63743      0.0209303   78.23    &lt;1e-99\nF: HF                                        0.019366     0.00431955   4.48    &lt;1e-05\nP: rel                                       0.0188081    0.00236113   7.97    &lt;1e-14\nQ: clr                                       0.042901     0.00396827  10.81    &lt;1e-26\nlQ: clr                                      0.00174653   0.00231531   0.75    0.4506\nlT: WD                                       0.00836023   0.00446181   1.87    0.0610\nF: HF & P: rel                              -0.00711249   0.00236171  -3.01    0.0026\nF: HF & Q: clr                              -0.00141259   0.00230067  -0.61    0.5392\nP: rel & Q: clr                              0.00134142   0.00230239   0.58    0.5601\nF: HF & lQ: clr                             -0.00105227   0.00232139  -0.45    0.6503\nP: rel & lQ: clr                            -0.00240743   0.00232056  -1.04    0.2995\nQ: clr & lQ: clr                             0.00759804   0.00231922   3.28    0.0011\nF: HF & lT: WD                               0.000536555  0.00231365   0.23    0.8166\nP: rel & lT: WD                              2.93227e-5   0.00231751   0.01    0.9899\nQ: clr & lT: WD                              0.00185532   0.00231773   0.80    0.4234\nlQ: clr & lT: WD                            -0.00524632   0.00231608  -2.27    0.0235\nF: HF & P: rel & Q: clr                     -0.000362238  0.00230272  -0.16    0.8750\nF: HF & P: rel & lQ: clr                    -0.00117754   0.00232025  -0.51    0.6118\nF: HF & Q: clr & lQ: clr                     0.00273699   0.00232283   1.18    0.2387\nP: rel & Q: clr & lQ: clr                   -0.0039323    0.0023222   -1.69    0.0904\nF: HF & P: rel & lT: WD                      0.0019864    0.00231879   0.86    0.3916\nF: HF & Q: clr & lT: WD                     -0.00112586   0.00231657  -0.49    0.6270\nP: rel & Q: clr & lT: WD                     0.000261826  0.00231924   0.11    0.9101\nF: HF & lQ: clr & lT: WD                     0.00150677   0.00232198   0.65    0.5164\nP: rel & lQ: clr & lT: WD                    8.67964e-5   0.00232177   0.04    0.9702\nQ: clr & lQ: clr & lT: WD                    0.00916751   0.00231827   3.95    &lt;1e-04\nF: HF & P: rel & Q: clr & lQ: clr           -0.00234443   0.00231978  -1.01    0.3122\nF: HF & P: rel & Q: clr & lT: WD            -0.00148721   0.00232057  -0.64    0.5216\nF: HF & P: rel & lQ: clr & lT: WD            0.00308764   0.00232145   1.33    0.1835\nF: HF & Q: clr & lQ: clr & lT: WD            0.00393598   0.00232128   1.70    0.0900\nP: rel & Q: clr & lQ: clr & lT: WD           0.00202623   0.00232125   0.87    0.3827\nF: HF & P: rel & Q: clr & lQ: clr & lT: WD   0.00144888   0.00231838   0.62    0.5320\n─────────────────────────────────────────────────────────────────────────────────────\n\n\nFinally, a look at the fixed effects. The four-factor interaction reported in Masson & Kliegl (2013) was not replicated.",
    "crumbs": [
      "Worked examples",
      "RePsychLing Masson, Rabe, & Kliegl, 2017) with Julia: Specification and selection"
    ]
  },
  {
    "objectID": "mrk17.html#nested-fixed-effects",
    "href": "mrk17.html#nested-fixed-effects",
    "title": "RePsychLing Masson, Rabe, & Kliegl, 2017) with Julia: Specification and selection",
    "section": "7.2 Nested fixed effects",
    "text": "7.2 Nested fixed effects\n\nm_mrk17_nested =let\n   form = @formula (1000/rt) ~ 1 + Q/(F/P) +\n        (1+Q | subj) + zerocorr(0+lT | subj) + zerocorr(1 + P | item) ;\n    fit(MixedModel, form, dat; contrasts)\nend\n\n┌ Warning: ProgressMeter by default refresh meters with additional information in IJulia via `IJulia.clear_output`, which clears all outputs in the cell. \n│  - To prevent this behaviour, do `ProgressMeter.ijulia_behavior(:append)`. \n│  - To disable this warning message, do `ProgressMeter.ijulia_behavior(:clear)`.\n└ @ ProgressMeter ~/.julia/packages/ProgressMeter/dMfiC/src/ProgressMeter.jl:594\nMinimizing 176    Time: 0:00:00 ( 0.66 ms/it)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEst.\nSE\nz\np\nσ_item\nσ_subj\n\n\n\n\n(Intercept)\n1.6374\n0.0209\n78.20\n&lt;1e-99\n0.0565\n0.1751\n\n\nQ: clr\n0.0428\n0.0040\n10.74\n&lt;1e-26\n\n0.0278\n\n\nQ: deg & F: HF\n0.0209\n0.0049\n4.26\n&lt;1e-04\n\n\n\n\nQ: clr & F: HF\n0.0179\n0.0049\n3.65\n0.0003\n\n\n\n\nQ: deg & F: LF & P: rel\n0.0244\n0.0047\n5.18\n&lt;1e-06\n\n\n\n\nQ: clr & F: LF & P: rel\n0.0278\n0.0047\n5.94\n&lt;1e-08\n\n\n\n\nQ: deg & F: HF & P: rel\n0.0110\n0.0047\n2.35\n0.0186\n\n\n\n\nQ: clr & F: HF & P: rel\n0.0126\n0.0046\n2.71\n0.0067\n\n\n\n\nP: rel\n\n\n\n\n0.0086\n\n\n\nlT: WD\n\n\n\n\n\n0.0337\n\n\nResidual\n0.2944",
    "crumbs": [
      "Worked examples",
      "RePsychLing Masson, Rabe, & Kliegl, 2017) with Julia: Specification and selection"
    ]
  },
  {
    "objectID": "mrk17.html#nesting-within-products-of-factors",
    "href": "mrk17.html#nesting-within-products-of-factors",
    "title": "RePsychLing Masson, Rabe, & Kliegl, 2017) with Julia: Specification and selection",
    "section": "8.1 Nesting within products of factors",
    "text": "8.1 Nesting within products of factors\nInclude parenthesis\n\nm_mrk17_nested =let\n   form = @formula (1000/rt) ~ 1 + Q/(F/P) +\n        (1+Q | subj) + zerocorr(0+lT | subj) + zerocorr(1 + P | item) ;\n    fit(MixedModel, form, dat; contrasts)\nend\n\n┌ Warning: ProgressMeter by default refresh meters with additional information in IJulia via `IJulia.clear_output`, which clears all outputs in the cell. \n│  - To prevent this behaviour, do `ProgressMeter.ijulia_behavior(:append)`. \n│  - To disable this warning message, do `ProgressMeter.ijulia_behavior(:clear)`.\n└ @ ProgressMeter ~/.julia/packages/ProgressMeter/dMfiC/src/ProgressMeter.jl:594\nMinimizing 176    Time: 0:00:00 ( 0.64 ms/it)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEst.\nSE\nz\np\nσ_item\nσ_subj\n\n\n\n\n(Intercept)\n1.6374\n0.0209\n78.20\n&lt;1e-99\n0.0565\n0.1751\n\n\nQ: clr\n0.0428\n0.0040\n10.74\n&lt;1e-26\n\n0.0278\n\n\nQ: deg & F: HF\n0.0209\n0.0049\n4.26\n&lt;1e-04\n\n\n\n\nQ: clr & F: HF\n0.0179\n0.0049\n3.65\n0.0003\n\n\n\n\nQ: deg & F: LF & P: rel\n0.0244\n0.0047\n5.18\n&lt;1e-06\n\n\n\n\nQ: clr & F: LF & P: rel\n0.0278\n0.0047\n5.94\n&lt;1e-08\n\n\n\n\nQ: deg & F: HF & P: rel\n0.0110\n0.0047\n2.35\n0.0186\n\n\n\n\nQ: clr & F: HF & P: rel\n0.0126\n0.0046\n2.71\n0.0067\n\n\n\n\nP: rel\n\n\n\n\n0.0086\n\n\n\nlT: WD\n\n\n\n\n\n0.0337\n\n\nResidual\n0.2944",
    "crumbs": [
      "Worked examples",
      "RePsychLing Masson, Rabe, & Kliegl, 2017) with Julia: Specification and selection"
    ]
  },
  {
    "objectID": "mrk17.html#selection-in-fixed-effects",
    "href": "mrk17.html#selection-in-fixed-effects",
    "title": "RePsychLing Masson, Rabe, & Kliegl, 2017) with Julia: Specification and selection",
    "section": "8.2 Selection in fixed effects",
    "text": "8.2 Selection in fixed effects\n\nusing RegressionFormulae\n# m_prm_5 is equivalent to m_prm\nm_prm_5 = let\n    form = @formula (1000/rt) ~ 1+(F+P+Q+lQ+lT)^5 + (1+Q+lT | subj) + (1 | item);\n    fit(MixedModel, form, dat; contrasts)\nend;\n\nm_prm_4 = let\n    form = @formula (1000/rt) ~ 1+(F+P+Q+lQ+lT)^4 + (1+Q+lT | subj) + (1 | item);\n    fit(MixedModel, form, dat; contrasts)\nend;\n\nm_prm_3 = let\n    form = @formula (1000/rt) ~ 1+(F+P+Q+lQ+lT)^3 + (1+Q+lT | subj) + (1 | item);\n    fit(MixedModel, form, dat; contrasts)\nend;\n\nm_prm_2 = let\n    form = @formula (1000/rt) ~ 1+(F+P+Q+lQ+lT)^2 + (1+Q+lT | subj) + (1 | item);\n    fit(MixedModel, form, dat; contrasts)\nend;\n\nm_prm_1 = let\n    form = @formula (1000/rt) ~ 1+ F+P+Q+lQ+lT +   (1+Q+lT | subj) + (1 | item);\n    fit(MixedModel, form, dat; contrasts)\nend;\n\n# Compare the fits\ngof_summary = let\n  nms = [:m_prm_1, :m_prm_2, :m_prm_3, :m_prm_4, :m_prm_5]\n  mods = eval.(nms)\n  lrt = MixedModels.likelihoodratiotest(m_prm_1, m_prm_2, m_prm_3, m_prm_4, m_prm_5)\n  DataFrame(;\n    name = nms,\n    dof=dof.(mods),\n    deviance=deviance.(mods),\n    AIC=aic.(mods),\n    AICc=aicc.(mods),\n    BIC=bic.(mods),\n    χ²=vcat(:.,lrt.tests.deviancediff),\n    χ²_dof=vcat(:.,lrt.tests.dofdiff),\n    pvalue=vcat(:., round.(lrt.tests.pvalues, digits=3))\n  )\nend\n\n┌ Warning: ProgressMeter by default refresh meters with additional information in IJulia via `IJulia.clear_output`, which clears all outputs in the cell. \n│  - To prevent this behaviour, do `ProgressMeter.ijulia_behavior(:append)`. \n│  - To disable this warning message, do `ProgressMeter.ijulia_behavior(:clear)`.\n└ @ ProgressMeter ~/.julia/packages/ProgressMeter/dMfiC/src/ProgressMeter.jl:594\nMinimizing 240    Time: 0:00:00 ( 0.43 ms/it)\n\n\n5×9 DataFrame\n\n\n\nRow\nname\ndof\ndeviance\nAIC\nAICc\nBIC\nχ²\nχ²_dof\npvalue\n\n\n\nSymbol\nInt64\nFloat64\nFloat64\nFloat64\nFloat64\nAny\nAny\nAny\n\n\n\n\n1\nm_prm_1\n14\n7237.39\n7265.39\n7265.41\n7373.27\n.\n.\n.\n\n\n2\nm_prm_2\n24\n7209.2\n7257.2\n7257.27\n7442.13\n28.1868\n10\n0.002\n\n\n3\nm_prm_3\n34\n7187.57\n7255.57\n7255.71\n7517.56\n21.6317\n10\n0.017\n\n\n4\nm_prm_4\n39\n7180.61\n7258.61\n7258.8\n7559.13\n6.95863\n5\n0.224\n\n\n5\nm_prm_5\n40\n7180.21\n7260.21\n7260.41\n7568.44\n0.398591\n1\n0.528\n\n\n\n\n\n\nDepending on the level of precision of your hypothesis. You could stay with main effect (BIC), include 2-factor interactions (AIC; also called simple interactions) or include 3-factor interactions [χ² &lt; 2*(χ²-dof); also called 2-way interactions].",
    "crumbs": [
      "Worked examples",
      "RePsychLing Masson, Rabe, & Kliegl, 2017) with Julia: Specification and selection"
    ]
  },
  {
    "objectID": "mrk17.html#posthoc-lmm",
    "href": "mrk17.html#posthoc-lmm",
    "title": "RePsychLing Masson, Rabe, & Kliegl, 2017) with Julia: Specification and selection",
    "section": "8.3 Posthoc LMM",
    "text": "8.3 Posthoc LMM\nWe are using only three factors for the illustruation.\n\nm_prm3 = let\n    form = @formula (1000/rt) ~ 1 + lT*lQ*Q +\n                               (1+    Q+lT | subj) + (1 | item);\n    fit(MixedModel, form, dat; contrasts)\nend\n\n┌ Warning: ProgressMeter by default refresh meters with additional information in IJulia via `IJulia.clear_output`, which clears all outputs in the cell. \n│  - To prevent this behaviour, do `ProgressMeter.ijulia_behavior(:append)`. \n│  - To disable this warning message, do `ProgressMeter.ijulia_behavior(:clear)`.\n└ @ ProgressMeter ~/.julia/packages/ProgressMeter/dMfiC/src/ProgressMeter.jl:594\nMinimizing 282    Time: 0:00:00 ( 0.46 ms/it)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEst.\nSE\nz\np\nσ_item\nσ_subj\n\n\n\n\n(Intercept)\n1.6376\n0.0210\n78.10\n&lt;1e-99\n0.0596\n0.1750\n\n\nlT: WD\n0.0087\n0.0045\n1.93\n0.0530\n\n0.0328\n\n\nlQ: clr\n0.0016\n0.0023\n0.70\n0.4818\n\n\n\n\nQ: clr\n0.0428\n0.0040\n10.78\n&lt;1e-26\n\n0.0276\n\n\nlT: WD & lQ: clr\n-0.0056\n0.0023\n-2.39\n0.0167\n\n\n\n\nlT: WD & Q: clr\n0.0020\n0.0023\n0.87\n0.3862\n\n\n\n\nlQ: clr & Q: clr\n0.0076\n0.0023\n3.26\n0.0011\n\n\n\n\nlT: WD & lQ: clr & Q: clr\n0.0092\n0.0023\n3.94\n&lt;1e-04\n\n\n\n\nResidual\n0.2949\n\n\n\n\n\n\n\n\n\n\nThe lT & lQ & Q interactions is significant. Let’s follow it up with a post-hoc LMM, that is looking at the lQ & Q interaction in the two levels of whether the last word was a target or not.\n\nm_prm3_posthoc = let\n    form = @formula (1000/rt) ~ 1 + lT/(lQ*Q) +\n                               (1+    Q+lT | subj) + (1 | item);\n    fit(MixedModel, form, dat; contrasts)\nend\n\n┌ Warning: ProgressMeter by default refresh meters with additional information in IJulia via `IJulia.clear_output`, which clears all outputs in the cell. \n│  - To prevent this behaviour, do `ProgressMeter.ijulia_behavior(:append)`. \n│  - To disable this warning message, do `ProgressMeter.ijulia_behavior(:clear)`.\n└ @ ProgressMeter ~/.julia/packages/ProgressMeter/dMfiC/src/ProgressMeter.jl:594\nMinimizing 264    Time: 0:00:00 ( 0.46 ms/it)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEst.\nSE\nz\np\nσ_item\nσ_subj\n\n\n\n\n(Intercept)\n1.6376\n0.0210\n78.10\n&lt;1e-99\n0.0596\n0.1750\n\n\nlT: WD\n0.0087\n0.0045\n1.93\n0.0530\n\n0.0328\n\n\nlT: NW & lQ: clr\n0.0072\n0.0033\n2.19\n0.0285\n\n\n\n\nlT: WD & lQ: clr\n-0.0039\n0.0033\n-1.19\n0.2326\n\n\n\n\nlT: NW & Q: clr\n0.0408\n0.0046\n8.87\n&lt;1e-18\n\n\n\n\nlT: WD & Q: clr\n0.0448\n0.0046\n9.73\n&lt;1e-21\n\n\n\n\nlT: NW & lQ: clr & Q: clr\n-0.0016\n0.0033\n-0.49\n0.6275\n\n\n\n\nlT: WD & lQ: clr & Q: clr\n0.0167\n0.0033\n5.08\n&lt;1e-06\n\n\n\n\nQ: clr\n\n\n\n\n\n0.0276\n\n\nResidual\n0.2949\n\n\n\n\n\n\n\n\n\n\nThe source of the interaction are trials where the last trial was a word target; there is no evidence for the interaction when the last trial was a nonword target.\nThe original and post-hoc LMM have the same goodness of fit.\n\n[objective(m_prm3), objective(m_prm3_posthoc)]\n\n2-element Vector{Float64}:\n 7291.14543745544\n 7291.145437448449",
    "crumbs": [
      "Worked examples",
      "RePsychLing Masson, Rabe, & Kliegl, 2017) with Julia: Specification and selection"
    ]
  },
  {
    "objectID": "mrk17.html#info",
    "href": "mrk17.html#info",
    "title": "RePsychLing Masson, Rabe, & Kliegl, 2017) with Julia: Specification and selection",
    "section": "8.4 Info",
    "text": "8.4 Info\n\nversioninfo()\n\nJulia Version 1.9.4\nCommit 8e5136fa297 (2023-11-14 08:46 UTC)\nBuild Info:\n  Official https://julialang.org/ release\nPlatform Info:\n  OS: Linux (x86_64-linux-gnu)\n  CPU: 16 × Intel(R) Xeon(R) E-2288G CPU @ 3.70GHz\n  WORD_SIZE: 64\n  LIBM: libopenlibm\n  LLVM: libLLVM-14.0.6 (ORCJIT, skylake)\n  Threads: 17 on 16 virtual cores",
    "crumbs": [
      "Worked examples",
      "RePsychLing Masson, Rabe, & Kliegl, 2017) with Julia: Specification and selection"
    ]
  },
  {
    "objectID": "kwdyz11.html",
    "href": "kwdyz11.html",
    "title": "RePsychLing Kliegl et al. (2011)",
    "section": "",
    "text": "We take the kwdyz11 dataset (Kliegl et al., 2011) from an experiment looking at three effects of visual cueing under four different cue-target relations (CTRs). Two horizontal rectangles are displayed above and below a central fixation point or they displayed in vertical orientation to the left and right of the fixation point. Subjects react to the onset of a small visual target occurring at one of the four ends of the two rectangles. The target is cued validly on 70% of trials by a brief flash of the corner of the rectangle at which it appears; it is cued invalidly at the three other locations 10% of the trials each.\nWe specify three contrasts for the four-level factor CTR that are derived from spatial, object-based, and attractor-like features of attention. They map onto sequential differences between appropriately ordered factor levels. At the level of fixed effects, there is the noteworthy result, that the attraction effect was estimated at 2 ms, that is clearly not significant. Nevertheless, there was a highly reliable variance component (VC) estimated for this effect. Moreover, the reliable individual differences in the attraction effect were negatively correlated with those in the spatial effect.\nUnfortunately, a few years after the publication, we determined that the reported LMM is actually singular and that the singularity is linked to a theoretically critical correlation parameter (CP) between the spatial effect and the attraction effect. Fortunately, there is also a larger dataset kkl15 from a replication and extension of this study (Kliegl et al., 2015), analyzed with kkl15.jl notebook. The critical CP (along with other fixed effects and CPs) was replicated in this study.\nA more comprehensive analysis was reported in the parsimonious mixed-model paper (Bates et al., 2015). Data and R scripts are also available in R-package RePsychLing. In this and the complementary kkl15.jl scripts, we provide some corresponding analyses with MixedModels.jl.",
    "crumbs": [
      "Worked examples",
      "RePsychLing Kliegl et al. (2011)"
    ]
  },
  {
    "objectID": "kwdyz11.html#residual-over-fitted-plot",
    "href": "kwdyz11.html#residual-over-fitted-plot",
    "title": "RePsychLing Kliegl et al. (2011)",
    "section": "5.1 Residual-over-fitted plot",
    "text": "5.1 Residual-over-fitted plot\nThe slant in residuals show a lower and upper boundary of reaction times, that is we have have too few short and too few long residuals. Not ideal, but at least width of the residual band looks similar across the fitted values, that is there is no evidence for heteroskedasticity.\n\n\nCode\nCairoMakie.activate!(; type=\"png\")\nset_aog_theme!()\ndraw(\n  data((; f=fitted(m1), r=residuals(m1))) *\n  mapping(:f =&gt; \"Fitted values\", :r =&gt; \"Residual from model m1\") *\n  visual(Scatter);\n)\n\n\n\n\n\n\n\n\nFigure 3: Residuals versus the fitted values for model m1 of the log response time.\n\n\n\n\n\nWith many observations the scatterplot is not that informative. Contour plots or heatmaps may be an alternative.\n\n\nCode\nCairoMakie.activate!(; type=\"png\")\ndraw(\n  data((; f=fitted(m1), r=residuals(m1))) *\n  mapping(\n    :f =&gt; \"Fitted log response time\", :r =&gt; \"Residual from model m1\"\n  ) *\n  density();\n)\n\n\n\n\n\n\n\n\nFigure 4: Heatmap of residuals versus fitted values for model m1",
    "crumbs": [
      "Worked examples",
      "RePsychLing Kliegl et al. (2011)"
    ]
  },
  {
    "objectID": "kwdyz11.html#q-q-plot",
    "href": "kwdyz11.html#q-q-plot",
    "title": "RePsychLing Kliegl et al. (2011)",
    "section": "5.2 Q-Q plot",
    "text": "5.2 Q-Q plot\nThe plot of quantiles of model residuals over corresponding quantiles of the normal distribution should yield a straight line along the main diagonal.\n\nqqnorm(residuals(m1); qqline=:none)",
    "crumbs": [
      "Worked examples",
      "RePsychLing Kliegl et al. (2011)"
    ]
  },
  {
    "objectID": "kwdyz11.html#observed-and-theoretical-normal-distribution",
    "href": "kwdyz11.html#observed-and-theoretical-normal-distribution",
    "title": "RePsychLing Kliegl et al. (2011)",
    "section": "5.3 Observed and theoretical normal distribution",
    "text": "5.3 Observed and theoretical normal distribution\nThe violation of expectation is again due to the fact that the distribution of residuals is much narrower than expected from a normal distribution, as shown in Figure 5. Overall, it does not look too bad.\n\n\nCode\nlet\n  n = nrow(dat)\n  dat_rz = DataFrame(;\n    value=vcat(residuals(m1) ./ std(residuals(m1)), randn(n)),\n    curve=vcat(fill.(\"residual\", n), fill.(\"normal\", n)),\n  )\n  draw(\n    data(dat_rz) *\n    mapping(:value =&gt; \"Standardized residuals\"; color=:curve) *\n    density(; bandwidth=0.1);\n  )\nend\n\n\n\n\n\n\n\n\nFigure 5: Kernel density plot of the standardized residuals from model m1 compared to a Gaussian",
    "crumbs": [
      "Worked examples",
      "RePsychLing Kliegl et al. (2011)"
    ]
  },
  {
    "objectID": "kwdyz11.html#overlay",
    "href": "kwdyz11.html#overlay",
    "title": "RePsychLing Kliegl et al. (2011)",
    "section": "6.1 Overlay",
    "text": "6.1 Overlay\nThe first plot overlays shrinkage-corrected conditional modes of the random effects with within-subject-based and pooled GMs and experimental effects.\nTo be done",
    "crumbs": [
      "Worked examples",
      "RePsychLing Kliegl et al. (2011)"
    ]
  },
  {
    "objectID": "kwdyz11.html#caterpillar-plot",
    "href": "kwdyz11.html#caterpillar-plot",
    "title": "RePsychLing Kliegl et al. (2011)",
    "section": "6.2 Caterpillar plot",
    "text": "6.2 Caterpillar plot\nThe caterpillar plot, Figure 6, also reveals the high correlation between spatial sod and attraction dod effects.\n\n\nCode\ncaterpillar!(\n  Figure(; resolution=(800, 1000)), ranefinfo(m1, :Subj); orderby=2\n)\n\n\n\n\n\n\n\n\nFigure 6: Prediction intervals on the random effects for Subj in model m1",
    "crumbs": [
      "Worked examples",
      "RePsychLing Kliegl et al. (2011)"
    ]
  },
  {
    "objectID": "kwdyz11.html#shrinkage-plot",
    "href": "kwdyz11.html#shrinkage-plot",
    "title": "RePsychLing Kliegl et al. (2011)",
    "section": "6.3 Shrinkage plot",
    "text": "6.3 Shrinkage plot\nFigure 7 provides more evidence for a problem with the visualization of the spatial sod and attraction dod CP. The corresponding panel illustrates an implosion of conditional modes.\n\n\nCode\nshrinkageplot!(Figure(; resolution=(1000, 1000)), m1)\n\n\n\n\n\n\n\n\nFigure 7: Shrinkage plot of the conditional means of the random effects for model m1",
    "crumbs": [
      "Worked examples",
      "RePsychLing Kliegl et al. (2011)"
    ]
  },
  {
    "objectID": "kwdyz11.html#generate-a-bootstrap-sample",
    "href": "kwdyz11.html#generate-a-bootstrap-sample",
    "title": "RePsychLing Kliegl et al. (2011)",
    "section": "7.1 Generate a bootstrap sample",
    "text": "7.1 Generate a bootstrap sample\nWe generate 2500 samples for the 15 model parameters (4 fixed effect, 4 VCs, 6 CPs, and 1 residual).\n\n\nCode\nRandom.seed!(1234321)\nsamp = parametricbootstrap(2500, m1)\ntbl = samp.tbl\n\n\nTable with 26 columns and 2500 rows:\n      obj       β1       β2         β3         β4            σ         ⋯\n    ┌───────────────────────────────────────────────────────────────────\n 1  │ -12802.3  5.93392  0.0864217  0.048887   -0.0121824    0.192002  ⋯\n 2  │ -12794.3  5.95776  0.0989839  0.0318752  -0.00666179   0.191977  ⋯\n 3  │ -12930.6  5.93669  0.088561   0.0209803  -0.000658234  0.191496  ⋯\n 4  │ -12946.5  5.93165  0.0767128  0.0343559  -0.00499475   0.191438  ⋯\n 5  │ -12677.5  5.91938  0.0984591  0.0272961  -0.00555834   0.19247   ⋯\n 6  │ -12869.7  5.96313  0.104457   0.0321629  -0.00719225   0.191751  ⋯\n 7  │ -12478.1  5.93281  0.0947864  0.0409705  -0.0046158    0.193076  ⋯\n 8  │ -12463.7  5.94439  0.0977164  0.0321099  -0.0062546    0.193115  ⋯\n 9  │ -12836.2  5.9728   0.0971351  0.0416612  6.2017e-5     0.191843  ⋯\n 10 │ -12932.3  5.94709  0.0742454  0.0505263  -0.0131451    0.191583  ⋯\n 11 │ -12736.2  5.93625  0.0728173  0.047942   -0.0183152    0.192193  ⋯\n 12 │ -13048.0  5.89421  0.0680602  0.0481648  -0.0214149    0.191101  ⋯\n 13 │ -13313.8  5.92608  0.0892317  0.047389   -0.0141679    0.190142  ⋯\n 14 │ -12575.0  5.89235  0.0846168  0.0341529  -0.00613056   0.192728  ⋯\n 15 │ -12833.1  5.94217  0.0774876  0.0422338  -0.00278894   0.191772  ⋯\n 16 │ -12504.5  5.94385  0.0898018  0.0429783  -0.00705609   0.192833  ⋯\n 17 │ -12875.5  5.96624  0.0976098  0.0370804  -0.000718399  0.191577  ⋯\n 18 │ -13287.4  5.97492  0.101651   0.0407503  -0.0163215    0.190317  ⋯\n 19 │ -13088.2  5.95426  0.101522   0.0342398  0.0022965     0.190991  ⋯\n 20 │ -12820.6  5.92995  0.0775968  0.0394342  -0.0112547    0.191861  ⋯\n 21 │ -12634.4  5.92989  0.0836736  0.035749   -0.00206045   0.192456  ⋯\n 22 │ -13097.6  5.92438  0.088869   0.0332515  -0.00701304   0.191018  ⋯\n 23 │ -12837.4  5.90782  0.0861051  0.0385006  -0.00752588   0.191891  ⋯\n ⋮  │    ⋮         ⋮         ⋮          ⋮           ⋮           ⋮      ⋱",
    "crumbs": [
      "Worked examples",
      "RePsychLing Kliegl et al. (2011)"
    ]
  },
  {
    "objectID": "kwdyz11.html#shortest-coverage-interval",
    "href": "kwdyz11.html#shortest-coverage-interval",
    "title": "RePsychLing Kliegl et al. (2011)",
    "section": "7.2 Shortest coverage interval",
    "text": "7.2 Shortest coverage interval\nThe upper limit of the interval for the critical CP CTR: sod, CTR: dod is hitting the upper wall of a perfect correlation. This is evidence of singularity. The other intervals do not exhibit such pathologies; they appear to be ok.\n\n\nCode\nconfint(samp)\n\n\nDictTable with 2 columns and 15 rows:\n par   lower       upper\n ────┬───────────────────────\n β1  │ 5.89917     5.97256\n β2  │ 0.0719305   0.104588\n β3  │ 0.0251181   0.0491664\n β4  │ -0.0207184  0.00268322\n ρ1  │ 0.243806    0.712967\n ρ2  │ -0.926998   0.231027\n ρ3  │ -0.687501   0.536383\n ρ4  │ -0.132528   0.739327\n ρ5  │ 0.573732    0.999995\n ρ6  │ -0.896244   0.44397\n σ   │ 0.19051     0.19359\n σ1  │ 0.116558    0.168559\n σ2  │ 0.0454979   0.0708042\n σ3  │ 0.00939896  0.041265\n σ4  │ 0.0133637   0.0375904",
    "crumbs": [
      "Worked examples",
      "RePsychLing Kliegl et al. (2011)"
    ]
  },
  {
    "objectID": "kwdyz11.html#comparative-density-plots-of-bootstrapped-parameter-estimates",
    "href": "kwdyz11.html#comparative-density-plots-of-bootstrapped-parameter-estimates",
    "title": "RePsychLing Kliegl et al. (2011)",
    "section": "7.3 Comparative density plots of bootstrapped parameter estimates",
    "text": "7.3 Comparative density plots of bootstrapped parameter estimates",
    "crumbs": [
      "Worked examples",
      "RePsychLing Kliegl et al. (2011)"
    ]
  },
  {
    "objectID": "kwdyz11.html#residual",
    "href": "kwdyz11.html#residual",
    "title": "RePsychLing Kliegl et al. (2011)",
    "section": "7.4 Residual",
    "text": "7.4 Residual\n\n\nCode\ndraw(\n  data(tbl) *\n  mapping(:σ =&gt; \"Residual standard deviation\") *\n  density();\n)",
    "crumbs": [
      "Worked examples",
      "RePsychLing Kliegl et al. (2011)"
    ]
  },
  {
    "objectID": "kwdyz11.html#fixed-effects-wo-gm",
    "href": "kwdyz11.html#fixed-effects-wo-gm",
    "title": "RePsychLing Kliegl et al. (2011)",
    "section": "7.5 Fixed effects (w/o GM)",
    "text": "7.5 Fixed effects (w/o GM)\nThe shortest coverage interval for the GM ranges from 376 to 404 ms. To keep the plot range small we do not include its density here.\n\n\nCode\nlabels = [\n  \"CTR: sod\" =&gt; \"spatial effect\",\n  \"CTR: dos\" =&gt; \"object effect\",\n  \"CTR: dod\" =&gt; \"attraction effect\",\n  \"(Intercept)\" =&gt; \"grand mean\",\n]\ndraw(\n  data(tbl) *\n  mapping(\n    [:β2, :β3, :β4] .=&gt; \"Experimental effect size [ms]\";\n    color=dims(1) =&gt; renamer([\"spatial\", \"object\", \"attraction\"] .* \" effect\") =&gt;\n    \"Experimental effects\",\n  ) *\n  density();\n)\n\n\n\n\n\n\n\n\nFigure 9: Comparative density plots of the fixed-effects parameters for model m1\n\n\n\n\n\nThe densitiies correspond nicely with the shortest coverage intervals.",
    "crumbs": [
      "Worked examples",
      "RePsychLing Kliegl et al. (2011)"
    ]
  },
  {
    "objectID": "kwdyz11.html#variance-components-vcs",
    "href": "kwdyz11.html#variance-components-vcs",
    "title": "RePsychLing Kliegl et al. (2011)",
    "section": "7.6 Variance components (VCs)",
    "text": "7.6 Variance components (VCs)\n\n\nCode\ndraw(\n  data(tbl) *\n  mapping(\n    [:σ1, :σ2, :σ3, :σ4] .=&gt; \"Standard deviations [ms]\";\n    color=dims(1) =&gt; \n    renamer(append!([\"Grand mean\"],[\"spatial\", \"object\", \"attraction\"] .* \" effect\")) =&gt;\n    \"Variance components\",\n  ) *\n  density();\n)\n\n\n\n\n\n\n\n\nFigure 10: Comparative density plots of the variance components for model m1\n\n\n\n\n\nThe VC are all very nicely defined.",
    "crumbs": [
      "Worked examples",
      "RePsychLing Kliegl et al. (2011)"
    ]
  },
  {
    "objectID": "kwdyz11.html#correlation-parameters-cps",
    "href": "kwdyz11.html#correlation-parameters-cps",
    "title": "RePsychLing Kliegl et al. (2011)",
    "section": "7.7 Correlation parameters (CPs)",
    "text": "7.7 Correlation parameters (CPs)\n\n\nCode\nlet\n  labels = [\n    \"(Intercept), CTR: sod\" =&gt; \"GM, spatial\",\n    \"(Intercept), CTR: dos\" =&gt; \"GM, object\",\n    \"CTR: sod, CTR: dos\" =&gt; \"spatial, object\",\n    \"(Intercept), CTR: dod\" =&gt; \"GM, attraction\",\n    \"CTR: sod, CTR: dod\" =&gt; \"spatial, attraction\",\n    \"CTR: dos, CTR: dod\" =&gt; \"object, attraction\",\n  ]\n  draw(\n    data(tbl) *\n    mapping(\n      [:ρ1, :ρ2, :ρ3, :ρ4, :ρ5, :ρ6] .=&gt; \"Correlation\";\n      color=dims(1) =&gt; renamer(last.(labels)) =&gt; \"Correlation parameters\",\n    ) *\n    density();\n  )\nend\n\n\n\n\n\n\n\n\nFigure 11: Comparative density plots of the correlation parameters for model m1\n\n\n\n\n\nTwo of the CPs stand out positively. First, the correlation between GM and the spatial effect is well defined. Second, as discussed throughout this script, the CP between spatial and attraction effect is close to the 1.0 border and clearly not well defined. Therefore, this CP will be replicated with a larger sample in script kkl15.jl (Kliegl et al., 2015).",
    "crumbs": [
      "Worked examples",
      "RePsychLing Kliegl et al. (2011)"
    ]
  },
  {
    "objectID": "contrasts_fggk21.html",
    "href": "contrasts_fggk21.html",
    "title": "Mixed Models Tutorial: Contrast Coding",
    "section": "",
    "text": "This script uses a subset of data reported in Fühner et al. (2021).\nTo circumvent delays associated with model fitting we work with models that are less complex than those in the reference publication. All the data to reproduce the models in the publication are used here, too; the script requires only a few changes to specify the more complex models in the paper.\nAll children were between 6.0 and 6.99 years at legal keydate (30 September) of school enrollment, that is in their ninth year of life in the third grade. To avoid delays associated with model fitting we work with a reduced data set and less complex models than those in the reference publication. The script requires only a few changes to specify the more complex models in the paper.\nThe script is structured in three main sections:",
    "crumbs": [
      "Contrast coding",
      "Mixed Models Tutorial: Contrast Coding"
    ]
  },
  {
    "objectID": "contrasts_fggk21.html#packages-and-functions",
    "href": "contrasts_fggk21.html#packages-and-functions",
    "title": "Mixed Models Tutorial: Contrast Coding",
    "section": "1.1 Packages and functions",
    "text": "1.1 Packages and functions\n\n\nCode\nusing AlgebraOfGraphics\nusing CairoMakie\nusing Chain\nusing CategoricalArrays\nusing DataFrames\nusing DataFrameMacros\nusing MixedModels\nusing ProgressMeter\nusing SMLP2023: dataset\nusing Statistics\nusing StatsBase\n\nProgressMeter.ijulia_behavior(:clear);",
    "crumbs": [
      "Contrast coding",
      "Mixed Models Tutorial: Contrast Coding"
    ]
  },
  {
    "objectID": "contrasts_fggk21.html#readme-for-datasetfggk21",
    "href": "contrasts_fggk21.html#readme-for-datasetfggk21",
    "title": "Mixed Models Tutorial: Contrast Coding",
    "section": "1.2 Readme for dataset(\"fggk21\")",
    "text": "1.2 Readme for dataset(\"fggk21\")\nNumber of scores: 525126\n\nCohort: 9 levels; 2011-2019\nSchool: 515 levels\nChild: 108295 levels; all children are between 8.0 and 8.99 years old\nSex: “Girls” (n=55,086), “Boys” (n= 53,209)\nage: testdate - middle of month of birthdate\nTest: 5 levels\n\nEndurance (Run): 6 minute endurance run [m]; to nearest 9m in 9x18m field\nCoordination (Star_r): star coordination run [m/s]; 9x9m field, 4 x diagonal = 50.912 m\nSpeed(S20_r): 20-meters sprint [m/s]\nMuscle power low (SLJ): standing long jump [cm]\nMuscle power up (BPT): 1-kg medicine ball push test [m]\n\nscore - see units",
    "crumbs": [
      "Contrast coding",
      "Mixed Models Tutorial: Contrast Coding"
    ]
  },
  {
    "objectID": "contrasts_fggk21.html#preprocessing",
    "href": "contrasts_fggk21.html#preprocessing",
    "title": "Mixed Models Tutorial: Contrast Coding",
    "section": "1.3 Preprocessing",
    "text": "1.3 Preprocessing\n\n1.3.1 Read data\n\ntbl = dataset(:fggk21)\n\nArrow.Table with 525126 rows, 7 columns, and schema:\n :Cohort  String\n :School  String\n :Child   String\n :Sex     String\n :age     Float64\n :Test    String\n :score   Float64\n\n\n\ndf = DataFrame(tbl)\ndescribe(df)\n\n7×7 DataFrame\n\n\n\nRow\nvariable\nmean\nmin\nmedian\nmax\nnmissing\neltype\n\n\n\nSymbol\nUnion…\nAny\nUnion…\nAny\nInt64\nDataType\n\n\n\n\n1\nCohort\n\n2011\n\n2019\n0\nString\n\n\n2\nSchool\n\nS100043\n\nS800200\n0\nString\n\n\n3\nChild\n\nC002352\n\nC117966\n0\nString\n\n\n4\nSex\n\nfemale\n\nmale\n0\nString\n\n\n5\nage\n8.56073\n7.99452\n8.55852\n9.10609\n0\nFloat64\n\n\n6\nTest\n\nBPT\n\nStar_r\n0\nString\n\n\n7\nscore\n226.141\n1.14152\n4.65116\n1530.0\n0\nFloat64\n\n\n\n\n\n\n\n\n1.3.2 Extract a stratified subsample\nWe extract a random sample of 500 children from the Sex (2) x Test (5) cells of the design. Cohort and School are random.\n\ndat = @chain df begin\n  @transform(:Sex = :Sex == \"female\" ? \"Girls\" : \"Boys\")\n  @groupby(:Test, :Sex)\n  combine(x -&gt; x[sample(1:nrow(x), 500), :])\nend\n\n5000×7 DataFrame4975 rows omitted\n\n\n\nRow\nTest\nSex\nCohort\nSchool\nChild\nage\nscore\n\n\n\nString\nString\nString\nString\nString\nFloat64\nFloat64\n\n\n\n\n1\nS20_r\nBoys\n2014\nS110905\nC026446\n8.29021\n3.92157\n\n\n2\nS20_r\nBoys\n2012\nS103081\nC079758\n8.75017\n5.0\n\n\n3\nS20_r\nBoys\n2018\nS104760\nC072504\n8.69268\n5.12821\n\n\n4\nS20_r\nBoys\n2014\nS100250\nC035297\n8.37509\n4.25532\n\n\n5\nS20_r\nBoys\n2018\nS103561\nC090685\n8.85421\n4.25532\n\n\n6\nS20_r\nBoys\n2012\nS112458\nC032270\n8.33402\n4.65116\n\n\n7\nS20_r\nBoys\n2017\nS112525\nC074350\n8.70363\n4.54545\n\n\n8\nS20_r\nBoys\n2011\nS101187\nC014723\n8.16975\n4.0\n\n\n9\nS20_r\nBoys\n2011\nS103408\nC023307\n8.25188\n3.7037\n\n\n10\nS20_r\nBoys\n2013\nS100948\nC021065\n8.24641\n4.65116\n\n\n11\nS20_r\nBoys\n2016\nS106719\nC015767\n8.18617\n4.0\n\n\n12\nS20_r\nBoys\n2013\nS113244\nC013538\n8.16427\n4.34783\n\n\n13\nS20_r\nBoys\n2017\nS104980\nC079138\n8.74743\n4.7619\n\n\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n\n\n4989\nRun\nGirls\n2016\nS106434\nC042585\n8.4271\n700.0\n\n\n4990\nRun\nGirls\n2019\nS101485\nC105087\n8.9692\n1188.0\n\n\n4991\nRun\nGirls\n2012\nS102350\nC031604\n8.33402\n1218.0\n\n\n4992\nRun\nGirls\n2018\nS101862\nC072002\n8.69268\n927.0\n\n\n4993\nRun\nGirls\n2018\nS111200\nC117776\n9.10609\n1044.0\n\n\n4994\nRun\nGirls\n2011\nS102260\nC008136\n8.08487\n900.0\n\n\n4995\nRun\nGirls\n2017\nS100213\nC096675\n8.8898\n980.0\n\n\n4996\nRun\nGirls\n2018\nS101710\nC052735\n8.52567\n1287.0\n\n\n4997\nRun\nGirls\n2019\nS106355\nC038195\n8.38877\n1098.0\n\n\n4998\nRun\nGirls\n2016\nS104176\nC065900\n8.63244\n995.0\n\n\n4999\nRun\nGirls\n2017\nS106331\nC086277\n8.80767\n1143.0\n\n\n5000\nRun\nGirls\n2015\nS102799\nC082605\n8.77207\n864.0\n\n\n\n\n\n\n\n\n1.3.3 Transformations\n\ntransform!(dat, :age, :age =&gt; (x -&gt; x .- 8.5) =&gt; :a1) # centered age (linear)\nselect!(groupby(dat, :Test), :, :score =&gt; zscore =&gt; :zScore) # z-score\n\n5000×9 DataFrame4975 rows omitted\n\n\n\nRow\nTest\nSex\nCohort\nSchool\nChild\nage\nscore\na1\nzScore\n\n\n\nString\nString\nString\nString\nString\nFloat64\nFloat64\nFloat64\nFloat64\n\n\n\n\n1\nS20_r\nBoys\n2014\nS110905\nC026446\n8.29021\n3.92157\n-0.209788\n-1.43408\n\n\n2\nS20_r\nBoys\n2012\nS103081\nC079758\n8.75017\n5.0\n0.250171\n1.1729\n\n\n3\nS20_r\nBoys\n2018\nS104760\nC072504\n8.69268\n5.12821\n0.192676\n1.48282\n\n\n4\nS20_r\nBoys\n2014\nS100250\nC035297\n8.37509\n4.25532\n-0.124914\n-0.627277\n\n\n5\nS20_r\nBoys\n2018\nS103561\nC090685\n8.85421\n4.25532\n0.354209\n-0.627277\n\n\n6\nS20_r\nBoys\n2012\nS112458\nC032270\n8.33402\n4.65116\n-0.165982\n0.32963\n\n\n7\nS20_r\nBoys\n2017\nS112525\nC074350\n8.70363\n4.54545\n0.203628\n0.0740921\n\n\n8\nS20_r\nBoys\n2011\nS101187\nC014723\n8.16975\n4.0\n-0.330253\n-1.24448\n\n\n9\nS20_r\nBoys\n2011\nS103408\nC023307\n8.25188\n3.7037\n-0.248118\n-1.96074\n\n\n10\nS20_r\nBoys\n2013\nS100948\nC021065\n8.24641\n4.65116\n-0.253593\n0.32963\n\n\n11\nS20_r\nBoys\n2016\nS106719\nC015767\n8.18617\n4.0\n-0.313826\n-1.24448\n\n\n12\nS20_r\nBoys\n2013\nS113244\nC013538\n8.16427\n4.34783\n-0.335729\n-0.403652\n\n\n13\nS20_r\nBoys\n2017\nS104980\nC079138\n8.74743\n4.7619\n0.247433\n0.597336\n\n\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n\n\n4989\nRun\nGirls\n2016\nS106434\nC042585\n8.4271\n700.0\n-0.0728953\n-2.06289\n\n\n4990\nRun\nGirls\n2019\nS101485\nC105087\n8.9692\n1188.0\n0.469199\n1.16633\n\n\n4991\nRun\nGirls\n2012\nS102350\nC031604\n8.33402\n1218.0\n-0.165982\n1.36485\n\n\n4992\nRun\nGirls\n2018\nS101862\nC072002\n8.69268\n927.0\n0.192676\n-0.560773\n\n\n4993\nRun\nGirls\n2018\nS111200\nC117776\n9.10609\n1044.0\n0.606092\n0.213446\n\n\n4994\nRun\nGirls\n2011\nS102260\nC008136\n8.08487\n900.0\n-0.415127\n-0.739439\n\n\n4995\nRun\nGirls\n2017\nS100213\nC096675\n8.8898\n980.0\n0.389802\n-0.210058\n\n\n4996\nRun\nGirls\n2018\nS101710\nC052735\n8.52567\n1287.0\n0.0256674\n1.82144\n\n\n4997\nRun\nGirls\n2019\nS106355\nC038195\n8.38877\n1098.0\n-0.111225\n0.570778\n\n\n4998\nRun\nGirls\n2016\nS104176\nC065900\n8.63244\n995.0\n0.132444\n-0.110799\n\n\n4999\nRun\nGirls\n2017\nS106331\nC086277\n8.80767\n1143.0\n0.307666\n0.868555\n\n\n5000\nRun\nGirls\n2015\nS102799\nC082605\n8.77207\n864.0\n0.272074\n-0.97766\n\n\n\n\n\n\n\ndat2 = combine(\n  groupby(dat, [:Test, :Sex]),\n  :score =&gt; mean,\n  :score =&gt; std,\n  :zScore =&gt; mean,\n  :zScore =&gt; std,\n)\n\n10×6 DataFrame\n\n\n\nRow\nTest\nSex\nscore_mean\nscore_std\nzScore_mean\nzScore_std\n\n\n\nString\nString\nFloat64\nFloat64\nFloat64\nFloat64\n\n\n\n\n1\nS20_r\nBoys\n4.58728\n0.413559\n0.175204\n0.999732\n\n\n2\nBPT\nBoys\n3.9588\n0.689444\n0.305114\n0.988995\n\n\n3\nSLJ\nBoys\n129.6\n19.861\n0.150547\n1.05208\n\n\n4\nStar_r\nBoys\n2.08849\n0.307899\n0.131982\n1.08002\n\n\n5\nRun\nBoys\n1052.44\n149.696\n0.269296\n0.990575\n\n\n6\nS20_r\nGirls\n4.44233\n0.401287\n-0.175204\n0.970064\n\n\n7\nBPT\nGirls\n3.5334\n0.637901\n-0.305114\n0.915058\n\n\n8\nSLJ\nGirls\n123.916\n17.4015\n-0.150547\n0.921794\n\n\n9\nStar_r\nGirls\n2.01324\n0.255091\n-0.131982\n0.894789\n\n\n10\nRun\nGirls\n971.048\n141.395\n-0.269296\n0.935646\n\n\n\n\n\n\n\n\n1.3.4 Figure of age x Sex x Test interactions\nThe main results of relevance here are shown in Figure 2 of Scientific Reports 11:17566.",
    "crumbs": [
      "Contrast coding",
      "Mixed Models Tutorial: Contrast Coding"
    ]
  },
  {
    "objectID": "contrasts_fggk21.html#seqdiffcoding-contr1",
    "href": "contrasts_fggk21.html#seqdiffcoding-contr1",
    "title": "Mixed Models Tutorial: Contrast Coding",
    "section": "2.1 SeqDiffCoding: contr1",
    "text": "2.1 SeqDiffCoding: contr1\nSeqDiffCoding was used in the publication. This specification tests pairwise differences between the five neighboring levels of Test, that is:\n\nSDC1: 2-1\nSDC2: 3-2\nSDC3: 4-3\nSDC4: 5-4\n\nThe levels were sorted such that these contrasts map onto four a priori hypotheses; in other words, they are theoretically motivated pairwise comparisons. The motivation also encompasses theoretically motivated interactions with Sex. The order of levels can also be explicitly specified during contrast construction. This is very useful if levels are in a different order in the dataframe. We recommend the explicit specification to increase transparency of the code.\nThe statistical disadvantage of SeqDiffCoding is that the contrasts are not orthogonal, that is the contrasts are correlated. This is obvious from the fact that levels 2, 3, and 4 are all used in two contrasts. One consequence of this is that correlation parameters estimated between neighboring contrasts (e.g., 2-1 and 3-2) are difficult to interpret. Usually, they will be negative because assuming some practical limitation on the overall range (e.g., between levels 1 and 3), a small “2-1” effect “correlates” negatively with a larger “3-2” effect for mathematical reasons.\nObviously, the tradeoff between theoretical motivation and statistical purity is something that must be considered carefully when planning the analysis.\n\ncontr1 = merge(\n  Dict(nm =&gt; Grouping() for nm in (:School, :Child, :Cohort)),\n  Dict(\n    :Sex =&gt; EffectsCoding(; levels=[\"Girls\", \"Boys\"]),\n    :Test =&gt; SeqDiffCoding(;\n      levels=[\"Run\", \"Star_r\", \"S20_r\", \"SLJ\", \"BPT\"]\n    ),\n  ),\n)\n\nDict{Symbol, StatsModels.AbstractContrasts} with 5 entries:\n  :Child  =&gt; Grouping()\n  :School =&gt; Grouping()\n  :Test   =&gt; SeqDiffCoding([\"Run\", \"Star_r\", \"S20_r\", \"SLJ\", \"BPT\"])\n  :Cohort =&gt; Grouping()\n  :Sex    =&gt; EffectsCoding(nothing, [\"Girls\", \"Boys\"])\n\n\n\nf_ovi_1 = @formula zScore ~ 1 + Test + (1 | Child);\n\n\nm_ovi_SeqDiff_1 = fit(MixedModel, f_ovi_1, dat; contrasts=contr1)\n\nMinimizing 14    Time: 0:00:00 (18.64 ms/it)\n\n\n\n\n\n\nEst.\nSE\nz\np\nσ_Child\n\n\n\n\n(Intercept)\n0.0022\n0.0142\n0.16\n0.8750\n0.6422\n\n\nTest: Star_r\n-0.0034\n0.0444\n-0.08\n0.9384\n\n\n\nTest: S20_r\n0.0058\n0.0445\n0.13\n0.8965\n\n\n\nTest: SLJ\n-0.0042\n0.0445\n-0.09\n0.9252\n\n\n\nTest: BPT\n0.0016\n0.0445\n0.04\n0.9721\n\n\n\nResidual\n0.7655\n\n\n\n\n\n\n\n\n\nIn this case, any differences between tests identified by the contrasts would be spurious because each test was standardized (i.e., M=0, \\(SD\\)=1). The differences could also be due to an imbalance in the number of boys and girls or in the number of missing observations for each test.\nThe primary interest in this study related to interactions of the test contrasts with and age and Sex. We start with age (linear) and its interaction with the four test contrasts.\n\nm_ovi_SeqDiff_2 = let\n  form = @formula zScore ~ 1 + Test * a1 + (1 | Child)\n  fit(MixedModel, form, dat; contrasts=contr1)\nend\n\n\n\n\n\nEst.\nSE\nz\np\nσ_Child\n\n\n\n\n(Intercept)\n-0.0117\n0.0145\n-0.81\n0.4179\n0.6372\n\n\nTest: Star_r\n-0.0043\n0.0452\n-0.09\n0.9250\n\n\n\nTest: S20_r\n-0.0035\n0.0456\n-0.08\n0.9394\n\n\n\nTest: SLJ\n0.0103\n0.0455\n0.23\n0.8213\n\n\n\nTest: BPT\n-0.0201\n0.0450\n-0.45\n0.6546\n\n\n\na1\n0.2451\n0.0490\n5.01\n&lt;1e-06\n\n\n\nTest: Star_r & a1\n0.0074\n0.1528\n0.05\n0.9613\n\n\n\nTest: S20_r & a1\n0.1039\n0.1527\n0.68\n0.4963\n\n\n\nTest: SLJ & a1\n-0.1772\n0.1523\n-1.16\n0.2445\n\n\n\nTest: BPT & a1\n0.4825\n0.1529\n3.15\n0.0016\n\n\n\nResidual\n0.7648\n\n\n\n\n\n\n\n\n\nThe difference between older and younger childrend is larger for Star_r than for Run (0.2473). S20_r did not differ significantly from Star_r (-0.0377) and SLJ (-0.0113) The largest difference in developmental gain was between BPT and SLJ (0.3355).\nPlease note that standard errors of this LMM are anti-conservative because the LMM is missing a lot of information in the RES (e..g., contrast-related VCs snd CPs for Child, School, and Cohort.\nNext we add the main effect of Sex and its interaction with the four test contrasts.\n\nm_ovi_SeqDiff_3 = let\n  form = @formula zScore ~ 1 + Test * (a1 + Sex) + (1 | Child)\n  fit(MixedModel, form, dat; contrasts=contr1)\nend\n\n\n\n\n\n\n\n\n\n\n\n\n\nEst.\nSE\nz\np\nσ_Child\n\n\n\n\n(Intercept)\n-0.0109\n0.0141\n-0.77\n0.4413\n0.6079\n\n\nTest: Star_r\n-0.0036\n0.0442\n-0.08\n0.9356\n\n\n\nTest: S20_r\n-0.0077\n0.0445\n-0.17\n0.8633\n\n\n\nTest: SLJ\n0.0128\n0.0444\n0.29\n0.7735\n\n\n\nTest: BPT\n-0.0187\n0.0440\n-0.43\n0.6701\n\n\n\na1\n0.2324\n0.0478\n4.86\n&lt;1e-05\n\n\n\nSex: Boys\n0.2065\n0.0138\n14.91\n&lt;1e-49\n\n\n\nTest: Star_r & a1\n0.0168\n0.1493\n0.11\n0.9106\n\n\n\nTest: S20_r & a1\n0.1391\n0.1492\n0.93\n0.3513\n\n\n\nTest: SLJ & a1\n-0.1989\n0.1487\n-1.34\n0.1811\n\n\n\nTest: BPT & a1\n0.4527\n0.1494\n3.03\n0.0024\n\n\n\nTest: Star_r & Sex: Boys\n-0.1280\n0.0432\n-2.96\n0.0031\n\n\n\nTest: S20_r & Sex: Boys\n0.0435\n0.0433\n1.00\n0.3159\n\n\n\nTest: SLJ & Sex: Boys\n-0.0276\n0.0433\n-0.64\n0.5243\n\n\n\nTest: BPT & Sex: Boys\n0.1511\n0.0433\n3.49\n0.0005\n\n\n\nResidual\n0.7581\n\n\n\n\n\n\n\n\n\nThe significant interactions with Sex reflect mostly differences related to muscle power, where the physiological constitution gives boys an advantage. The sex difference is smaller when coordination and cognition play a role – as in the Star_r test. (Caveat: SEs are estimated with an underspecified RES.)\nThe final step in this first series is to add the interactions between the three covariates. A significant interaction between any of the four Test contrasts and age (linear) x Sex was hypothesized to reflect a prepubertal signal (i.e., hormones start to rise in girls’ ninth year of life). However, this hypothesis is linked to a specific shape of the interaction: Girls would need to gain more than boys in tests of muscular power.\n\nf_ovi = @formula zScore ~ 1 + Test * a1 * Sex + (1 | Child)\nm_ovi_SeqDiff = fit(MixedModel, f_ovi, dat; contrasts=contr1)\n\n\n\n\n\n\n\n\n\n\n\n\n\nEst.\nSE\nz\np\nσ_Child\n\n\n\n\n(Intercept)\n-0.0107\n0.0141\n-0.76\n0.4488\n0.6087\n\n\nTest: Star_r\n-0.0029\n0.0442\n-0.07\n0.9468\n\n\n\nTest: S20_r\n-0.0054\n0.0445\n-0.12\n0.9029\n\n\n\nTest: SLJ\n0.0111\n0.0444\n0.25\n0.8018\n\n\n\nTest: BPT\n-0.0171\n0.0440\n-0.39\n0.6969\n\n\n\na1\n0.2304\n0.0478\n4.82\n&lt;1e-05\n\n\n\nSex: Boys\n0.2030\n0.0141\n14.35\n&lt;1e-45\n\n\n\nTest: Star_r & a1\n0.0215\n0.1493\n0.14\n0.8854\n\n\n\nTest: S20_r & a1\n0.1325\n0.1492\n0.89\n0.3748\n\n\n\nTest: SLJ & a1\n-0.1964\n0.1487\n-1.32\n0.1866\n\n\n\nTest: BPT & a1\n0.4483\n0.1495\n3.00\n0.0027\n\n\n\nTest: Star_r & Sex: Boys\n-0.1207\n0.0442\n-2.73\n0.0063\n\n\n\nTest: S20_r & Sex: Boys\n0.0365\n0.0445\n0.82\n0.4129\n\n\n\nTest: SLJ & Sex: Boys\n-0.0193\n0.0444\n-0.43\n0.6647\n\n\n\nTest: BPT & Sex: Boys\n0.1586\n0.0440\n3.61\n0.0003\n\n\n\na1 & Sex: Boys\n0.0464\n0.0478\n0.97\n0.3321\n\n\n\nTest: Star_r & a1 & Sex: Boys\n-0.1271\n0.1493\n-0.85\n0.3946\n\n\n\nTest: S20_r & a1 & Sex: Boys\n0.0913\n0.1492\n0.61\n0.5406\n\n\n\nTest: SLJ & a1 & Sex: Boys\n-0.1081\n0.1487\n-0.73\n0.4672\n\n\n\nTest: BPT & a1 & Sex: Boys\n-0.1543\n0.1495\n-1.03\n0.3018\n\n\n\nResidual\n0.7568\n\n\n\n\n\n\n\n\n\nThe results are very clear: Despite an abundance of statistical power there is no evidence for the differences between boys and girls in how much they gain in the ninth year of life in these five tests. The authors argue that, in this case, absence of evidence looks very much like evidence of absence of a hypothesized interaction.\nIn the next two sections we use different contrasts. Does this have a bearing on this result? We still ignore for now that we are looking at anti-conservative test statistics.",
    "crumbs": [
      "Contrast coding",
      "Mixed Models Tutorial: Contrast Coding"
    ]
  },
  {
    "objectID": "contrasts_fggk21.html#helmertcoding-contr2",
    "href": "contrasts_fggk21.html#helmertcoding-contr2",
    "title": "Mixed Models Tutorial: Contrast Coding",
    "section": "2.2 HelmertCoding: contr2",
    "text": "2.2 HelmertCoding: contr2\nThe second set of contrasts uses HelmertCoding. Helmert coding codes each level as the difference from the average of the lower levels. With the default order of Test levels we get the following test statistics which we describe in reverse order of appearance in model output\n\nHeC4: 5 - mean(1,2,3,4)\nHeC3: 4 - mean(1,2,3)\nHeC2: 3 - mean(1,2)\nHeC1: 2 - 1\n\nIn the model output, HeC1 will be reported first and HeC4 last.\nThere is some justification for the HeC4 specification in a post-hoc manner because the fifth test (BPT) turned out to be different from the other four tests in that high performance is most likely not only related to physical fitness, but also to overweight/obesity, that is for a subset of children high scores on this test might be indicative of physical unfitness. A priori the SDC4 contrast 5-4 between BPT (5) and SLJ (4) was motivated because conceptually both are tests of the physical fitness component Muscular Power, BPT for upper limbs and SLJ for lower limbs, respectively.\nOne could argue that there is justification for HeC3 because Run (1), Star_r (2), and S20 (3) involve running but SLJ (4) does not. Sports scientists, however, recoil. For them it does not make much sense to average the different running tests, because they draw on completely different physiological resources; it is a variant of the old apples-and-oranges problem.\nThe justification for HeC3 is thatRun (1) and Star_r (2) draw more strongly on cardiosrespiratory Endurance than S20 (3) due to the longer duration of the runs compared to sprinting for 20 m which is a pure measure of the physical-fitness component Speed. Again, sports scientists are not very happy with this proposal.\nFinally, HeC1 contrasts the fitness components Endurance, indicated best by Run (1), and Coordination, indicated by Star_r (2). Endurance (i.e., running for 6 minutes) is considered to be the best indicator of health-related status among the five tests because it is a rather pure measure of cardiorespiratory fitness. The Star_r test requires execution of a pre-instructed sequence of forward, sideways, and backward runs. This coordination of body movements implies a demand on working memory (i.e., remembering the order of these subruns) and executive control processes, but performats also depends on endurance. HeC1 yields a measure of Coordination “corrected” for the contribution of Endurance.\nThe statistical advantage of HelmertCoding is that the resulting contrasts are orthogonal (uncorrelated). This allows for optimal partitioning of variance and statistical power. It is also more efficient to estimate “orthogonal” than “non-orthogonal” random-effect structures.\n\ncontr2 = Dict(\n  :School =&gt; Grouping(),\n  :Child =&gt; Grouping(),\n  :Cohort =&gt; Grouping(),\n  :Sex =&gt; EffectsCoding(; levels=[\"Girls\", \"Boys\"]),\n  :Test =&gt; HelmertCoding(;\n    levels=[\"Run\", \"Star_r\", \"S20_r\", \"SLJ\", \"BPT\"],\n  ),\n);\n\n\nm_ovi_Helmert = fit(MixedModel, f_ovi, dat; contrasts=contr2)\n\n\n\n\n\n\n\n\n\n\n\n\n\nEst.\nSE\nz\np\nσ_Child\n\n\n\n\n(Intercept)\n-0.0107\n0.0141\n-0.76\n0.4488\n0.6087\n\n\nTest: Star_r\n-0.0015\n0.0221\n-0.07\n0.9468\n\n\n\nTest: S20_r\n-0.0023\n0.0129\n-0.18\n0.8581\n\n\n\nTest: SLJ\n0.0016\n0.0090\n0.18\n0.8558\n\n\n\nTest: BPT\n-0.0024\n0.0069\n-0.35\n0.7251\n\n\n\na1\n0.2304\n0.0478\n4.82\n&lt;1e-05\n\n\n\nSex: Boys\n0.2030\n0.0141\n14.35\n&lt;1e-45\n\n\n\nTest: Star_r & a1\n0.0108\n0.0747\n0.14\n0.8854\n\n\n\nTest: S20_r & a1\n0.0477\n0.0431\n1.11\n0.2684\n\n\n\nTest: SLJ & a1\n-0.0252\n0.0303\n-0.83\n0.4047\n\n\n\nTest: BPT & a1\n0.0745\n0.0238\n3.13\n0.0017\n\n\n\nTest: Star_r & Sex: Boys\n-0.0604\n0.0221\n-2.73\n0.0063\n\n\n\nTest: S20_r & Sex: Boys\n-0.0080\n0.0129\n-0.62\n0.5361\n\n\n\nTest: SLJ & Sex: Boys\n-0.0088\n0.0090\n-0.98\n0.3287\n\n\n\nTest: BPT & Sex: Boys\n0.0264\n0.0069\n3.81\n0.0001\n\n\n\na1 & Sex: Boys\n0.0464\n0.0478\n0.97\n0.3321\n\n\n\nTest: Star_r & a1 & Sex: Boys\n-0.0635\n0.0747\n-0.85\n0.3946\n\n\n\nTest: S20_r & a1 & Sex: Boys\n0.0093\n0.0431\n0.21\n0.8301\n\n\n\nTest: SLJ & a1 & Sex: Boys\n-0.0224\n0.0303\n-0.74\n0.4594\n\n\n\nTest: BPT & a1 & Sex: Boys\n-0.0443\n0.0238\n-1.86\n0.0625\n\n\n\nResidual\n0.7568\n\n\n\n\n\n\n\n\n\nWe forego a detailed discussion of the effects, but note that again none of the interactions between age x Sex with the four test contrasts was significant.\nThe default labeling of Helmert contrasts may lead to confusions with other contrasts. Therefore, we could provide our own labels:\nlabels=[\"c2.1\", \"c3.12\", \"c4.123\", \"c5.1234\"]\nOnce the order of levels is memorized the proposed labelling is very transparent.",
    "crumbs": [
      "Contrast coding",
      "Mixed Models Tutorial: Contrast Coding"
    ]
  },
  {
    "objectID": "contrasts_fggk21.html#hypothesiscoding-contr3",
    "href": "contrasts_fggk21.html#hypothesiscoding-contr3",
    "title": "Mixed Models Tutorial: Contrast Coding",
    "section": "2.3 HypothesisCoding: contr3",
    "text": "2.3 HypothesisCoding: contr3\nThe third set of contrasts uses HypothesisCoding. Hypothesis coding allows the user to specify their own a priori contrast matrix, subject to the mathematical constraint that the matrix has full rank. For example, sport scientists agree that the first four tests can be contrasted with BPT, because the difference is akin to a correction of overall physical fitness. However, they want to keep the pairwise comparisons for the first four tests.\n\nHyC1: BPT - mean(1,2,3,4)\nHyC2: Star_r - Run_r\nHyC3: Run_r - S20_r\nHyC4: S20_r - SLJ\n\n\ncontr3 = Dict(\n  :School =&gt; Grouping(),\n  :Child =&gt; Grouping(),\n  :Cohort =&gt; Grouping(),\n  :Sex =&gt; EffectsCoding(; levels=[\"Girls\", \"Boys\"]),\n  :Test =&gt; HypothesisCoding(\n    [\n      -1 -1 -1 -1 +4\n      -1 +1 0 0 0\n       0 -1 +1 0 0\n       0 0 -1 +1 0\n    ];\n    levels=[\"Run\", \"Star_r\", \"S20_r\", \"SLJ\", \"BPT\"],\n    labels=[\"BPT-other\", \"Star-End\", \"S20-Star\", \"SLJ-S20\"],\n  ),\n);\n\n\nm_ovi_Hypo = fit(MixedModel, f_ovi, dat; contrasts=contr3)\n\n\n\n\n\n\n\n\n\n\n\n\n\nEst.\nSE\nz\np\nσ_Child\n\n\n\n\n(Intercept)\n-0.0107\n0.0141\n-0.76\n0.4488\n0.6087\n\n\nTest: BPT-other\n-0.0488\n0.1389\n-0.35\n0.7251\n\n\n\nTest: Star-End\n-0.0029\n0.0442\n-0.07\n0.9468\n\n\n\nTest: S20-Star\n-0.0054\n0.0445\n-0.12\n0.9029\n\n\n\nTest: SLJ-S20\n0.0111\n0.0444\n0.25\n0.8018\n\n\n\na1\n0.2304\n0.0478\n4.82\n&lt;1e-05\n\n\n\nSex: Boys\n0.2030\n0.0141\n14.35\n&lt;1e-45\n\n\n\nTest: BPT-other & a1\n1.4902\n0.4757\n3.13\n0.0017\n\n\n\nTest: Star-End & a1\n0.0215\n0.1493\n0.14\n0.8854\n\n\n\nTest: S20-Star & a1\n0.1325\n0.1492\n0.89\n0.3748\n\n\n\nTest: SLJ-S20 & a1\n-0.1964\n0.1487\n-1.32\n0.1866\n\n\n\nTest: BPT-other & Sex: Boys\n0.5287\n0.1389\n3.81\n0.0001\n\n\n\nTest: Star-End & Sex: Boys\n-0.1207\n0.0442\n-2.73\n0.0063\n\n\n\nTest: S20-Star & Sex: Boys\n0.0365\n0.0445\n0.82\n0.4129\n\n\n\nTest: SLJ-S20 & Sex: Boys\n-0.0193\n0.0444\n-0.43\n0.6647\n\n\n\na1 & Sex: Boys\n0.0464\n0.0478\n0.97\n0.3321\n\n\n\nTest: BPT-other & a1 & Sex: Boys\n-0.8862\n0.4757\n-1.86\n0.0625\n\n\n\nTest: Star-End & a1 & Sex: Boys\n-0.1271\n0.1493\n-0.85\n0.3946\n\n\n\nTest: S20-Star & a1 & Sex: Boys\n0.0913\n0.1492\n0.61\n0.5406\n\n\n\nTest: SLJ-S20 & a1 & Sex: Boys\n-0.1081\n0.1487\n-0.73\n0.4672\n\n\n\nResidual\n0.7568\n\n\n\n\n\n\n\n\n\nWith HypothesisCoding we must generate our own labels for the contrasts. The default labeling of contrasts is usually not interpretable. Therefore, we provide our own.\nAnyway, none of the interactions between age x Sex with the four Test contrasts was significant for these contrasts.\n\ncontr1b = Dict(\n  :School =&gt; Grouping(),\n  :Child =&gt; Grouping(),\n  :Cohort =&gt; Grouping(),\n  :Sex =&gt; EffectsCoding(; levels=[\"Girls\", \"Boys\"]),\n  :Test =&gt; HypothesisCoding(\n    [\n      -1 +1 0 0 0\n      0 -1 +1 0 0\n      0 0 -1 +1 0\n      0 0 0 -1 +1\n    ];\n    levels=[\"Run\", \"Star_r\", \"S20_r\", \"SLJ\", \"BPT\"],\n    labels=[\"Star-Run\", \"S20-Star\", \"SLJ-S20\", \"BPT-SLJ\"],\n  ),\n);\n\n\nm_ovi_SeqDiff_v2 = fit(MixedModel, f_ovi, dat; contrasts=contr1b)\n\n\n\n\n\n\n\n\n\n\n\n\n\nEst.\nSE\nz\np\nσ_Child\n\n\n\n\n(Intercept)\n-0.0107\n0.0141\n-0.76\n0.4488\n0.6087\n\n\nTest: Star-Run\n-0.0029\n0.0442\n-0.07\n0.9468\n\n\n\nTest: S20-Star\n-0.0054\n0.0445\n-0.12\n0.9029\n\n\n\nTest: SLJ-S20\n0.0111\n0.0444\n0.25\n0.8018\n\n\n\nTest: BPT-SLJ\n-0.0171\n0.0440\n-0.39\n0.6969\n\n\n\na1\n0.2304\n0.0478\n4.82\n&lt;1e-05\n\n\n\nSex: Boys\n0.2030\n0.0141\n14.35\n&lt;1e-45\n\n\n\nTest: Star-Run & a1\n0.0215\n0.1493\n0.14\n0.8854\n\n\n\nTest: S20-Star & a1\n0.1325\n0.1492\n0.89\n0.3748\n\n\n\nTest: SLJ-S20 & a1\n-0.1964\n0.1487\n-1.32\n0.1866\n\n\n\nTest: BPT-SLJ & a1\n0.4483\n0.1495\n3.00\n0.0027\n\n\n\nTest: Star-Run & Sex: Boys\n-0.1207\n0.0442\n-2.73\n0.0063\n\n\n\nTest: S20-Star & Sex: Boys\n0.0365\n0.0445\n0.82\n0.4129\n\n\n\nTest: SLJ-S20 & Sex: Boys\n-0.0193\n0.0444\n-0.43\n0.6647\n\n\n\nTest: BPT-SLJ & Sex: Boys\n0.1586\n0.0440\n3.61\n0.0003\n\n\n\na1 & Sex: Boys\n0.0464\n0.0478\n0.97\n0.3321\n\n\n\nTest: Star-Run & a1 & Sex: Boys\n-0.1271\n0.1493\n-0.85\n0.3946\n\n\n\nTest: S20-Star & a1 & Sex: Boys\n0.0913\n0.1492\n0.61\n0.5406\n\n\n\nTest: SLJ-S20 & a1 & Sex: Boys\n-0.1081\n0.1487\n-0.73\n0.4672\n\n\n\nTest: BPT-SLJ & a1 & Sex: Boys\n-0.1543\n0.1495\n-1.03\n0.3018\n\n\n\nResidual\n0.7568\n\n\n\n\n\n\n\n\n\n\nm_zcp_SeqD = let\n  form = @formula(\n    zScore ~ 1 + Test * a1 * Sex + zerocorr(1 + Test | Child)\n  )\n  fit(MixedModel, form, dat; contrasts=contr1b)\nend\n\nMinimizing 116    Time: 0:00:00 ( 6.30 ms/it)\n  objective:  13873.949546391092\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEst.\nSE\nz\np\nσ_Child\n\n\n\n\n(Intercept)\n-0.0110\n0.0141\n-0.78\n0.4376\n0.6111\n\n\nTest: Star-Run\n-0.0025\n0.0445\n-0.06\n0.9556\n0.0000\n\n\nTest: S20-Star\n-0.0059\n0.0444\n-0.13\n0.8940\n0.4768\n\n\nTest: SLJ-S20\n0.0121\n0.0441\n0.27\n0.7838\n0.3355\n\n\nTest: BPT-SLJ\n-0.0186\n0.0438\n-0.42\n0.6712\n0.0000\n\n\na1\n0.2312\n0.0478\n4.84\n&lt;1e-05\n\n\n\nSex: Boys\n0.2032\n0.0141\n14.37\n&lt;1e-46\n\n\n\nTest: Star-Run & a1\n0.0310\n0.1506\n0.21\n0.8368\n\n\n\nTest: S20-Star & a1\n0.1248\n0.1490\n0.84\n0.4020\n\n\n\nTest: SLJ-S20 & a1\n-0.1969\n0.1475\n-1.33\n0.1819\n\n\n\nTest: BPT-SLJ & a1\n0.4525\n0.1491\n3.04\n0.0024\n\n\n\nTest: Star-Run & Sex: Boys\n-0.1195\n0.0445\n-2.68\n0.0073\n\n\n\nTest: S20-Star & Sex: Boys\n0.0351\n0.0444\n0.79\n0.4295\n\n\n\nTest: SLJ-S20 & Sex: Boys\n-0.0197\n0.0441\n-0.45\n0.6545\n\n\n\nTest: BPT-SLJ & Sex: Boys\n0.1589\n0.0438\n3.62\n0.0003\n\n\n\na1 & Sex: Boys\n0.0463\n0.0478\n0.97\n0.3330\n\n\n\nTest: Star-Run & a1 & Sex: Boys\n-0.1286\n0.1506\n-0.85\n0.3930\n\n\n\nTest: S20-Star & a1 & Sex: Boys\n0.0901\n0.1490\n0.60\n0.5455\n\n\n\nTest: SLJ-S20 & a1 & Sex: Boys\n-0.1082\n0.1475\n-0.73\n0.4633\n\n\n\nTest: BPT-SLJ & a1 & Sex: Boys\n-0.1591\n0.1491\n-1.07\n0.2858\n\n\n\nResidual\n0.6988\n\n\n\n\n\n\n\n\n\n\nm_zcp_SeqD_2 = let\n  form = @formula(\n    zScore ~ 1 + Test * a1 * Sex + (0 + Test | Child)\n  )\n  fit(MixedModel, form, dat; contrasts=contr1b)\nend\n\nMinimizing 3140    Time: 0:00:25 ( 8.04 ms/it)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEst.\nSE\nz\np\nσ_Child\n\n\n\n\n(Intercept)\n-0.0127\n0.0141\n-0.90\n0.3665\n\n\n\nTest: Star-Run\n-0.0024\n0.0441\n-0.06\n0.9558\n\n\n\nTest: S20-Star\n-0.0123\n0.0448\n-0.28\n0.7827\n\n\n\nTest: SLJ-S20\n0.0187\n0.0453\n0.41\n0.6792\n\n\n\nTest: BPT-SLJ\n-0.0126\n0.0439\n-0.29\n0.7738\n\n\n\na1\n0.2310\n0.0476\n4.85\n&lt;1e-05\n\n\n\nSex: Boys\n0.2051\n0.0141\n14.57\n&lt;1e-47\n\n\n\nTest: Star-Run & a1\n0.0401\n0.1490\n0.27\n0.7880\n\n\n\nTest: S20-Star & a1\n0.1273\n0.1501\n0.85\n0.3962\n\n\n\nTest: SLJ-S20 & a1\n-0.1968\n0.1517\n-1.30\n0.1945\n\n\n\nTest: BPT-SLJ & a1\n0.4479\n0.1493\n3.00\n0.0027\n\n\n\nTest: Star-Run & Sex: Boys\n-0.1245\n0.0441\n-2.83\n0.0047\n\n\n\nTest: S20-Star & Sex: Boys\n0.0373\n0.0448\n0.83\n0.4052\n\n\n\nTest: SLJ-S20 & Sex: Boys\n-0.0199\n0.0453\n-0.44\n0.6598\n\n\n\nTest: BPT-SLJ & Sex: Boys\n0.1559\n0.0439\n3.55\n0.0004\n\n\n\na1 & Sex: Boys\n0.0510\n0.0476\n1.07\n0.2841\n\n\n\nTest: Star-Run & a1 & Sex: Boys\n-0.1409\n0.1490\n-0.95\n0.3445\n\n\n\nTest: S20-Star & a1 & Sex: Boys\n0.0953\n0.1501\n0.63\n0.5255\n\n\n\nTest: SLJ-S20 & a1 & Sex: Boys\n-0.1127\n0.1517\n-0.74\n0.4575\n\n\n\nTest: BPT-SLJ & a1 & Sex: Boys\n-0.1457\n0.1493\n-0.98\n0.3290\n\n\n\nTest: BPT\n\n\n\n\n0.9343\n\n\nTest: SLJ\n\n\n\n\n1.0006\n\n\nTest: Star_r\n\n\n\n\n0.9722\n\n\nTest: Run\n\n\n\n\n0.9611\n\n\nTest: S20_r\n\n\n\n\n0.9785\n\n\nResidual\n0.0000\n\n\n\n\n\n\n\n\n\n\nm_cpx_0_SeqDiff = let\n  f_cpx_0 = @formula(\n    zScore ~ 1 + Test * a1 * Sex + (0 + Test | Child)\n  )\n  fit(MixedModel, f_cpx_0, dat; contrasts=contr1b)\nend\n\nMinimizing 3140    Time: 0:00:26 ( 8.44 ms/it)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEst.\nSE\nz\np\nσ_Child\n\n\n\n\n(Intercept)\n-0.0127\n0.0141\n-0.90\n0.3665\n\n\n\nTest: Star-Run\n-0.0024\n0.0441\n-0.06\n0.9558\n\n\n\nTest: S20-Star\n-0.0123\n0.0448\n-0.28\n0.7827\n\n\n\nTest: SLJ-S20\n0.0187\n0.0453\n0.41\n0.6792\n\n\n\nTest: BPT-SLJ\n-0.0126\n0.0439\n-0.29\n0.7738\n\n\n\na1\n0.2310\n0.0476\n4.85\n&lt;1e-05\n\n\n\nSex: Boys\n0.2051\n0.0141\n14.57\n&lt;1e-47\n\n\n\nTest: Star-Run & a1\n0.0401\n0.1490\n0.27\n0.7880\n\n\n\nTest: S20-Star & a1\n0.1273\n0.1501\n0.85\n0.3962\n\n\n\nTest: SLJ-S20 & a1\n-0.1968\n0.1517\n-1.30\n0.1945\n\n\n\nTest: BPT-SLJ & a1\n0.4479\n0.1493\n3.00\n0.0027\n\n\n\nTest: Star-Run & Sex: Boys\n-0.1245\n0.0441\n-2.83\n0.0047\n\n\n\nTest: S20-Star & Sex: Boys\n0.0373\n0.0448\n0.83\n0.4052\n\n\n\nTest: SLJ-S20 & Sex: Boys\n-0.0199\n0.0453\n-0.44\n0.6598\n\n\n\nTest: BPT-SLJ & Sex: Boys\n0.1559\n0.0439\n3.55\n0.0004\n\n\n\na1 & Sex: Boys\n0.0510\n0.0476\n1.07\n0.2841\n\n\n\nTest: Star-Run & a1 & Sex: Boys\n-0.1409\n0.1490\n-0.95\n0.3445\n\n\n\nTest: S20-Star & a1 & Sex: Boys\n0.0953\n0.1501\n0.63\n0.5255\n\n\n\nTest: SLJ-S20 & a1 & Sex: Boys\n-0.1127\n0.1517\n-0.74\n0.4575\n\n\n\nTest: BPT-SLJ & a1 & Sex: Boys\n-0.1457\n0.1493\n-0.98\n0.3290\n\n\n\nTest: BPT\n\n\n\n\n0.9343\n\n\nTest: SLJ\n\n\n\n\n1.0006\n\n\nTest: Star_r\n\n\n\n\n0.9722\n\n\nTest: Run\n\n\n\n\n0.9611\n\n\nTest: S20_r\n\n\n\n\n0.9785\n\n\nResidual\n0.0000\n\n\n\n\n\n\n\n\n\n\nVarCorr(m_cpx_0_SeqDiff)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nColumn\nVariance\nStd.Dev\nCorr.\n\n\n\n\n\n\n\nChild\nTest: Run\n0.92380604\n0.96114829\n\n\n\n\n\n\n\nTest: Star_r\n0.94508363\n0.97215412\n+0.30\n\n\n\n\n\n\nTest: S20_r\n0.95749430\n0.97851638\n-0.63\n+0.33\n\n\n\n\n\nTest: SLJ\n1.00116952\n1.00058459\n+0.35\n+0.34\n+0.41\n\n\n\n\nTest: BPT\n0.87300628\n0.93434805\n+0.02\n+0.59\n+0.38\n+0.18\n\n\nResidual\n\n0.00000000\n0.00000482\n\n\n\n\n\n\n\n\n\n\nm_cpx_0_SeqDiff.PCA\n\n(Child = \nPrincipal components based on correlation matrix\n Test: Run      1.0     .      .      .      .\n Test: Star_r   0.3    1.0     .      .      .\n Test: S20_r   -0.63   0.33   1.0     .      .\n Test: SLJ      0.35   0.34   0.41   1.0     .\n Test: BPT      0.02   0.59   0.38   0.18   1.0\n\nNormalized cumulative variances:\n[0.4251, 0.7483, 0.9265, 0.9999, 1.0]\n\nComponent loadings\n                 PC1    PC2    PC3    PC4    PC5\n Test: Run     -0.02   0.79  -0.04   0.06  -0.62\n Test: Star_r  -0.55   0.24   0.29  -0.7    0.24\n Test: S20_r   -0.48  -0.52  -0.25  -0.12  -0.64\n Test: SLJ     -0.43   0.23  -0.74   0.24   0.38\n Test: BPT     -0.52   0.0    0.54   0.66   0.05,)\n\n\n\nf_cpx_1 = @formula(\n  zScore ~ 1 + Test * a1 * Sex + (1 + Test | Child)\n)\nm_cpx_1_SeqDiff =\nfit(MixedModel, f_cpx_1, dat; contrasts=contr1b)\n\nMinimizing 3296    Time: 0:00:19 ( 5.89 ms/it)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEst.\nSE\nz\np\nσ_Child\n\n\n\n\n(Intercept)\n-0.0112\n0.0141\n-0.79\n0.4290\n0.6566\n\n\nTest: Star-Run\n-0.0054\n0.0440\n-0.12\n0.9021\n1.0016\n\n\nTest: S20-Star\n-0.0083\n0.0448\n-0.18\n0.8538\n0.8706\n\n\nTest: SLJ-S20\n0.0137\n0.0452\n0.30\n0.7624\n0.8767\n\n\nTest: BPT-SLJ\n-0.0142\n0.0439\n-0.32\n0.7468\n1.2302\n\n\na1\n0.2325\n0.0477\n4.87\n&lt;1e-05\n\n\n\nSex: Boys\n0.2049\n0.0141\n14.50\n&lt;1e-46\n\n\n\nTest: Star-Run & a1\n0.0338\n0.1488\n0.23\n0.8206\n\n\n\nTest: S20-Star & a1\n0.1387\n0.1502\n0.92\n0.3557\n\n\n\nTest: SLJ-S20 & a1\n-0.2013\n0.1514\n-1.33\n0.1838\n\n\n\nTest: BPT-SLJ & a1\n0.4480\n0.1493\n3.00\n0.0027\n\n\n\nTest: Star-Run & Sex: Boys\n-0.1221\n0.0440\n-2.77\n0.0055\n\n\n\nTest: S20-Star & Sex: Boys\n0.0338\n0.0448\n0.75\n0.4504\n\n\n\nTest: SLJ-S20 & Sex: Boys\n-0.0179\n0.0452\n-0.40\n0.6917\n\n\n\nTest: BPT-SLJ & Sex: Boys\n0.1560\n0.0439\n3.55\n0.0004\n\n\n\na1 & Sex: Boys\n0.0497\n0.0477\n1.04\n0.2978\n\n\n\nTest: Star-Run & a1 & Sex: Boys\n-0.1332\n0.1488\n-0.89\n0.3709\n\n\n\nTest: S20-Star & a1 & Sex: Boys\n0.0909\n0.1502\n0.61\n0.5449\n\n\n\nTest: SLJ-S20 & a1 & Sex: Boys\n-0.1048\n0.1514\n-0.69\n0.4887\n\n\n\nTest: BPT-SLJ & a1 & Sex: Boys\n-0.1511\n0.1493\n-1.01\n0.3116\n\n\n\nResidual\n0.0000\n\n\n\n\n\n\n\n\n\n\nm_cpx_1_SeqDiff.PCA\n\n(Child = \nPrincipal components based on correlation matrix\n (Intercept)      1.0     .      .      .      .\n Test: Star-Run   0.45   1.0     .      .      .\n Test: S20-Star  -0.21   0.21   1.0     .      .\n Test: SLJ-S20    0.14  -0.71  -0.36   1.0     .\n Test: BPT-SLJ   -0.29   0.48  -0.23  -0.47   1.0\n\nNormalized cumulative variances:\n[0.4323, 0.7042, 0.9473, 0.9991, 1.0]\n\nComponent loadings\n                   PC1    PC2    PC3    PC4    PC5\n (Intercept)     -0.01   0.85  -0.14  -0.07   0.51\n Test: Star-Run  -0.61   0.37  -0.07  -0.2   -0.67\n Test: S20-Star  -0.21  -0.31  -0.77  -0.46   0.24\n Test: SLJ-S20    0.62   0.14   0.11  -0.71  -0.28\n Test: BPT-SLJ   -0.45  -0.18   0.61  -0.49   0.39,)",
    "crumbs": [
      "Contrast coding",
      "Mixed Models Tutorial: Contrast Coding"
    ]
  },
  {
    "objectID": "contrasts_fggk21.html#pca-based-hypothesiscoding-contr4",
    "href": "contrasts_fggk21.html#pca-based-hypothesiscoding-contr4",
    "title": "Mixed Models Tutorial: Contrast Coding",
    "section": "2.4 PCA-based HypothesisCoding: contr4",
    "text": "2.4 PCA-based HypothesisCoding: contr4\nThe fourth set of contrasts uses HypothesisCoding to specify the set of contrasts implementing the loadings of the four principle components of the published LMM based on test scores, not test effects (contrasts) - coarse-grained, that is roughly according to their signs. This is actually a very interesting and plausible solution nobody had proposed a priori.\n\nPC1: BPT - Run_r\nPC2: (Star_r + S20_r + SLJ) - (BPT + Run_r)\nPC3: Star_r - (S20_r + SLJ)\nPC4: S20_r - SLJ\n\nPC1 contrasts the worst and the best indicator of physical health; PC2 contrasts these two against the core indicators of physical fitness; PC3 contrasts the cognitive and the physical tests within the narrow set of physical fitness components; and PC4, finally, contrasts two types of lower muscular fitness differing in speed and power.\n\ncontr4 = Dict(\n  :School =&gt; Grouping(),\n  :Child =&gt; Grouping(),\n  :Cohort =&gt; Grouping(),\n  :Sex =&gt; EffectsCoding(; levels=[\"Girls\", \"Boys\"]),\n  :Test =&gt; HypothesisCoding(\n    [\n      -1 0 0 0 +1\n      -3 +2 +2 +2 -3\n      0 +2 -1 -1 0\n      0 0 +1 -1 0\n    ];\n    levels=[\"Run\", \"Star_r\", \"S20_r\", \"SLJ\", \"BPT\"],\n    labels=[\"c5.1\", \"c234.15\", \"c2.34\", \"c3.4\"],\n  ),\n);\n\n\nm_cpx_1_PC = fit(MixedModel, f_cpx_1, dat; contrasts=contr4)\n\nMinimizing 1534    Time: 0:00:12 ( 8.30 ms/it)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEst.\nSE\nz\np\nσ_Child\n\n\n\n\n(Intercept)\n-0.0105\n0.0141\n-0.74\n0.4594\n0.6430\n\n\nTest: c5.1\n-0.0113\n0.0429\n-0.26\n0.7920\n1.4300\n\n\nTest: c234.15\n0.0073\n0.1683\n0.04\n0.9656\n0.9880\n\n\nTest: c2.34\n-0.0117\n0.0781\n-0.15\n0.8813\n1.3538\n\n\nTest: c3.4\n-0.0113\n0.0452\n-0.25\n0.8033\n1.6103\n\n\na1\n0.2378\n0.0478\n4.97\n&lt;1e-06\n\n\n\nSex: Boys\n0.2043\n0.0141\n14.44\n&lt;1e-46\n\n\n\nTest: c5.1 & a1\n0.4108\n0.1465\n2.80\n0.0050\n\n\n\nTest: c234.15 & a1\n-0.8395\n0.5708\n-1.47\n0.1414\n\n\n\nTest: c2.34 & a1\n-0.0799\n0.2623\n-0.30\n0.7607\n\n\n\nTest: c3.4 & a1\n0.2216\n0.1515\n1.46\n0.1437\n\n\n\nTest: c5.1 & Sex: Boys\n0.0542\n0.0429\n1.27\n0.2059\n\n\n\nTest: c234.15 & Sex: Boys\n-0.7856\n0.1683\n-4.67\n&lt;1e-05\n\n\n\nTest: c2.34 & Sex: Boys\n-0.0471\n0.0781\n-0.60\n0.5465\n\n\n\nTest: c3.4 & Sex: Boys\n0.0161\n0.0452\n0.36\n0.7224\n\n\n\na1 & Sex: Boys\n0.0487\n0.0478\n1.02\n0.3081\n\n\n\nTest: c5.1 & a1 & Sex: Boys\n-0.2955\n0.1465\n-2.02\n0.0436\n\n\n\nTest: c234.15 & a1 & Sex: Boys\n0.3089\n0.5708\n0.54\n0.5884\n\n\n\nTest: c2.34 & a1 & Sex: Boys\n-0.0625\n0.2623\n-0.24\n0.8117\n\n\n\nTest: c3.4 & a1 & Sex: Boys\n0.0929\n0.1515\n0.61\n0.5400\n\n\n\nResidual\n0.0000\n\n\n\n\n\n\n\n\n\n\nVarCorr(m_cpx_1_PC)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nColumn\nVariance\nStd.Dev\nCorr.\n\n\n\n\n\n\n\nChild\n(Intercept)\n0.41338814\n0.64295267\n\n\n\n\n\n\n\nTest: c5.1\n2.04484693\n1.42998144\n-0.14\n\n\n\n\n\n\nTest: c234.15\n0.97608251\n0.98796888\n+0.33\n-0.72\n\n\n\n\n\nTest: c2.34\n1.83288851\n1.35384213\n+0.60\n+0.50\n-0.12\n\n\n\n\nTest: c3.4\n2.59301071\n1.61028281\n-0.11\n-0.16\n+0.70\n-0.10\n\n\nResidual\n\n0.00000000\n0.00001636\n\n\n\n\n\n\n\n\n\n\nm_cpx_1_PC.PCA\n\n(Child = \nPrincipal components based on correlation matrix\n (Intercept)     1.0     .      .      .     .\n Test: c5.1     -0.14   1.0     .      .     .\n Test: c234.15   0.33  -0.72   1.0     .     .\n Test: c2.34     0.6    0.5   -0.12   1.0    .\n Test: c3.4     -0.11  -0.16   0.7   -0.1   1.0\n\nNormalized cumulative variances:\n[0.4454, 0.7738, 0.9693, 0.9991, 1.0]\n\nComponent loadings\n                  PC1   PC2    PC3    PC4    PC5\n (Intercept)    -0.05  0.71  -0.33   0.6    0.16\n Test: c5.1      0.56  0.05   0.54   0.41  -0.48\n Test: c234.15  -0.63  0.26   0.08  -0.12  -0.72\n Test: c2.34     0.3   0.65   0.23  -0.66   0.1\n Test: c3.4     -0.45  0.06   0.74   0.18   0.46,)\n\n\nThere is a numerical interaction with a z-value &gt; 2.0 for the first PCA (i.e., BPT - Run_r). This interaction would really need to be replicated to be taken seriously. It is probably due to larger “unfitness” gains in boys than girls (i.e., in BPT) relative to the slightly larger health-related “fitness” gains of girls than boys (i.e., in Run_r).\n\ncontr4b = merge(\n  Dict(nm =&gt; Grouping() for nm in (:School, :Child, :Cohort)),\n  Dict(\n    :Sex =&gt; EffectsCoding(; levels=[\"Girls\", \"Boys\"]),\n    :Test =&gt; HypothesisCoding(\n      [\n        0.49 -0.04 0.20 0.03 -0.85\n        0.70 -0.56 -0.21 -0.13 0.37\n        0.31 0.68 -0.56 -0.35 0.00\n        0.04 0.08 0.61 -0.78 0.13\n      ];\n      levels=[\"Run\", \"Star_r\", \"S20_r\", \"SLJ\", \"BPT\"],\n      labels=[\"c5.1\", \"c234.15\", \"c12.34\", \"c3.4\"],\n    ),\n  ),\n);\n\n\nm_cpx_1_PC_2 = fit(MixedModel, f_cpx_1, dat; contrasts=contr4b)\n\nMinimizing 2384    Time: 0:00:16 ( 7.05 ms/it)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEst.\nSE\nz\np\nσ_Child\n\n\n\n\n(Intercept)\n-0.0124\n0.0143\n-0.87\n0.3861\n0.5488\n\n\nTest: c5.1\n0.0063\n0.0302\n0.21\n0.8361\n0.6452\n\n\nTest: c234.15\n0.0026\n0.0316\n0.08\n0.9332\n0.8329\n\n\nTest: c12.34\n0.0041\n0.0320\n0.13\n0.8983\n1.1700\n\n\nTest: c3.4\n-0.0136\n0.0316\n-0.43\n0.6666\n0.8093\n\n\na1\n0.2188\n0.0481\n4.55\n&lt;1e-05\n\n\n\nSex: Boys\n0.1967\n0.0143\n13.79\n&lt;1e-42\n\n\n\nTest: c5.1 & a1\n-0.2874\n0.1033\n-2.78\n0.0054\n\n\n\nTest: c234.15 & a1\n0.0745\n0.1068\n0.70\n0.4858\n\n\n\nTest: c12.34 & a1\n-0.0613\n0.1075\n-0.57\n0.5686\n\n\n\nTest: c3.4 & a1\n0.1842\n0.1061\n1.74\n0.0826\n\n\n\nTest: c5.1 & Sex: Boys\n-0.0730\n0.0302\n-2.41\n0.0158\n\n\n\nTest: c234.15 & Sex: Boys\n0.1316\n0.0316\n4.16\n&lt;1e-04\n\n\n\nTest: c12.34 & Sex: Boys\n0.0064\n0.0320\n0.20\n0.8416\n\n\n\nTest: c3.4 & Sex: Boys\n0.0305\n0.0316\n0.96\n0.3353\n\n\n\na1 & Sex: Boys\n0.0502\n0.0481\n1.04\n0.2963\n\n\n\nTest: c5.1 & a1 & Sex: Boys\n0.2192\n0.1033\n2.12\n0.0338\n\n\n\nTest: c234.15 & a1 & Sex: Boys\n0.0228\n0.1068\n0.21\n0.8309\n\n\n\nTest: c12.34 & a1 & Sex: Boys\n-0.0071\n0.1075\n-0.07\n0.9471\n\n\n\nTest: c3.4 & a1 & Sex: Boys\n0.0462\n0.1061\n0.44\n0.6635\n\n\n\nResidual\n0.0000\n\n\n\n\n\n\n\n\n\n\nVarCorr(m_cpx_1_PC_2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nColumn\nVariance\nStd.Dev\nCorr.\n\n\n\n\n\n\n\nChild\n(Intercept)\n0.30118044\n0.54879909\n\n\n\n\n\n\n\nTest: c5.1\n0.41627796\n0.64519606\n-0.53\n\n\n\n\n\n\nTest: c234.15\n0.69367490\n0.83287148\n+0.15\n+0.54\n\n\n\n\n\nTest: c12.34\n1.36897619\n1.17003256\n+0.02\n-0.33\n+0.12\n\n\n\n\nTest: c3.4\n0.65494448\n0.80928640\n-0.37\n-0.17\n-0.23\n-0.22\n\n\nResidual\n\n0.00000000\n0.00000798\n\n\n\n\n\n\n\n\n\n\nm_cpx_1_PC_2.PCA\n\n(Child = \nPrincipal components based on correlation matrix\n (Intercept)     1.0     .      .      .      .\n Test: c5.1     -0.53   1.0     .      .      .\n Test: c234.15   0.15   0.54   1.0     .      .\n Test: c12.34    0.02  -0.33   0.12   1.0     .\n Test: c3.4     -0.37  -0.17  -0.23  -0.22   1.0\n\nNormalized cumulative variances:\n[0.3479, 0.6644, 0.8616, 0.9971, 1.0]\n\nComponent loadings\n                  PC1    PC2    PC3    PC4    PC5\n (Intercept)    -0.47  -0.43   0.52  -0.25  -0.5\n Test: c5.1      0.74  -0.14   0.02   0.16  -0.64\n Test: c234.15   0.38  -0.55  -0.02  -0.62   0.4\n Test: c12.34   -0.29  -0.29  -0.85  -0.1   -0.31\n Test: c3.4      0.04   0.64  -0.05  -0.72  -0.27,)\n\n\n\nf_zcp_1 = @formula(zScore ~ 1 + Test*a1*Sex + zerocorr(1 + Test | Child))\nm_zcp_1_PC_2 = fit(MixedModel, f_zcp_1, dat; contrasts=contr4b)\n\nMinimizing 495    Time: 0:00:02 ( 4.39 ms/it)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEst.\nSE\nz\np\nσ_Child\n\n\n\n\n(Intercept)\n-0.0106\n0.0144\n-0.73\n0.4624\n0.6597\n\n\nTest: c5.1\n0.0086\n0.0292\n0.29\n0.7683\n0.5881\n\n\nTest: c234.15\n-0.0017\n0.0315\n-0.05\n0.9566\n0.7884\n\n\nTest: c12.34\n0.0021\n0.0327\n0.06\n0.9495\n0.9596\n\n\nTest: c3.4\n-0.0093\n0.0322\n-0.29\n0.7718\n0.8222\n\n\na1\n0.2167\n0.0486\n4.46\n&lt;1e-05\n\n\n\nSex: Boys\n0.1963\n0.0144\n13.61\n&lt;1e-41\n\n\n\nTest: c5.1 & a1\n-0.2989\n0.1000\n-2.99\n0.0028\n\n\n\nTest: c234.15 & a1\n0.0897\n0.1071\n0.84\n0.4021\n\n\n\nTest: c12.34 & a1\n-0.0531\n0.1100\n-0.48\n0.6295\n\n\n\nTest: c3.4 & a1\n0.1735\n0.1078\n1.61\n0.1075\n\n\n\nTest: c5.1 & Sex: Boys\n-0.0725\n0.0292\n-2.48\n0.0131\n\n\n\nTest: c234.15 & Sex: Boys\n0.1256\n0.0315\n3.99\n&lt;1e-04\n\n\n\nTest: c12.34 & Sex: Boys\n0.0083\n0.0327\n0.25\n0.7999\n\n\n\nTest: c3.4 & Sex: Boys\n0.0311\n0.0322\n0.97\n0.3331\n\n\n\na1 & Sex: Boys\n0.0534\n0.0486\n1.10\n0.2721\n\n\n\nTest: c5.1 & a1 & Sex: Boys\n0.2220\n0.1000\n2.22\n0.0264\n\n\n\nTest: c234.15 & a1 & Sex: Boys\n0.0085\n0.1071\n0.08\n0.9368\n\n\n\nTest: c12.34 & a1 & Sex: Boys\n-0.0072\n0.1100\n-0.07\n0.9479\n\n\n\nTest: c3.4 & a1 & Sex: Boys\n0.0529\n0.1078\n0.49\n0.6233\n\n\n\nResidual\n0.0000\n\n\n\n\n\n\n\n\n\n\nVarCorr(m_zcp_1_PC_2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nColumn\nVariance\nStd.Dev\nCorr.\n\n\n\n\n\n\n\nChild\n(Intercept)\n0.43514940\n0.65965855\n\n\n\n\n\n\n\nTest: c5.1\n0.34583510\n0.58807746\n.\n\n\n\n\n\n\nTest: c234.15\n0.62155002\n0.78838444\n.\n.\n\n\n\n\n\nTest: c12.34\n0.92087872\n0.95962426\n.\n.\n.\n\n\n\n\nTest: c3.4\n0.67599521\n0.82218928\n.\n.\n.\n.\n\n\nResidual\n\n0.00000000\n0.00000070\n\n\n\n\n\n\n\n\n\n\nMixedModels.likelihoodratiotest(m_zcp_1_PC_2, m_cpx_1_PC_2)\n\n\n\n\n\n\n\n\n\n\n\n\n\nmodel-dof\ndeviance\nχ²\nχ²-dof\nP(&gt;χ²)\n\n\n\n\nzScore ~ 1 + Test + a1 + Sex + Test & a1 + Test & Sex + a1 & Sex + Test & a1 & Sex + zerocorr(1 + Test | Child)\n26\n13134\n\n\n\n\n\nzScore ~ 1 + Test + a1 + Sex + Test & a1 + Test & Sex + a1 & Sex + Test & a1 & Sex + (1 + Test | Child)\n36\n13249\n-115\n10\nNaN",
    "crumbs": [
      "Contrast coding",
      "Mixed Models Tutorial: Contrast Coding"
    ]
  },
  {
    "objectID": "contrasts_fggk21.html#contrasts-are-re-parameterizations-of-the-same-model",
    "href": "contrasts_fggk21.html#contrasts-are-re-parameterizations-of-the-same-model",
    "title": "Mixed Models Tutorial: Contrast Coding",
    "section": "3.1 Contrasts are re-parameterizations of the same model",
    "text": "3.1 Contrasts are re-parameterizations of the same model\nThe choice of contrast does not affect the model objective, in other words, they all yield the same goodness of fit. It does not matter whether a contrast is orthogonal or not.\n\n[\n  objective(m_ovi_SeqDiff),\n  objective(m_ovi_Helmert),\n  objective(m_ovi_Hypo),\n]\n\n3-element Vector{Float64}:\n 13876.09548624337\n 13876.09548624337\n 13876.095486243365",
    "crumbs": [
      "Contrast coding",
      "Mixed Models Tutorial: Contrast Coding"
    ]
  },
  {
    "objectID": "contrasts_fggk21.html#vcs-and-cps-depend-on-contrast-coding",
    "href": "contrasts_fggk21.html#vcs-and-cps-depend-on-contrast-coding",
    "title": "Mixed Models Tutorial: Contrast Coding",
    "section": "3.2 VCs and CPs depend on contrast coding",
    "text": "3.2 VCs and CPs depend on contrast coding\nTrivially, the meaning of a contrast depends on its definition. Consequently, the contrast specification has a big effect on the random-effect structure. As an illustration, we refit the LMMs with variance components (VCs) and correlation parameters (CPs) for Child-related contrasts of Test. Unfortunately, it is not easy, actually rather quite difficult, to grasp the meaning of correlations of contrast-based effects; they represent two-way interactions.\n\nbegin\n  f_Child = @formula zScore ~\n    1 + Test * a1 * Sex + (1 + Test | Child)\n  m_Child_SDC = fit(MixedModel, f_Child, dat; contrasts=contr1)\n  m_Child_HeC = fit(MixedModel, f_Child, dat; contrasts=contr2)\n  m_Child_HyC = fit(MixedModel, f_Child, dat; contrasts=contr3)\n  m_Child_PCA = fit(MixedModel, f_Child, dat; contrasts=contr4)\nend\n\nMinimizing 1534    Time: 0:00:10 ( 6.68 ms/it)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEst.\nSE\nz\np\nσ_Child\n\n\n\n\n(Intercept)\n-0.0105\n0.0141\n-0.74\n0.4594\n0.6430\n\n\nTest: c5.1\n-0.0113\n0.0429\n-0.26\n0.7920\n1.4300\n\n\nTest: c234.15\n0.0073\n0.1683\n0.04\n0.9656\n0.9880\n\n\nTest: c2.34\n-0.0117\n0.0781\n-0.15\n0.8813\n1.3538\n\n\nTest: c3.4\n-0.0113\n0.0452\n-0.25\n0.8033\n1.6103\n\n\na1\n0.2378\n0.0478\n4.97\n&lt;1e-06\n\n\n\nSex: Boys\n0.2043\n0.0141\n14.44\n&lt;1e-46\n\n\n\nTest: c5.1 & a1\n0.4108\n0.1465\n2.80\n0.0050\n\n\n\nTest: c234.15 & a1\n-0.8395\n0.5708\n-1.47\n0.1414\n\n\n\nTest: c2.34 & a1\n-0.0799\n0.2623\n-0.30\n0.7607\n\n\n\nTest: c3.4 & a1\n0.2216\n0.1515\n1.46\n0.1437\n\n\n\nTest: c5.1 & Sex: Boys\n0.0542\n0.0429\n1.27\n0.2059\n\n\n\nTest: c234.15 & Sex: Boys\n-0.7856\n0.1683\n-4.67\n&lt;1e-05\n\n\n\nTest: c2.34 & Sex: Boys\n-0.0471\n0.0781\n-0.60\n0.5465\n\n\n\nTest: c3.4 & Sex: Boys\n0.0161\n0.0452\n0.36\n0.7224\n\n\n\na1 & Sex: Boys\n0.0487\n0.0478\n1.02\n0.3081\n\n\n\nTest: c5.1 & a1 & Sex: Boys\n-0.2955\n0.1465\n-2.02\n0.0436\n\n\n\nTest: c234.15 & a1 & Sex: Boys\n0.3089\n0.5708\n0.54\n0.5884\n\n\n\nTest: c2.34 & a1 & Sex: Boys\n-0.0625\n0.2623\n-0.24\n0.8117\n\n\n\nTest: c3.4 & a1 & Sex: Boys\n0.0929\n0.1515\n0.61\n0.5400\n\n\n\nResidual\n0.0000\n\n\n\n\n\n\n\n\n\n\nVarCorr(m_Child_SDC)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nColumn\nVariance\nStd.Dev\nCorr.\n\n\n\n\n\n\n\nChild\n(Intercept)\n0.407724\n0.638533\n\n\n\n\n\n\n\nTest: Star_r\n1.056832\n1.028024\n+0.43\n\n\n\n\n\n\nTest: S20_r\n0.813550\n0.901970\n-0.10\n+0.39\n\n\n\n\n\nTest: SLJ\n0.467706\n0.683890\n+0.28\n-0.61\n-0.47\n\n\n\n\nTest: BPT\n1.144422\n1.069777\n-0.39\n-0.47\n+0.23\n-0.25\n\n\nResidual\n\n0.000000\n0.000011\n\n\n\n\n\n\n\n\n\n\nVarCorr(m_Child_HeC)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nColumn\nVariance\nStd.Dev\nCorr.\n\n\n\n\n\n\n\nChild\n(Intercept)\n0.35474562\n0.59560526\n\n\n\n\n\n\n\nTest: Star_r\n0.29928970\n0.54707376\n+0.42\n\n\n\n\n\n\nTest: S20_r\n0.22589565\n0.47528481\n-0.12\n+0.46\n\n\n\n\n\nTest: SLJ\n0.03500164\n0.18708724\n+0.30\n-0.39\n+0.27\n\n\n\n\nTest: BPT\n0.02756931\n0.16604008\n+0.11\n+0.52\n+0.03\n-0.48\n\n\nResidual\n\n0.00000000\n0.00002613\n\n\n\n\n\n\n\n\n\n\nVarCorr(m_Child_HyC)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nColumn\nVariance\nStd.Dev\nCorr.\n\n\n\n\n\n\n\nChild\n(Intercept)\n0.40400839\n0.63561654\n\n\n\n\n\n\n\nTest: BPT-other\n2.22593133\n1.49195554\n+0.98\n\n\n\n\n\n\nTest: Star-End\n1.23204816\n1.10997665\n+0.01\n+0.23\n\n\n\n\n\nTest: S20-Star\n1.75345888\n1.32418235\n-0.16\n-0.17\n-0.06\n\n\n\n\nTest: SLJ-S20\n1.42782262\n1.19491532\n+0.25\n+0.06\n-0.84\n-0.32\n\n\nResidual\n\n0.00000000\n0.00000437\n\n\n\n\n\n\n\n\n\n\nVarCorr(m_Child_PCA)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nColumn\nVariance\nStd.Dev\nCorr.\n\n\n\n\n\n\n\nChild\n(Intercept)\n0.41338814\n0.64295267\n\n\n\n\n\n\n\nTest: c5.1\n2.04484693\n1.42998144\n-0.14\n\n\n\n\n\n\nTest: c234.15\n0.97608251\n0.98796888\n+0.33\n-0.72\n\n\n\n\n\nTest: c2.34\n1.83288851\n1.35384213\n+0.60\n+0.50\n-0.12\n\n\n\n\nTest: c3.4\n2.59301071\n1.61028281\n-0.11\n-0.16\n+0.70\n-0.10\n\n\nResidual\n\n0.00000000\n0.00001636\n\n\n\n\n\n\n\n\n\nThe CPs for the various contrasts are in line with expectations. For the SDC we observe substantial negative CPs between neighboring contrasts. For the orthogonal HeC, all CPs are small; they are uncorrelated. HyC contains some of the SDC contrasts and we observe again the negative CPs. The (roughly) PCA-based contrasts are small with one exception; there is a sizeable CP of +.41 between GM and the core of adjusted physical fitness (c234.15).\nDo these differences in CPs imply that we can move to zcpLMMs when we have orthogonal contrasts? We pursue this question with by refitting the four LMMs with zerocorr() and compare the goodness of fit.\n\nbegin\n  f_Child0 = @formula zScore ~\n    1 + Test * a1 * Sex + zerocorr(1 + Test | Child)\n  m_Child_SDC0 = fit(MixedModel, f_Child0, dat; contrasts=contr1)\n  m_Child_HeC0 = fit(MixedModel, f_Child0, dat; contrasts=contr2)\n  m_Child_HyC0 = fit(MixedModel, f_Child0, dat; contrasts=contr3)\n  m_Child_PCA0 = fit(MixedModel, f_Child0, dat; contrasts=contr4)\nend\n\nMinimizing 547    Time: 0:00:01 ( 3.47 ms/it)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEst.\nSE\nz\np\nσ_Child\n\n\n\n\n(Intercept)\n-0.0105\n0.0141\n-0.74\n0.4593\n0.6726\n\n\nTest: c5.1\n-0.0135\n0.0434\n-0.31\n0.7563\n1.3181\n\n\nTest: c234.15\n0.0266\n0.1691\n0.16\n0.8753\n1.5594\n\n\nTest: c2.34\n-0.0038\n0.0782\n-0.05\n0.9615\n2.1713\n\n\nTest: c3.4\n-0.0094\n0.0446\n-0.21\n0.8327\n1.1917\n\n\na1\n0.2323\n0.0478\n4.86\n&lt;1e-05\n\n\n\nSex: Boys\n0.2043\n0.0141\n14.45\n&lt;1e-46\n\n\n\nTest: c5.1 & a1\n0.4159\n0.1481\n2.81\n0.0050\n\n\n\nTest: c234.15 & a1\n-0.9476\n0.5737\n-1.65\n0.0986\n\n\n\nTest: c2.34 & a1\n-0.0278\n0.2631\n-0.11\n0.9157\n\n\n\nTest: c3.4 & a1\n0.1935\n0.1494\n1.30\n0.1952\n\n\n\nTest: c5.1 & Sex: Boys\n0.0529\n0.0434\n1.22\n0.2220\n\n\n\nTest: c234.15 & Sex: Boys\n-0.7771\n0.1691\n-4.59\n&lt;1e-05\n\n\n\nTest: c2.34 & Sex: Boys\n-0.0490\n0.0782\n-0.63\n0.5314\n\n\n\nTest: c3.4 & Sex: Boys\n0.0177\n0.0446\n0.40\n0.6915\n\n\n\na1 & Sex: Boys\n0.0468\n0.0478\n0.98\n0.3276\n\n\n\nTest: c5.1 & a1 & Sex: Boys\n-0.3017\n0.1481\n-2.04\n0.0417\n\n\n\nTest: c234.15 & a1 & Sex: Boys\n0.3045\n0.5737\n0.53\n0.5956\n\n\n\nTest: c2.34 & a1 & Sex: Boys\n-0.0782\n0.2631\n-0.30\n0.7663\n\n\n\nTest: c3.4 & a1 & Sex: Boys\n0.1068\n0.1494\n0.72\n0.4746\n\n\n\nResidual\n0.0000\n\n\n\n\n\n\n\n\n\n\nMixedModels.likelihoodratiotest(m_Child_SDC0, m_Child_SDC)\n\n\n\n\n\n\n\n\n\n\n\n\n\nmodel-dof\ndeviance\nχ²\nχ²-dof\nP(&gt;χ²)\n\n\n\n\nzScore ~ 1 + Test + a1 + Sex + Test & a1 + Test & Sex + a1 & Sex + Test & a1 & Sex + zerocorr(1 + Test | Child)\n26\n13874\n\n\n\n\n\nzScore ~ 1 + Test + a1 + Sex + Test & a1 + Test & Sex + a1 & Sex + Test & a1 & Sex + (1 + Test | Child)\n36\n13276\n597\n10\n&lt;1e-99\n\n\n\n\n\n\nMixedModels.likelihoodratiotest(m_Child_HeC0, m_Child_HeC)\n\n\n\n\n\n\n\n\n\n\n\n\n\nmodel-dof\ndeviance\nχ²\nχ²-dof\nP(&gt;χ²)\n\n\n\n\nzScore ~ 1 + Test + a1 + Sex + Test & a1 + Test & Sex + a1 & Sex + Test & a1 & Sex + zerocorr(1 + Test | Child)\n26\n13242\n\n\n\n\n\nzScore ~ 1 + Test + a1 + Sex + Test & a1 + Test & Sex + a1 & Sex + Test & a1 & Sex + (1 + Test | Child)\n36\n13309\n-67\n10\nNaN\n\n\n\n\n\n\nMixedModels.likelihoodratiotest(m_Child_HyC0, m_Child_HyC)\n\n\n\n\n\n\n\n\n\n\n\n\n\nmodel-dof\ndeviance\nχ²\nχ²-dof\nP(&gt;χ²)\n\n\n\n\nzScore ~ 1 + Test + a1 + Sex + Test & a1 + Test & Sex + a1 & Sex + Test & a1 & Sex + zerocorr(1 + Test | Child)\n26\n13228\n\n\n\n\n\nzScore ~ 1 + Test + a1 + Sex + Test & a1 + Test & Sex + a1 & Sex + Test & a1 & Sex + (1 + Test | Child)\n36\n13226\n3\n10\n0.9903\n\n\n\n\n\n\nMixedModels.likelihoodratiotest(m_Child_PCA0, m_Child_PCA)\n\n\n\n\n\n\n\n\n\n\n\n\n\nmodel-dof\ndeviance\nχ²\nχ²-dof\nP(&gt;χ²)\n\n\n\n\nzScore ~ 1 + Test + a1 + Sex + Test & a1 + Test & Sex + a1 & Sex + Test & a1 & Sex + zerocorr(1 + Test | Child)\n26\n13194\n\n\n\n\n\nzScore ~ 1 + Test + a1 + Sex + Test & a1 + Test & Sex + a1 & Sex + Test & a1 & Sex + (1 + Test | Child)\n36\n13300\n-106\n10\nNaN\n\n\n\n\n\nObviously, we can not drop CPs from any of the LMMs. The full LMMs all have the same objective, but we can compare the goodness-of-fit statistics of zcpLMMs more directly.\n\nbegin\n  zcpLMM = [\"SDC0\", \"HeC0\", \"HyC0\", \"PCA0\"]\n  mods = [m_Child_SDC0, m_Child_HeC0, m_Child_HyC0, m_Child_PCA0]\n  gof_summary = sort!(\n    DataFrame(;\n      zcpLMM=zcpLMM,\n      dof=dof.(mods),\n      deviance=deviance.(mods),\n      AIC=aic.(mods),\n      BIC=bic.(mods),\n    ),\n    :deviance,\n  )\nend\n\n4×5 DataFrame\n\n\n\nRow\nzcpLMM\ndof\ndeviance\nAIC\nBIC\n\n\n\nString\nInt64\nFloat64\nFloat64\nFloat64\n\n\n\n\n1\nPCA0\n26\n13194.0\n13246.0\n13415.4\n\n\n2\nHyC0\n26\n13228.2\n13280.2\n13449.6\n\n\n3\nHeC0\n26\n13242.0\n13294.0\n13463.5\n\n\n4\nSDC0\n26\n13873.9\n13925.9\n14095.4\n\n\n\n\n\n\nThe best fit was obtained for the PCA-based zcpLMM. Somewhat surprisingly the second best fit was obtained for the SDC. The relatively poor performance of HeC-based zcpLMM is puzzling to me. I thought it might be related to imbalance in design in the present data, but this does not appear to be the case. The same comparison of SequentialDifferenceCoding and Helmert Coding also showed a worse fit for the zcp-HeC LMM than the zcp-SDC LMM.",
    "crumbs": [
      "Contrast coding",
      "Mixed Models Tutorial: Contrast Coding"
    ]
  },
  {
    "objectID": "contrasts_fggk21.html#vcs-and-cps-depend-on-random-factor",
    "href": "contrasts_fggk21.html#vcs-and-cps-depend-on-random-factor",
    "title": "Mixed Models Tutorial: Contrast Coding",
    "section": "3.3 VCs and CPs depend on random factor",
    "text": "3.3 VCs and CPs depend on random factor\nVCs and CPs resulting from a set of test contrasts can also be estimated for the random factor School. Of course, these VCs and CPs may look different from the ones we just estimated for Child.\nThe effect of age (i.e., developmental gain) varies within School. Therefore, we also include its VCs and CPs in this model; the school-related VC for Sex was not significant.\n\nf_School = @formula zScore ~\n  1 + Test * a1 * Sex + (1 + Test + a1 | School);\nm_School_SeqDiff = fit(MixedModel, f_School, dat; contrasts=contr1);\nm_School_Helmert = fit(MixedModel, f_School, dat; contrasts=contr2);\nm_School_Hypo = fit(MixedModel, f_School, dat; contrasts=contr3);\nm_School_PCA = fit(MixedModel, f_School, dat; contrasts=contr4);\n\nMinimizing 1240    Time: 0:00:00 ( 0.69 ms/it)\n\n\n\nVarCorr(m_School_SeqDiff)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nColumn\nVariance\nStd.Dev\nCorr.\n\n\n\n\n\n\n\n\nSchool\n(Intercept)\n0.037707\n0.194184\n\n\n\n\n\n\n\n\nTest: Star_r\n0.106569\n0.326449\n+0.35\n\n\n\n\n\n\n\nTest: S20_r\n0.063627\n0.252244\n+0.21\n-0.07\n\n\n\n\n\n\nTest: SLJ\n0.129961\n0.360501\n-0.08\n-0.71\n-0.27\n\n\n\n\n\nTest: BPT\n0.149083\n0.386113\n-0.48\n+0.54\n+0.03\n-0.81\n\n\n\n\na1\n0.025147\n0.158577\n-0.44\n-0.19\n-0.93\n+0.35\n+0.01\n\n\nResidual\n\n0.862747\n0.928842\n\n\n\n\n\n\n\n\n\n\n\nVarCorr(m_School_Helmert)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nColumn\nVariance\nStd.Dev\nCorr.\n\n\n\n\n\n\n\n\nSchool\n(Intercept)\n0.0377016\n0.1941691\n\n\n\n\n\n\n\n\nTest: Star_r\n0.0266388\n0.1632140\n+0.35\n\n\n\n\n\n\n\nTest: S20_r\n0.0093783\n0.0968416\n+0.37\n+0.50\n\n\n\n\n\n\nTest: SLJ\n0.0049383\n0.0702729\n+0.15\n-0.57\n-0.12\n\n\n\n\n\nTest: BPT\n0.0024522\n0.0495201\n-0.63\n+0.35\n+0.40\n-0.41\n\n\n\n\na1\n0.0249275\n0.1578846\n-0.45\n-0.19\n-0.92\n-0.18\n-0.14\n\n\nResidual\n\n0.8628151\n0.9288784\n\n\n\n\n\n\n\n\n\n\n\nVarCorr(m_School_Hypo)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nColumn\nVariance\nStd.Dev\nCorr.\n\n\n\n\n\n\n\n\nSchool\n(Intercept)\n0.037710\n0.194191\n\n\n\n\n\n\n\n\nTest: BPT-other\n0.984103\n0.992019\n-0.62\n\n\n\n\n\n\n\nTest: Star-End\n0.106587\n0.326477\n+0.35\n+0.35\n\n\n\n\n\n\nTest: S20-Star\n0.063625\n0.252240\n+0.21\n+0.24\n-0.07\n\n\n\n\n\nTest: SLJ-S20\n0.129977\n0.360523\n-0.08\n-0.54\n-0.71\n-0.27\n\n\n\n\na1\n0.025203\n0.158753\n-0.44\n-0.13\n-0.19\n-0.93\n+0.35\n\n\nResidual\n\n0.862733\n0.928834\n\n\n\n\n\n\n\n\n\n\n\nVarCorr(m_School_PCA)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nColumn\nVariance\nStd.Dev\nCorr.\n\n\n\n\n\n\n\n\nSchool\n(Intercept)\n0.037703\n0.194174\n\n\n\n\n\n\n\n\nTest: c5.1\n0.136693\n0.369720\n-0.14\n\n\n\n\n\n\n\nTest: c234.15\n1.104159\n1.050790\n+0.94\n+0.15\n\n\n\n\n\n\nTest: c2.34\n0.287138\n0.535853\n-0.14\n+0.09\n-0.10\n\n\n\n\n\nTest: c3.4\n0.129866\n0.360369\n+0.08\n+0.68\n+0.18\n+0.42\n\n\n\n\na1\n0.025136\n0.158543\n-0.44\n-0.45\n-0.53\n+0.64\n-0.35\n\n\nResidual\n\n0.862773\n0.928856\n\n\n\n\n\n\n\n\n\n\nWe compare again how much of the fit resides in the CPs.\n\nbegin\n  f_School0 = @formula zScore ~\n    1 + Test * a1 * Sex + zerocorr(1 + Test + a1 | School)\n  m_School_SDC0 = fit(MixedModel, f_School0, dat; contrasts=contr1)\n  m_School_HeC0 = fit(MixedModel, f_School0, dat; contrasts=contr2)\n  m_School_HyC0 = fit(MixedModel, f_School0, dat; contrasts=contr3)\n  m_School_PCA0 = fit(MixedModel, f_School0, dat; contrasts=contr4)\n  #\n  zcpLMM2 = [\"SDC0\", \"HeC0\", \"HyC0\", \"PCA0\"]\n  mods2 = [\n    m_School_SDC0, m_School_HeC0, m_School_HyC0, m_School_PCA0\n  ]\n  gof_summary2 = sort!(\n    DataFrame(;\n      zcpLMM=zcpLMM2,\n      dof=dof.(mods2),\n      deviance=deviance.(mods2),\n      AIC=aic.(mods2),\n      BIC=bic.(mods2),\n    ),\n    :deviance,\n  )\nend\n\nMinimizing 223    Time: 0:00:00 ( 0.46 ms/it)\n  objective:  13848.77736950177\n\n\n4×5 DataFrame\n\n\n\nRow\nzcpLMM\ndof\ndeviance\nAIC\nBIC\n\n\n\nString\nInt64\nFloat64\nFloat64\nFloat64\n\n\n\n\n1\nPCA0\n27\n13848.8\n13902.8\n14078.7\n\n\n2\nHeC0\n27\n13849.6\n13903.6\n14079.6\n\n\n3\nHyC0\n27\n13853.4\n13907.4\n14083.3\n\n\n4\nSDC0\n27\n13857.1\n13911.1\n14087.1\n\n\n\n\n\n\nFor the random factor School the Helmert contrast, followed by PCA-based contrasts have least information in the CPs; SDC has the largest contribution from CPs. Interesting.",
    "crumbs": [
      "Contrast coding",
      "Mixed Models Tutorial: Contrast Coding"
    ]
  },
  {
    "objectID": "kkl15.html",
    "href": "kkl15.html",
    "title": "RePsychLing Kliegl, Kuschela, & Laubrock (2015)- Reduction of Model Complexity",
    "section": "",
    "text": "Kliegl et al. (2015) is a follow-up to Kliegl et al. (2011) (see also script kwdyz11.qmd) from an experiment looking at a variety of effects of visual cueing under four different cue-target relations (CTRs). In this experiment two rectangles are displayed (1) in horizontal orientation , (2) in vertical orientation, (3) in left diagonal orientation, or in (4) right diagonal orientation relative to a central fixation point. Subjects react to the onset of a small or a large visual target occurring at one of the four ends of the two rectangles. The target is cued validly on 70% of trials by a brief flash of the corner of the rectangle at which it appears; it is cued invalidly at the three other locations 10% of the trials each. This implies a latent imbalance in design that is not visible in the repeated-measures ANOVA, but we will show its effect in the random-effect structure and conditional modes.\nThere are a couple of differences between the first and this follow-up experiment, rendering it more a conceptual than a direct replication. First, the original experiment was carried out at Peking University and this follow-up at Potsdam University. Second, diagonal orientations of rectangles and large target sizes were not part of the design of Kliegl et al. (2011).\nWe specify three contrasts for the four-level factor CTR that are derived from spatial, object-based, and attractor-like features of attention. They map onto sequential differences between appropriately ordered factor levels. Replicating Kliegl et al. (2011), the attraction effect was not significant as a fixed effect, but yielded a highly reliable variance component (VC; i.e., reliable individual differences in positive and negative attraction effects cancel the fixed effect). Moreover, these individual differences in the attraction effect were negatively correlated with those in the spatial effect.\nThis comparison is of interest because a few years after the publication of Kliegl et al. (2011), the theoretically critical correlation parameter (CP) between the spatial effect and the attraction effect was determined as the source of a non-singular LMM in that paper. The present study served the purpose to estimate this parameter with a larger sample and a wider variety of experimental conditions.\nHere we also include two additional experimental manipulations of target size and orientation of cue rectangle. A similar analysis was reported in the parsimonious mixed-model paper (Bates et al., 2015); it was also used in a paper of GAMMs (Baayen et al., 2017). Data and R scripts of those analyses are also available in R-package RePsychLing.\nThe analysis is based on log-transformed reaction times lrt, indicated by a boxcox() check of model residuals.\nIn this vignette we focus on the reduction of model complexity. And we start with a quote:\n“Neither the [maximal] nor the [minimal] linear mixed models are appropriate for most repeated measures analysis. Using the [maximal] model is generally wasteful and costly in terms of statiscal power for testing hypotheses. On the other hand, the [minimal] model fails to account for nontrivial correlation among repeated measurements. This results in inflated [T]ype I error rates when non-negligible correlation does in fact exist. We can usually find middle ground, a covariance model that adequately accounts for correlation but is more parsimonious than the [maximal] model. Doing so allows us full control over [T]ype I error rates without needlessly sacrificing power.”\nStroup, W. W. (2012, p. 185). Generalized linear mixed models: Modern concepts, methods and applica?ons. CRC Press, Boca Raton.",
    "crumbs": [
      "Worked examples",
      "RePsychLing Kliegl, Kuschela, & Laubrock (2015)- Reduction of Model Complexity"
    ]
  },
  {
    "objectID": "kkl15.html#contrasts",
    "href": "kkl15.html#contrasts",
    "title": "RePsychLing Kliegl, Kuschela, & Laubrock (2015)- Reduction of Model Complexity",
    "section": "4.1 Contrasts",
    "text": "4.1 Contrasts\n\ncontrasts = Dict(\n  :Subj =&gt; Grouping(),\n  :CTR =&gt; SeqDiffCoding(; levels=[\"val\", \"sod\", \"dos\", \"dod\"]),\n  :cardinal =&gt; EffectsCoding(; levels=[\"cardinal\", \"diagonal\"]),\n  :size =&gt; EffectsCoding(; levels=[\"big\", \"small\"])\n)\n\nDict{Symbol, StatsModels.AbstractContrasts} with 4 entries:\n  :CTR      =&gt; SeqDiffCoding([\"val\", \"sod\", \"dos\", \"dod\"])\n  :size     =&gt; EffectsCoding(nothing, [\"big\", \"small\"])\n  :Subj     =&gt; Grouping()\n  :cardinal =&gt; EffectsCoding(nothing, [\"cardinal\", \"diagonal\"])\n\n\n\nm_max_rt = let\n  form = @formula rt ~ 1 + CTR * size * cardinal + \n                           (1 + CTR * size * cardinal | Subj)\n  fit(MixedModel, form, dat; contrasts)\nend\n\nMinimizing 3872    Time: 0:00:06 ( 1.59 ms/it)\n  objective:  599297.6610000695\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEst.\nSE\nz\np\nσ_Subj\n\n\n\n\n(Intercept)\n308.4078\n5.8365\n52.84\n&lt;1e-99\n0.0000\n\n\nCTR: sod\n23.2685\n2.5676\n9.06\n&lt;1e-18\n9.9631\n\n\nCTR: dos\n13.0802\n1.5433\n8.48\n&lt;1e-16\n5.0118\n\n\nCTR: dod\n2.7724\n2.0921\n1.33\n0.1851\n13.1147\n\n\nsize: small\n26.4130\n5.8365\n4.53\n&lt;1e-05\n54.0095\n\n\ncardinal: diagonal\n6.6749\n1.7890\n3.73\n0.0002\n9.7692\n\n\nCTR: sod & size: small\n8.7924\n2.5676\n3.42\n0.0006\n19.8731\n\n\nCTR: dos & size: small\n-0.7061\n1.5433\n-0.46\n0.6473\n7.2502\n\n\nCTR: dod & size: small\n7.4980\n2.0921\n3.58\n0.0003\n8.7387\n\n\nCTR: sod & cardinal: diagonal\n3.6315\n1.1022\n3.29\n0.0010\n4.2292\n\n\nCTR: dos & cardinal: diagonal\n1.3344\n1.2450\n1.07\n0.2838\n1.7717\n\n\nCTR: dod & cardinal: diagonal\n-0.2247\n1.3224\n-0.17\n0.8651\n4.0782\n\n\nsize: small & cardinal: diagonal\n2.0502\n1.7890\n1.15\n0.2518\n12.9327\n\n\nCTR: sod & size: small & cardinal: diagonal\n-0.7638\n1.1022\n-0.69\n0.4883\n3.7198\n\n\nCTR: dos & size: small & cardinal: diagonal\n-0.1556\n1.2450\n-0.12\n0.9005\n1.7390\n\n\nCTR: dod & size: small & cardinal: diagonal\n4.1768\n1.3224\n3.16\n0.0016\n2.3972\n\n\nResidual\n63.0091\n\n\n\n\n\n\n\n\n\n\nm_cpx_rt = let\n  form = @formula rt ~ 1 + CTR * size * cardinal + \n                           (1 + CTR + size + cardinal | Subj)\n  fit(MixedModel, form, dat; contrasts)\nend\n\nMinimizing 770    Time: 0:00:00 ( 0.25 ms/it)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEst.\nSE\nz\np\nσ_Subj\n\n\n\n\n(Intercept)\n308.4074\n6.0527\n50.95\n&lt;1e-99\n40.4433\n\n\nCTR: sod\n23.2723\n2.4739\n9.41\n&lt;1e-20\n21.2925\n\n\nCTR: dos\n13.1099\n1.4621\n8.97\n&lt;1e-18\n7.5060\n\n\nCTR: dod\n2.7140\n1.9182\n1.41\n0.1571\n13.7125\n\n\nsize: small\n26.3865\n6.0527\n4.36\n&lt;1e-04\n38.7600\n\n\ncardinal: diagonal\n6.6506\n1.7311\n3.84\n0.0001\n15.6567\n\n\nCTR: sod & size: small\n8.7956\n2.4739\n3.56\n0.0004\n\n\n\nCTR: dos & size: small\n-0.7230\n1.4621\n-0.49\n0.6209\n\n\n\nCTR: dod & size: small\n7.4191\n1.9182\n3.87\n0.0001\n\n\n\nCTR: sod & cardinal: diagonal\n3.6414\n0.9210\n3.95\n&lt;1e-04\n\n\n\nCTR: dos & cardinal: diagonal\n1.3276\n1.2176\n1.09\n0.2755\n\n\n\nCTR: dod & cardinal: diagonal\n-0.3142\n1.2217\n-0.26\n0.7970\n\n\n\nsize: small & cardinal: diagonal\n2.0470\n1.7311\n1.18\n0.2370\n\n\n\nCTR: sod & size: small & cardinal: diagonal\n-0.7611\n0.9210\n-0.83\n0.4086\n\n\n\nCTR: dos & size: small & cardinal: diagonal\n-0.1334\n1.2176\n-0.11\n0.9128\n\n\n\nCTR: dod & size: small & cardinal: diagonal\n4.1115\n1.2217\n3.37\n0.0008\n\n\n\nResidual\n63.1011",
    "crumbs": [
      "Worked examples",
      "RePsychLing Kliegl, Kuschela, & Laubrock (2015)- Reduction of Model Complexity"
    ]
  },
  {
    "objectID": "kkl15.html#box-cox",
    "href": "kkl15.html#box-cox",
    "title": "RePsychLing Kliegl, Kuschela, & Laubrock (2015)- Reduction of Model Complexity",
    "section": "4.2 Box-Cox",
    "text": "4.2 Box-Cox\n\nbc1 = fit(BoxCoxTransformation, m_max_rt)\n\n\nbc2 = fit(BoxCoxTransformation, m_cpx_rt)\n\nMinimizing 1191    Time: 0:00:00 ( 0.14 ms/it)\n\n\nBox-Cox transformation\n\nestimated λ: -0.7062\nresultant transformation:\n\n y^-0.7 - 1\n------------\n    -0.7\n\n\n\nboxcoxplot(bc2; conf_level=0.95)\n\nClear evidence for skew. Traditionally, we used log transforms for reaction times. even stronger than log. We stay with log for now. Could try 1/sqrt(rt).",
    "crumbs": [
      "Worked examples",
      "RePsychLing Kliegl, Kuschela, & Laubrock (2015)- Reduction of Model Complexity"
    ]
  },
  {
    "objectID": "kkl15.html#zero-correlation-parameter-lmm-1",
    "href": "kkl15.html#zero-correlation-parameter-lmm-1",
    "title": "RePsychLing Kliegl, Kuschela, & Laubrock (2015)- Reduction of Model Complexity",
    "section": "6.1 Zero-correlation parameter LMM (1)",
    "text": "6.1 Zero-correlation parameter LMM (1)\nForce CPs to zero.\n\nm_zcp1 = let\n  form = @formula log(rt) ~ 1 + CTR * size * cardinal + \n                   zerocorr(1 + CTR * size * cardinal | Subj)\n  fit(MixedModel, form, dat; contrasts)\nend\n\nMinimizing 637    Time: 0:00:00 ( 0.32 ms/it)\n  objective:  -24588.607469898558\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEst.\nSE\nz\np\nσ_Subj\n\n\n\n\n(Intercept)\n5.6910\n0.0173\n329.51\n&lt;1e-99\n0.0151\n\n\nCTR: sod\n0.0744\n0.0077\n9.66\n&lt;1e-21\n0.0109\n\n\nCTR: dos\n0.0409\n0.0045\n9.09\n&lt;1e-19\n0.0240\n\n\nCTR: dod\n0.0015\n0.0053\n0.29\n0.7749\n0.0299\n\n\nsize: small\n0.0921\n0.0173\n5.33\n&lt;1e-07\n0.1591\n\n\ncardinal: diagonal\n0.0204\n0.0052\n3.92\n&lt;1e-04\n0.0312\n\n\nCTR: sod & size: small\n0.0244\n0.0077\n3.17\n0.0015\n0.0657\n\n\nCTR: dos & size: small\n-0.0053\n0.0045\n-1.19\n0.2344\n0.0010\n\n\nCTR: dod & size: small\n0.0181\n0.0053\n3.39\n0.0007\n0.0196\n\n\nCTR: sod & cardinal: diagonal\n0.0101\n0.0033\n3.03\n0.0025\n0.0172\n\n\nCTR: dos & cardinal: diagonal\n0.0046\n0.0037\n1.26\n0.2092\n0.0000\n\n\nCTR: dod & cardinal: diagonal\n-0.0055\n0.0037\n-1.49\n0.1358\n0.0033\n\n\nsize: small & cardinal: diagonal\n0.0041\n0.0052\n0.79\n0.4291\n0.0354\n\n\nCTR: sod & size: small & cardinal: diagonal\n-0.0032\n0.0033\n-0.95\n0.3427\n0.0008\n\n\nCTR: dos & size: small & cardinal: diagonal\n-0.0005\n0.0037\n-0.13\n0.8945\n0.0000\n\n\nCTR: dod & size: small & cardinal: diagonal\n0.0109\n0.0037\n2.94\n0.0033\n0.0010\n\n\nResidual\n0.1903\n\n\n\n\n\n\n\n\n\n\nissingular(m_zcp1)\n\ntrue\n\n\n\nonly(MixedModels.PCA(m_zcp1))\n\n\nPrincipal components based on correlation matrix\n (Intercept)                                  …  .    .    .    .    .    .    .\n CTR: sod                                        .    .    .    .    .    .    .\n CTR: dos                                        .    .    .    .    .    .    .\n CTR: dod                                        .    .    .    .    .    .    .\n size: small                                     .    .    .    .    .    .    .\n CTR: sod & size: small                       …  .    .    .    .    .    .    .\n CTR: dos & size: small                          .    .    .    .    .    .    .\n CTR: dod & size: small                          .    .    .    .    .    .    .\n cardinal: diagonal                              .    .    .    .    .    .    .\n CTR: sod & cardinal: diagonal                   1.0  .    .    .    .    .    .\n CTR: dos & cardinal: diagonal                …  0.0  0.0  .    .    .    .    .\n CTR: dod & cardinal: diagonal                   0.0  0.0  1.0  .    .    .    .\n size: small & cardinal: diagonal                0.0  0.0  0.0  1.0  .    .    .\n CTR: sod & size: small & cardinal: diagonal     0.0  0.0  0.0  0.0  1.0  .    .\n CTR: dos & size: small & cardinal: diagonal     0.0  0.0  0.0  0.0  0.0  0.0  .\n CTR: dod & size: small & cardinal: diagonal  …  0.0  0.0  0.0  0.0  0.0  0.0  1.0\n\nNormalized cumulative variances:\n[0.0714, 0.1429, 0.2143, 0.2857, 0.3571, 0.4286, 0.5, 0.5714, 0.6429, 0.7143, 0.7857, 0.8571, 0.9286, 1.0, 1.0, 1.0]\n\nComponent loadings\n                                              …   PC13   PC14     PC15     PC16\n (Intercept)                                     0.0    0.0      0.0      0.0\n CTR: sod                                        0.0    0.0      0.0      0.0\n CTR: dos                                        0.0    0.0      0.0      0.0\n CTR: dod                                        0.0    0.0      0.0      0.0\n size: small                                  …  0.0    0.0      0.0      0.0\n CTR: sod & size: small                          0.0    0.0      0.0      0.0\n CTR: dos & size: small                          0.0    0.0      0.0      0.0\n CTR: dod & size: small                          1.0    0.0      0.0      0.0\n cardinal: diagonal                              0.0    1.0      0.0      0.0\n CTR: sod & cardinal: diagonal                …  0.0    0.0      0.0      0.0\n CTR: dos & cardinal: diagonal                   0.0    0.0    NaN        0.0\n CTR: dod & cardinal: diagonal                   0.0    0.0      0.0      0.0\n size: small & cardinal: diagonal                0.0    0.0      0.0      0.0\n CTR: sod & size: small & cardinal: diagonal     0.0    0.0      0.0      0.0\n CTR: dos & size: small & cardinal: diagonal  …  0.0    0.0      0.0    NaN\n CTR: dod & size: small & cardinal: diagonal     0.0    0.0      0.0      0.0\n\n\n\nVarCorr(m_zcp1)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nColumn\nVariance\nStd.Dev\nCorr.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSubj\n(Intercept)\n0.000228119\n0.015103595\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCTR: sod\n0.000118184\n0.010871228\n.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCTR: dos\n0.000577145\n0.024023842\n.\n.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCTR: dod\n0.000894364\n0.029905924\n.\n.\n.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nsize: small\n0.025310609\n0.159093081\n.\n.\n.\n.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCTR: sod & size: small\n0.004316747\n0.065701959\n.\n.\n.\n.\n.\n\n\n\n\n\n\n\n\n\n\n\n\n\nCTR: dos & size: small\n0.000000926\n0.000962365\n.\n.\n.\n.\n.\n.\n\n\n\n\n\n\n\n\n\n\n\n\nCTR: dod & size: small\n0.000383055\n0.019571794\n.\n.\n.\n.\n.\n.\n.\n\n\n\n\n\n\n\n\n\n\n\ncardinal: diagonal\n0.000972318\n0.031182012\n.\n.\n.\n.\n.\n.\n.\n.\n\n\n\n\n\n\n\n\n\n\nCTR: sod & cardinal: diagonal\n0.000294448\n0.017159499\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\n\n\n\n\n\n\n\n\nCTR: dos & cardinal: diagonal\n0.000000000\n0.000000000\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\n\n\n\n\n\n\n\nCTR: dod & cardinal: diagonal\n0.000010975\n0.003312812\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\n\n\n\n\n\n\nsize: small & cardinal: diagonal\n0.001254094\n0.035413184\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\n\n\n\n\n\nCTR: sod & size: small & cardinal: diagonal\n0.000000719\n0.000847671\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\n\n\n\n\nCTR: dos & size: small & cardinal: diagonal\n0.000000000\n0.000000000\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\n\n\n\nCTR: dod & size: small & cardinal: diagonal\n0.000001038\n0.001019064\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\n\nResidual\n\n0.036196631\n0.190254123",
    "crumbs": [
      "Worked examples",
      "RePsychLing Kliegl, Kuschela, & Laubrock (2015)- Reduction of Model Complexity"
    ]
  },
  {
    "objectID": "kkl15.html#reduced-zcp-lmm",
    "href": "kkl15.html#reduced-zcp-lmm",
    "title": "RePsychLing Kliegl, Kuschela, & Laubrock (2015)- Reduction of Model Complexity",
    "section": "6.2 Reduced zcp LMM",
    "text": "6.2 Reduced zcp LMM\nTake out VC for interactions.\n\nm_zcp1_rdc = let\n  form = @formula log(rt) ~ 1 + CTR * size * cardinal + \n                   zerocorr(1 + CTR + size + cardinal | Subj)\n  fit(MixedModel, form, dat; contrasts)\nend\n\n\n\n\n\n\n\n\n\n\n\n\n\nEst.\nSE\nz\np\nσ_Subj\n\n\n\n\n(Intercept)\n5.6911\n0.0173\n329.28\n&lt;1e-99\n0.1569\n\n\nCTR: sod\n0.0745\n0.0077\n9.66\n&lt;1e-21\n0.0666\n\n\nCTR: dos\n0.0409\n0.0045\n9.09\n&lt;1e-19\n0.0240\n\n\nCTR: dod\n0.0015\n0.0053\n0.29\n0.7751\n0.0357\n\n\nsize: small\n0.0920\n0.0173\n5.33\n&lt;1e-06\n0.0312\n\n\ncardinal: diagonal\n0.0204\n0.0053\n3.88\n0.0001\n0.0476\n\n\nCTR: sod & size: small\n0.0244\n0.0077\n3.17\n0.0015\n\n\n\nCTR: dos & size: small\n-0.0054\n0.0045\n-1.20\n0.2317\n\n\n\nCTR: dod & size: small\n0.0181\n0.0053\n3.39\n0.0007\n\n\n\nCTR: sod & cardinal: diagonal\n0.0101\n0.0028\n3.64\n0.0003\n\n\n\nCTR: dos & cardinal: diagonal\n0.0046\n0.0037\n1.25\n0.2125\n\n\n\nCTR: dod & cardinal: diagonal\n-0.0056\n0.0037\n-1.51\n0.1317\n\n\n\nsize: small & cardinal: diagonal\n0.0042\n0.0053\n0.79\n0.4286\n\n\n\nCTR: sod & size: small & cardinal: diagonal\n-0.0031\n0.0028\n-1.12\n0.2610\n\n\n\nCTR: dos & size: small & cardinal: diagonal\n-0.0005\n0.0037\n-0.12\n0.9023\n\n\n\nCTR: dod & size: small & cardinal: diagonal\n0.0109\n0.0037\n2.95\n0.0032\n\n\n\nResidual\n0.1904\n\n\n\n\n\n\n\n\n\n\nissingular(m_zcp1_rdc)\n\nfalse\n\n\n\nonly(MixedModels.PCA(m_zcp1_rdc))\n\n\nPrincipal components based on correlation matrix\n (Intercept)         1.0  .    .    .    .    .\n CTR: sod            0.0  1.0  .    .    .    .\n CTR: dos            0.0  0.0  1.0  .    .    .\n CTR: dod            0.0  0.0  0.0  1.0  .    .\n size: small         0.0  0.0  0.0  0.0  1.0  .\n cardinal: diagonal  0.0  0.0  0.0  0.0  0.0  1.0\n\nNormalized cumulative variances:\n[0.1667, 0.3333, 0.5, 0.6667, 0.8333, 1.0]\n\nComponent loadings\n                      PC1   PC2   PC3   PC4   PC5   PC6\n (Intercept)         1.0   0.0   0.0   0.0   0.0   0.0\n CTR: sod            0.0   1.0   0.0   0.0   0.0   0.0\n CTR: dos            0.0   0.0   1.0   0.0   0.0   0.0\n CTR: dod            0.0   0.0   0.0   1.0   0.0   0.0\n size: small         0.0   0.0   0.0   0.0   1.0   0.0\n cardinal: diagonal  0.0   0.0   0.0   0.0   0.0   1.0\n\n\n\nVarCorr(m_zcp1_rdc)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nColumn\nVariance\nStd.Dev\nCorr.\n\n\n\n\n\n\n\n\nSubj\n(Intercept)\n0.0246022\n0.1568508\n\n\n\n\n\n\n\n\nCTR: sod\n0.0044408\n0.0666395\n.\n\n\n\n\n\n\n\nCTR: dos\n0.0005763\n0.0240065\n.\n.\n\n\n\n\n\n\nCTR: dod\n0.0012752\n0.0357098\n.\n.\n.\n\n\n\n\n\nsize: small\n0.0009731\n0.0311946\n.\n.\n.\n.\n\n\n\n\ncardinal: diagonal\n0.0022679\n0.0476222\n.\n.\n.\n.\n.\n\n\nResidual\n\n0.0362601\n0.1904207",
    "crumbs": [
      "Worked examples",
      "RePsychLing Kliegl, Kuschela, & Laubrock (2015)- Reduction of Model Complexity"
    ]
  },
  {
    "objectID": "kkl15.html#model-comparison-1",
    "href": "kkl15.html#model-comparison-1",
    "title": "RePsychLing Kliegl, Kuschela, & Laubrock (2015)- Reduction of Model Complexity",
    "section": "6.3 Model comparison 1",
    "text": "6.3 Model comparison 1\nLet’s compare the three models.\n\ngof_summary = let\n  nms = [:m_zcp1_rdc, :m_zcp1, :m_max]\n  mods = eval.(nms)\n  lrt = MixedModels.likelihoodratiotest(m_zcp1_rdc, m_zcp1, m_max)\n  DataFrame(;\n    name = nms, \n    dof=dof.(mods),\n    deviance=round.(deviance.(mods), digits=0),\n    AIC=round.(aic.(mods),digits=0),\n    AICc=round.(aicc.(mods),digits=0),\n     BIC=round.(bic.(mods),digits=0),\n    χ²=vcat(:., round.(lrt.tests.deviancediff, digits=0)),\n    χ²_dof=vcat(:., round.(lrt.tests.dofdiff, digits=0)),\n    pvalue=vcat(:., round.(lrt.tests.pvalues, digits=3))\n  )\nend\n\n3×9 DataFrame\n\n\n\nRow\nname\ndof\ndeviance\nAIC\nAICc\nBIC\nχ²\nχ²_dof\npvalue\n\n\n\nSymbol\nInt64\nFloat64\nFloat64\nFloat64\nFloat64\nAny\nAny\nAny\n\n\n\n\n1\nm_zcp1_rdc\n23\n-24558.0\n-24512.0\n-24512.0\n-24308.0\n.\n.\n.\n\n\n2\nm_zcp1\n33\n-24589.0\n-24523.0\n-24523.0\n-24229.0\n30.0\n10.0\n0.001\n\n\n3\nm_max\n153\n-24691.0\n-24385.0\n-24384.0\n-23025.0\n103.0\n120.0\n0.872",
    "crumbs": [
      "Worked examples",
      "RePsychLing Kliegl, Kuschela, & Laubrock (2015)- Reduction of Model Complexity"
    ]
  },
  {
    "objectID": "kkl15.html#parsimonious-lmm-1",
    "href": "kkl15.html#parsimonious-lmm-1",
    "title": "RePsychLing Kliegl, Kuschela, & Laubrock (2015)- Reduction of Model Complexity",
    "section": "6.4 Parsimonious LMM (1)",
    "text": "6.4 Parsimonious LMM (1)\nExtend zcp-reduced LMM with CPs\n\nm_prm1 = let\n  form = @formula log(rt) ~ 1 + CTR * size * cardinal + \n                           (1 + CTR + size + cardinal | Subj)\n  fit(MixedModel, form, dat; contrasts)\nend\n\nMinimizing 801    Time: 0:00:00 ( 0.18 ms/it)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEst.\nSE\nz\np\nσ_Subj\n\n\n\n\n(Intercept)\n5.6911\n0.0176\n323.61\n&lt;1e-99\n0.1181\n\n\nCTR: sod\n0.0745\n0.0076\n9.75\n&lt;1e-21\n0.0660\n\n\nCTR: dos\n0.0408\n0.0043\n9.42\n&lt;1e-20\n0.0213\n\n\nCTR: dod\n0.0017\n0.0051\n0.34\n0.7374\n0.0326\n\n\nsize: small\n0.0921\n0.0176\n5.23\n&lt;1e-06\n0.1119\n\n\ncardinal: diagonal\n0.0204\n0.0053\n3.88\n0.0001\n0.0476\n\n\nCTR: sod & size: small\n0.0244\n0.0076\n3.19\n0.0014\n\n\n\nCTR: dos & size: small\n-0.0054\n0.0043\n-1.24\n0.2156\n\n\n\nCTR: dod & size: small\n0.0181\n0.0051\n3.55\n0.0004\n\n\n\nCTR: sod & cardinal: diagonal\n0.0101\n0.0028\n3.64\n0.0003\n\n\n\nCTR: dos & cardinal: diagonal\n0.0046\n0.0037\n1.25\n0.2118\n\n\n\nCTR: dod & cardinal: diagonal\n-0.0056\n0.0037\n-1.51\n0.1301\n\n\n\nsize: small & cardinal: diagonal\n0.0042\n0.0053\n0.79\n0.4272\n\n\n\nCTR: sod & size: small & cardinal: diagonal\n-0.0031\n0.0028\n-1.13\n0.2597\n\n\n\nCTR: dos & size: small & cardinal: diagonal\n-0.0005\n0.0037\n-0.13\n0.8997\n\n\n\nCTR: dod & size: small & cardinal: diagonal\n0.0110\n0.0037\n2.98\n0.0029\n\n\n\nResidual\n0.1904\n\n\n\n\n\n\n\n\n\n\nissingular(m_prm1)\n\nfalse\n\n\n\nonly(MixedModels.PCA(m_prm1))\n\n\nPrincipal components based on correlation matrix\n (Intercept)          1.0     .      .      .      .      .\n CTR: sod             0.64   1.0     .      .      .      .\n CTR: dos             0.16  -0.08   1.0     .      .      .\n CTR: dod             0.79   0.6    0.28   1.0     .      .\n size: small         -0.42  -0.16  -0.12  -0.03   1.0     .\n cardinal: diagonal   0.05  -0.05  -0.02   0.05   0.08   1.0\n\nNormalized cumulative variances:\n[0.414, 0.5967, 0.772, 0.926, 0.9822, 1.0]\n\nComponent loadings\n                       PC1    PC2    PC3    PC4    PC5    PC6\n (Intercept)         -0.6    0.0    0.03   0.12  -0.37  -0.7\n CTR: sod            -0.5   -0.26   0.31  -0.11   0.76   0.02\n CTR: dos            -0.16   0.52  -0.72  -0.23   0.34  -0.11\n CTR: dod            -0.55  -0.18  -0.2   -0.27  -0.4    0.63\n size: small          0.25  -0.59  -0.23  -0.66  -0.03  -0.32\n cardinal: diagonal  -0.0   -0.53  -0.54   0.64   0.12   0.03\n\n\n\nVarCorr(m_prm1)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nColumn\nVariance\nStd.Dev\nCorr.\n\n\n\n\n\n\n\n\nSubj\n(Intercept)\n0.0139568\n0.1181388\n\n\n\n\n\n\n\n\nCTR: sod\n0.0043556\n0.0659971\n+0.64\n\n\n\n\n\n\n\nCTR: dos\n0.0004536\n0.0212981\n+0.16\n-0.08\n\n\n\n\n\n\nCTR: dod\n0.0010640\n0.0326186\n+0.79\n+0.60\n+0.28\n\n\n\n\n\nsize: small\n0.0125269\n0.1119239\n-0.42\n-0.16\n-0.12\n-0.03\n\n\n\n\ncardinal: diagonal\n0.0022690\n0.0476336\n+0.05\n-0.05\n-0.02\n+0.05\n+0.08\n\n\nResidual\n\n0.0362661\n0.1904365\n\n\n\n\n\n\n\n\n\n\nWe note that the critical correlation parameter between spatial (sod) and attraction (dod) is now estimated at .60 – not that close to the 1.0 boundary that caused singularity in Kliegl et al. (2011).",
    "crumbs": [
      "Worked examples",
      "RePsychLing Kliegl, Kuschela, & Laubrock (2015)- Reduction of Model Complexity"
    ]
  },
  {
    "objectID": "kkl15.html#model-comparison-2",
    "href": "kkl15.html#model-comparison-2",
    "title": "RePsychLing Kliegl, Kuschela, & Laubrock (2015)- Reduction of Model Complexity",
    "section": "6.5 Model comparison 2",
    "text": "6.5 Model comparison 2\n\ngof_summary = let\n  nms = [:m_zcp1_rdc, :m_prm1, :m_max]\n  mods = eval.(nms)\n  lrt = MixedModels.likelihoodratiotest(m_prm1, m_zcp1, m_max)\n  DataFrame(;\n    name = nms, \n    dof=dof.(mods),\n    deviance=round.(deviance.(mods), digits=0),\n    AIC=round.(aic.(mods),digits=0),\n    AICc=round.(aicc.(mods),digits=0),\n     BIC=round.(bic.(mods),digits=0),\n    χ²=vcat(:., round.(lrt.tests.deviancediff, digits=0)),\n    χ²_dof=vcat(:., round.(lrt.tests.dofdiff, digits=0)),\n    pvalue=vcat(:., round.(lrt.tests.pvalues, digits=3))\n  )\nend\n\n3×9 DataFrame\n\n\n\nRow\nname\ndof\ndeviance\nAIC\nAICc\nBIC\nχ²\nχ²_dof\npvalue\n\n\n\nSymbol\nInt64\nFloat64\nFloat64\nFloat64\nFloat64\nAny\nAny\nAny\n\n\n\n\n1\nm_zcp1_rdc\n23\n-24558.0\n-24512.0\n-24512.0\n-24308.0\n.\n.\n.\n\n\n2\nm_prm1\n38\n-24615.0\n-24539.0\n-24539.0\n-24201.0\n27.0\n5.0\n0.0\n\n\n3\nm_max\n153\n-24691.0\n-24385.0\n-24384.0\n-23025.0\n76.0\n115.0\n0.998",
    "crumbs": [
      "Worked examples",
      "RePsychLing Kliegl, Kuschela, & Laubrock (2015)- Reduction of Model Complexity"
    ]
  },
  {
    "objectID": "kkl15.html#complex-lmm",
    "href": "kkl15.html#complex-lmm",
    "title": "RePsychLing Kliegl, Kuschela, & Laubrock (2015)- Reduction of Model Complexity",
    "section": "7.1 Complex LMM",
    "text": "7.1 Complex LMM\nTake out interaction VCs.\n\nm_cpx = let\n  form = @formula log(rt) ~ 1 + CTR * size * cardinal + \n                           (1 + CTR + size + cardinal | Subj)\n  fit(MixedModel, form, dat; contrasts)\nend\n\nMinimizing 801    Time: 0:00:00 ( 0.19 ms/it)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEst.\nSE\nz\np\nσ_Subj\n\n\n\n\n(Intercept)\n5.6911\n0.0176\n323.61\n&lt;1e-99\n0.1181\n\n\nCTR: sod\n0.0745\n0.0076\n9.75\n&lt;1e-21\n0.0660\n\n\nCTR: dos\n0.0408\n0.0043\n9.42\n&lt;1e-20\n0.0213\n\n\nCTR: dod\n0.0017\n0.0051\n0.34\n0.7374\n0.0326\n\n\nsize: small\n0.0921\n0.0176\n5.23\n&lt;1e-06\n0.1119\n\n\ncardinal: diagonal\n0.0204\n0.0053\n3.88\n0.0001\n0.0476\n\n\nCTR: sod & size: small\n0.0244\n0.0076\n3.19\n0.0014\n\n\n\nCTR: dos & size: small\n-0.0054\n0.0043\n-1.24\n0.2156\n\n\n\nCTR: dod & size: small\n0.0181\n0.0051\n3.55\n0.0004\n\n\n\nCTR: sod & cardinal: diagonal\n0.0101\n0.0028\n3.64\n0.0003\n\n\n\nCTR: dos & cardinal: diagonal\n0.0046\n0.0037\n1.25\n0.2118\n\n\n\nCTR: dod & cardinal: diagonal\n-0.0056\n0.0037\n-1.51\n0.1301\n\n\n\nsize: small & cardinal: diagonal\n0.0042\n0.0053\n0.79\n0.4272\n\n\n\nCTR: sod & size: small & cardinal: diagonal\n-0.0031\n0.0028\n-1.13\n0.2597\n\n\n\nCTR: dos & size: small & cardinal: diagonal\n-0.0005\n0.0037\n-0.13\n0.8997\n\n\n\nCTR: dod & size: small & cardinal: diagonal\n0.0110\n0.0037\n2.98\n0.0029\n\n\n\nResidual\n0.1904",
    "crumbs": [
      "Worked examples",
      "RePsychLing Kliegl, Kuschela, & Laubrock (2015)- Reduction of Model Complexity"
    ]
  },
  {
    "objectID": "kkl15.html#zero-correlation-parameter-lmm-2",
    "href": "kkl15.html#zero-correlation-parameter-lmm-2",
    "title": "RePsychLing Kliegl, Kuschela, & Laubrock (2015)- Reduction of Model Complexity",
    "section": "7.2 Zero-correlation parameter LMM (2)",
    "text": "7.2 Zero-correlation parameter LMM (2)\nTake out interaction VCs.\n\nm_zcp2 = let\n  form = @formula log(rt) ~ 1 + CTR * size * cardinal + \n                   zerocorr(1 + CTR + size + cardinal | Subj)\n  fit(MixedModel, form, dat; contrasts)\nend\n\n\n\n\n\n\n\n\n\n\n\n\n\nEst.\nSE\nz\np\nσ_Subj\n\n\n\n\n(Intercept)\n5.6911\n0.0173\n329.28\n&lt;1e-99\n0.1569\n\n\nCTR: sod\n0.0745\n0.0077\n9.66\n&lt;1e-21\n0.0666\n\n\nCTR: dos\n0.0409\n0.0045\n9.09\n&lt;1e-19\n0.0240\n\n\nCTR: dod\n0.0015\n0.0053\n0.29\n0.7751\n0.0357\n\n\nsize: small\n0.0920\n0.0173\n5.33\n&lt;1e-06\n0.0312\n\n\ncardinal: diagonal\n0.0204\n0.0053\n3.88\n0.0001\n0.0476\n\n\nCTR: sod & size: small\n0.0244\n0.0077\n3.17\n0.0015\n\n\n\nCTR: dos & size: small\n-0.0054\n0.0045\n-1.20\n0.2317\n\n\n\nCTR: dod & size: small\n0.0181\n0.0053\n3.39\n0.0007\n\n\n\nCTR: sod & cardinal: diagonal\n0.0101\n0.0028\n3.64\n0.0003\n\n\n\nCTR: dos & cardinal: diagonal\n0.0046\n0.0037\n1.25\n0.2125\n\n\n\nCTR: dod & cardinal: diagonal\n-0.0056\n0.0037\n-1.51\n0.1317\n\n\n\nsize: small & cardinal: diagonal\n0.0042\n0.0053\n0.79\n0.4286\n\n\n\nCTR: sod & size: small & cardinal: diagonal\n-0.0031\n0.0028\n-1.12\n0.2610\n\n\n\nCTR: dos & size: small & cardinal: diagonal\n-0.0005\n0.0037\n-0.12\n0.9023\n\n\n\nCTR: dod & size: small & cardinal: diagonal\n0.0109\n0.0037\n2.95\n0.0032\n\n\n\nResidual\n0.1904",
    "crumbs": [
      "Worked examples",
      "RePsychLing Kliegl, Kuschela, & Laubrock (2015)- Reduction of Model Complexity"
    ]
  },
  {
    "objectID": "kkl15.html#model-comparison-3",
    "href": "kkl15.html#model-comparison-3",
    "title": "RePsychLing Kliegl, Kuschela, & Laubrock (2015)- Reduction of Model Complexity",
    "section": "7.3 Model comparison 3",
    "text": "7.3 Model comparison 3\n\ngof_summary = let\n  nms = [:m_zcp2, :m_cpx, :m_max]\n  mods = eval.(nms)\n  lrt = MixedModels.likelihoodratiotest(m_zcp2, m_cpx, m_max)\n  DataFrame(;\n    name = nms, \n    dof=dof.(mods),\n    deviance=round.(deviance.(mods), digits=0),\n    AIC=round.(aic.(mods),digits=0),\n    AICc=round.(aicc.(mods),digits=0),\n     BIC=round.(bic.(mods),digits=0),\n    χ²=vcat(:., round.(lrt.tests.deviancediff, digits=0)),\n    χ²_dof=vcat(:., round.(lrt.tests.dofdiff, digits=0)),\n    pvalue=vcat(:., round.(lrt.tests.pvalues, digits=3))\n  )\nend\n\n3×9 DataFrame\n\n\n\nRow\nname\ndof\ndeviance\nAIC\nAICc\nBIC\nχ²\nχ²_dof\npvalue\n\n\n\nSymbol\nInt64\nFloat64\nFloat64\nFloat64\nFloat64\nAny\nAny\nAny\n\n\n\n\n1\nm_zcp2\n23\n-24558.0\n-24512.0\n-24512.0\n-24308.0\n.\n.\n.\n\n\n2\nm_cpx\n38\n-24615.0\n-24539.0\n-24539.0\n-24201.0\n57.0\n15.0\n0.0\n\n\n3\nm_max\n153\n-24691.0\n-24385.0\n-24384.0\n-23025.0\n76.0\n115.0\n0.998",
    "crumbs": [
      "Worked examples",
      "RePsychLing Kliegl, Kuschela, & Laubrock (2015)- Reduction of Model Complexity"
    ]
  },
  {
    "objectID": "kkl15.html#residual-over-fitted-plot",
    "href": "kkl15.html#residual-over-fitted-plot",
    "title": "RePsychLing Kliegl, Kuschela, & Laubrock (2015)- Reduction of Model Complexity",
    "section": "10.1 Residual-over-fitted plot",
    "text": "10.1 Residual-over-fitted plot\nThe slant in residuals show a lower and upper boundary of reaction times, that is we have have too few short and too few long residuals. Not ideal, but at least width of the residual band looks similar across the fitted values, that is there is no evidence for heteroskedasticity.\n\n\nCode\nCairoMakie.activate!(; type=\"png\")\nscatter(fitted(m_prm1), residuals(m_prm1); alpha=0.3)\n\n\n\n\n\n\n\n\nFigure 3: Residuals versus fitted values for model m1\n\n\n\n\n\nWith many observations the scatterplot is not that informative. Contour plots or heatmaps may be an alternative.\n\n\nCode\nset_aog_theme!()\ndraw(\n  data((; f=fitted(m_prm1), r=residuals(m_prm1))) *\n  mapping(\n    :f =&gt; \"Fitted values from m1\", :r =&gt; \"Residuals from m1\"\n  ) *\n  density();\n)\n\n\n\n\n\n\n\n\nFigure 4: Heatmap of residuals versus fitted values for model m1",
    "crumbs": [
      "Worked examples",
      "RePsychLing Kliegl, Kuschela, & Laubrock (2015)- Reduction of Model Complexity"
    ]
  },
  {
    "objectID": "kkl15.html#q-q-plot",
    "href": "kkl15.html#q-q-plot",
    "title": "RePsychLing Kliegl, Kuschela, & Laubrock (2015)- Reduction of Model Complexity",
    "section": "10.2 Q-Q plot",
    "text": "10.2 Q-Q plot\nThe plot of quantiles of model residuals over corresponding quantiles of the normal distribution should yield a straight line along the main diagonal.\n\n\nCode\nCairoMakie.activate!(; type=\"png\")\nqqnorm(\n  residuals(m_prm1);\n  qqline=:none,\n  axis=(;\n    xlabel=\"Standard normal quantiles\",\n    ylabel=\"Quantiles of the residuals from model m1\",\n  ),\n)\n\n\n\n\n\n\n\n\nFigure 5: Quantile-quantile plot of the residuals for model m1 versus a standard normal",
    "crumbs": [
      "Worked examples",
      "RePsychLing Kliegl, Kuschela, & Laubrock (2015)- Reduction of Model Complexity"
    ]
  },
  {
    "objectID": "kkl15.html#observed-and-theoretical-normal-distribution",
    "href": "kkl15.html#observed-and-theoretical-normal-distribution",
    "title": "RePsychLing Kliegl, Kuschela, & Laubrock (2015)- Reduction of Model Complexity",
    "section": "10.3 Observed and theoretical normal distribution",
    "text": "10.3 Observed and theoretical normal distribution\n******We****** can see this in this plot. Overall, it does not look too bad.\n\n\nCode\nCairoMakie.activate!(; type=\"svg\")\nlet\n  n = nrow(dat)\n  dat_rz = (;\n    value=vcat(residuals(m_prm1) ./ std(residuals(m_prm1)), randn(n)),\n    curve=repeat([\"residual\", \"normal\"]; inner=n),\n  )\n  draw(\n    data(dat_rz) *\n    mapping(:value; color=:curve) *\n    density(; bandwidth=0.1);\n  )\nend\n\n\n\n\n\n\n\n\nFigure 6: Kernel density plot of the standardized residuals for model m1 versus a standard normal",
    "crumbs": [
      "Worked examples",
      "RePsychLing Kliegl, Kuschela, & Laubrock (2015)- Reduction of Model Complexity"
    ]
  },
  {
    "objectID": "kkl15.html#caterpillar-plot",
    "href": "kkl15.html#caterpillar-plot",
    "title": "RePsychLing Kliegl, Kuschela, & Laubrock (2015)- Reduction of Model Complexity",
    "section": "11.1 Caterpillar plot",
    "text": "11.1 Caterpillar plot\n\n\nCode\ncm1 = only(ranefinfo(m_prm1))\ncaterpillar!(Figure(; resolution=(800, 1200)), cm1; orderby=2)\n\n\n\n\n\n\n\n\nFigure 7: Prediction intervals of the subject random effects in model m1",
    "crumbs": [
      "Worked examples",
      "RePsychLing Kliegl, Kuschela, & Laubrock (2015)- Reduction of Model Complexity"
    ]
  },
  {
    "objectID": "kkl15.html#shrinkage-plot",
    "href": "kkl15.html#shrinkage-plot",
    "title": "RePsychLing Kliegl, Kuschela, & Laubrock (2015)- Reduction of Model Complexity",
    "section": "11.2 Shrinkage plot",
    "text": "11.2 Shrinkage plot\n\n11.2.1 Log-transformed reaction times (LMM m_prm1)\n\n\nCode\nshrinkageplot!(Figure(; resolution=(1000, 1200)), m_prm1)\n\n\n\n\n\n\n\n\nFigure 8: Shrinkage plots of the subject random effects in model m1L",
    "crumbs": [
      "Worked examples",
      "RePsychLing Kliegl, Kuschela, & Laubrock (2015)- Reduction of Model Complexity"
    ]
  },
  {
    "objectID": "kkl15.html#generate-a-bootstrap-sample",
    "href": "kkl15.html#generate-a-bootstrap-sample",
    "title": "RePsychLing Kliegl, Kuschela, & Laubrock (2015)- Reduction of Model Complexity",
    "section": "12.1 Generate a bootstrap sample",
    "text": "12.1 Generate a bootstrap sample\nWe generate 2500 samples for the 15 model parameters (4 fixed effect, 7 VCs, 15 CPs, and 1 residual).\n\nsamp = parametricbootstrap(MersenneTwister(1234321), 2500, m_prm1;\n                           optsum_overrides=(; ftol_rel=1e-8));\n\n\ntbl = samp.tbl\n\nTable with 60 columns and 2500 rows:\n      obj       β01      β02        β03        β04           β05        ⋯\n    ┌────────────────────────────────────────────────────────────────────\n 1  │ -25123.8  5.68038  0.068683   0.039261   -0.00165125   0.0793335  ⋯\n 2  │ -24465.9  5.66021  0.0667158  0.0418969  -0.00305185   0.100958   ⋯\n 3  │ -24446.4  5.6924   0.0778707  0.0369443  0.00797653    0.100114   ⋯\n 4  │ -24617.7  5.69562  0.0766171  0.0463011  -0.00297233   0.0639383  ⋯\n 5  │ -25167.8  5.68415  0.0684032  0.0385817  0.00248626    0.118136   ⋯\n 6  │ -24727.8  5.69527  0.0843717  0.0442493  0.00193816    0.0936532  ⋯\n 7  │ -24512.0  5.67742  0.0707211  0.0418414  -0.00641572   0.076351   ⋯\n 8  │ -24798.9  5.69362  0.0765456  0.0458011  0.00328823    0.084062   ⋯\n 9  │ -24392.5  5.68477  0.0818642  0.0496969  -0.000832281  0.0977178  ⋯\n 10 │ -24585.7  5.68801  0.0796325  0.0344963  0.00629559    0.081441   ⋯\n 11 │ -25078.8  5.66697  0.0663796  0.0445354  -0.000736944  0.0943102  ⋯\n 12 │ -24348.5  5.67785  0.0698993  0.0446523  -0.000859312  0.102561   ⋯\n 13 │ -24973.2  5.68851  0.0732837  0.0421743  -0.00279674   0.0830709  ⋯\n 14 │ -24617.9  5.70183  0.07733    0.0433379  -0.00455447   0.0694739  ⋯\n 15 │ -24775.8  5.68347  0.0825422  0.0371819  0.00388279    0.0889206  ⋯\n 16 │ -24628.0  5.69649  0.0718965  0.0408693  0.000221452   0.0878074  ⋯\n 17 │ -24397.7  5.70743  0.0966594  0.0363991  0.00133149    0.087301   ⋯\n 18 │ -24836.3  5.73132  0.0753929  0.0424585  0.00735246    0.0482915  ⋯\n 19 │ -24806.7  5.68484  0.0744908  0.0369567  0.0060191     0.0529119  ⋯\n 20 │ -24234.2  5.6857   0.0691212  0.0481745  -6.76522e-5   0.0718787  ⋯\n 21 │ -24927.4  5.67965  0.066628   0.0441619  0.000997131   0.0939013  ⋯\n 22 │ -24044.7  5.68194  0.0675943  0.0364103  0.0108015     0.096463   ⋯\n 23 │ -24804.0  5.72333  0.0826845  0.0393307  0.00877465    0.0955252  ⋯\n ⋮  │    ⋮         ⋮         ⋮          ⋮           ⋮            ⋮      ⋱",
    "crumbs": [
      "Worked examples",
      "RePsychLing Kliegl, Kuschela, & Laubrock (2015)- Reduction of Model Complexity"
    ]
  },
  {
    "objectID": "kkl15.html#shortest-coverage-interval",
    "href": "kkl15.html#shortest-coverage-interval",
    "title": "RePsychLing Kliegl, Kuschela, & Laubrock (2015)- Reduction of Model Complexity",
    "section": "12.2 Shortest coverage interval",
    "text": "12.2 Shortest coverage interval\n\nconfint(samp)\n\nDictTable with 2 columns and 38 rows:\n par   lower        upper\n ────┬────────────────────────\n β01 │ 5.65966      5.72791\n β02 │ 0.0602683    0.0902347\n β03 │ 0.0326438    0.0490006\n β04 │ -0.00832038  0.0114628\n β05 │ 0.0585181    0.124489\n β06 │ 0.00932821   0.030229\n β07 │ 0.00994937   0.0391542\n β08 │ -0.0138358   0.00317468\n β09 │ 0.00770406   0.0274789\n β10 │ 0.00485596   0.0157058\n β11 │ -0.00249685  0.0117935\n β12 │ -0.0124823   0.00202088\n β13 │ -0.00582681  0.0137215\n β14 │ -0.00828443  0.00227533\n β15 │ -0.00842329  0.00599236\n β16 │ 0.00369609   0.0179879\n ρ01 │ 0.380561     1.0\n ρ02 │ -0.342485    0.999999\n ρ03 │ -0.685044    0.682981\n ρ04 │ 0.466487     0.989877\n ρ05 │ 0.330399     0.883748\n ρ06 │ -0.273379    0.932581\n ρ07 │ -0.729693    -0.0549567\n  ⋮  │      ⋮           ⋮\n\n\nWe can also visualize the shortest coverage intervals for fixed effects with the ridgeplot() command:\n\n\nCode\nridgeplot(samp; show_intercept=false)\n\n\n\n\n\n\n\n\nFigure 9: Ridge plot of fixed-effects bootstrap samples from model m1L",
    "crumbs": [
      "Worked examples",
      "RePsychLing Kliegl, Kuschela, & Laubrock (2015)- Reduction of Model Complexity"
    ]
  },
  {
    "objectID": "kkl15.html#comparative-density-plots-of-bootstrapped-parameter-estimates",
    "href": "kkl15.html#comparative-density-plots-of-bootstrapped-parameter-estimates",
    "title": "RePsychLing Kliegl, Kuschela, & Laubrock (2015)- Reduction of Model Complexity",
    "section": "12.3 Comparative density plots of bootstrapped parameter estimates",
    "text": "12.3 Comparative density plots of bootstrapped parameter estimates\n\n12.3.1 Residual\n\n\nCode\ndraw(\n  data(tbl) *\n  mapping(:σ =&gt; \"Residual\") *\n  density();\n  figure=(; resolution=(800, 400)),\n)\n\n\n\n\n\n\n\n\nFigure 10: Kernel density estimate from bootstrap samples of the residual standard deviation for model m_prm1\n\n\n\n\n\n\n\n12.3.2 Fixed effects and associated variance components (w/o GM)\nThe shortest coverage interval for the GM ranges from x to x ms and the associate variance component from .x to .x. To keep the plot range small we do not include their densities here.\n\n\nCode\nrn = renamer([\n  \"(Intercept)\" =&gt; \"GM\",\n  \"CTR: sod\" =&gt; \"spatial effect\",\n  \"CTR: dos\" =&gt; \"object effect\",\n  \"CTR: dod\" =&gt; \"attraction effect\",\n  \"(Intercept), CTR: sod\" =&gt; \"GM, spatial\",\n  \"(Intercept), CTR: dos\" =&gt; \"GM, object\",\n  \"CTR: sod, CTR: dos\" =&gt; \"spatial, object\",\n  \"(Intercept), CTR: dod\" =&gt; \"GM, attraction\",\n  \"CTR: sod, CTR: dod\" =&gt; \"spatial, attraction\",\n  \"CTR: dos, CTR: dod\" =&gt; \"object, attraction\",\n])\ndraw(\n  data(tbl) *\n  mapping(\n    [:β02, :β03, :β04] .=&gt; \"Experimental effect size [ms]\";\n    color=dims(1) =&gt; \n    renamer([\"spatial effect\", \"object effect\", \"attraction effect\"]) =&gt;\n    \"Experimental effects\",\n  ) *\n  density();\n  figure=(; resolution=(800, 350)),\n)\n\n\n\n\n\n\n\n\nFigure 11: Kernel density estimate from bootstrap samples of the fixed effects for model m_prm1\n\n\n\n\n\nThe densitiies correspond nicely with the shortest coverage intervals.\n\n\nCode\ndraw(\n  data(tbl) *\n  mapping(\n    [:σ2, :σ3, :σ4] .=&gt; \"Standard deviations [ms]\";\n    color=dims(1) =&gt;\n    renamer([\"spatial effect\", \"object effect\", \"attraction effect\"]) =&gt;\n    \"Variance components\",\n  ) *\n  density();\n  figure=(; resolution=(800, 350)),\n)\n\n\n\n\n\n\n\n\nFigure 12: Kernel density estimate from bootstrap samples of the standard deviations for model m1L (excluding Grand Mean)\n\n\n\n\n\nThe VC are all very nicely defined.\n\n\n12.3.3 Correlation parameters (CPs)\n\n\nCode\ndraw(\n  data(tbl) *\n  mapping(\n    [:ρ01, :ρ02, :ρ03, :ρ04, :ρ05, :ρ06] .=&gt; \"Correlation\";\n    color=dims(1) =&gt;\n    renamer([\"GM, spatial\", \"GM, object\", \"spatial, object\",\n    \"GM, attraction\", \"spatial, attraction\", \"object, attraction\"]) =&gt;\n    \"Correlation parameters\",\n  ) *\n  density();\n  figure=(; resolution=(800, 350)),\n)\n\n\n\n\n\n\n\n\nFigure 13: Kernel density estimate from bootstrap samples of the standard deviations for model m1L\n\n\n\n\n\nThree CPs stand out positively, the correlation between GM and the spatial effect, GM and attraction effect, and the correlation between spatial and attraction effects. The second CP was positive, but not significant in the first study. The third CP replicates a CP that was judged questionable in script kwdyz11.jl.\nThe three remaining CPs are not well defined for log-transformed reaction times; they only fit noise and should be removed. It is also possible that fitting the complex experimental design (including target size and rectangle orientation) will lead to more acceptable estimates. The corresponding plot based on LMM m1_rt for raw reaction times still shows them with very wide distributions, but acceptable.",
    "crumbs": [
      "Worked examples",
      "RePsychLing Kliegl, Kuschela, & Laubrock (2015)- Reduction of Model Complexity"
    ]
  }
]